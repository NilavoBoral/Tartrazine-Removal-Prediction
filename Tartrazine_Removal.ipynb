{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tartrazine_Removal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data"
      ],
      "metadata": {
        "id": "BQIjv6MvOEmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SeNSnXlZMgrh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/Tartazine_Removal_Final.xlsx')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7FX2wVhdNApw",
        "outputId": "e242a501-df9a-4074-ab55-32f59aee9c32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   conc(ppm)  ad_dose(g/L)  ph_value  temperature(⁰C)  time  absorbance  \\\n",
              "0         10           1.0         6               30    15      0.0229   \n",
              "1         10           1.0         6               30    30      0.0153   \n",
              "2         10           1.0         6               30    45      0.0152   \n",
              "3         10           1.0         6               30    60      0.0140   \n",
              "4         10           1.0         6               30   120      0.0110   \n",
              "\n",
              "   conc2(=absorbance/0.029)  removal(=30-conc2)  %removal(=removal*(100/30))  \n",
              "0                  0.789655           29.210345                    97.367816  \n",
              "1                  0.527586           29.472414                    98.241379  \n",
              "2                  0.524138           29.475862                    98.252874  \n",
              "3                  0.482759           29.517241                    98.390805  \n",
              "4                  0.379310           29.620690                    98.735632  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e035f68f-deac-4035-8985-668f641e5c28\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc(ppm)</th>\n",
              "      <th>ad_dose(g/L)</th>\n",
              "      <th>ph_value</th>\n",
              "      <th>temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "      <th>absorbance</th>\n",
              "      <th>conc2(=absorbance/0.029)</th>\n",
              "      <th>removal(=30-conc2)</th>\n",
              "      <th>%removal(=removal*(100/30))</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.0229</td>\n",
              "      <td>0.789655</td>\n",
              "      <td>29.210345</td>\n",
              "      <td>97.367816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.0153</td>\n",
              "      <td>0.527586</td>\n",
              "      <td>29.472414</td>\n",
              "      <td>98.241379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0152</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>29.475862</td>\n",
              "      <td>98.252874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>29.517241</td>\n",
              "      <td>98.390805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>29.620690</td>\n",
              "      <td>98.735632</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e035f68f-deac-4035-8985-668f641e5c28')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e035f68f-deac-4035-8985-668f641e5c28 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e035f68f-deac-4035-8985-668f641e5c28');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# removing similar columns (which can simply calculated mathematically)\n",
        "\n",
        "df = df.drop(columns=['removal(=30-conc2)', '%removal(=removal*(100/30))', 'absorbance'])\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "VeF7qyoTNPjf",
        "outputId": "32359426-de1a-4ea2-a521-41f20d003874"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 6)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   conc(ppm)  ad_dose(g/L)  ph_value  temperature(⁰C)  time  \\\n",
              "0         10           1.0         6               30    15   \n",
              "1         10           1.0         6               30    30   \n",
              "2         10           1.0         6               30    45   \n",
              "3         10           1.0         6               30    60   \n",
              "4         10           1.0         6               30   120   \n",
              "\n",
              "   conc2(=absorbance/0.029)  \n",
              "0                  0.789655  \n",
              "1                  0.527586  \n",
              "2                  0.524138  \n",
              "3                  0.482759  \n",
              "4                  0.379310  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8330df37-3618-4618-a750-1b9bdd948602\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc(ppm)</th>\n",
              "      <th>ad_dose(g/L)</th>\n",
              "      <th>ph_value</th>\n",
              "      <th>temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "      <th>conc2(=absorbance/0.029)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.789655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.527586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.524138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>0.482759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>0.379310</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8330df37-3618-4618-a750-1b9bdd948602')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8330df37-3618-4618-a750-1b9bdd948602 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8330df37-3618-4618-a750-1b9bdd948602');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feature columns\n",
        "x_df = df.drop(columns='conc2(=absorbance/0.029)')\n",
        "\n",
        "# target column\n",
        "y_df = df['conc2(=absorbance/0.029)']"
      ],
      "metadata": {
        "id": "j1YrZUwEzPT4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mms = MinMaxScaler()\n",
        "\n",
        "x_df_scaled = mms.fit_transform(x_df)"
      ],
      "metadata": {
        "id": "pPn77dYQ8sVE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train & test set\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "x_train, x_test, y_train, y_test = tts(x_df_scaled, y_df, test_size=0.1, random_state=10)"
      ],
      "metadata": {
        "id": "ZnqShe9B0Bol"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning"
      ],
      "metadata": {
        "id": "NwLcmhq0kQqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
        "\n",
        "from sklearn.metrics import r2_score as r2\n",
        "from sklearn.metrics import mean_absolute_error as mae"
      ],
      "metadata": {
        "id": "eKs99K49kYSq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin = LinearRegression()\n",
        "knn = KNeighborsRegressor()\n",
        "svr = SVR()\n",
        "tree = DecisionTreeRegressor(random_state=0)\n",
        "bag = BaggingRegressor(DecisionTreeRegressor(random_state=0), random_state=0)\n",
        "forest = RandomForestRegressor(random_state=0)\n",
        "grad = GradientBoostingRegressor(random_state=0)\n",
        "ada = AdaBoostRegressor(DecisionTreeRegressor(random_state=0),random_state=0)"
      ],
      "metadata": {
        "id": "2XIOVgnjkYQA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAE = []\n",
        "R2 = []\n",
        "for i in (lin, knn, svr, tree, bag, forest, grad, ada):\n",
        "  i.fit(x_train, y_train)\n",
        "  y_pred = i.predict(x_test)\n",
        "  \n",
        "  R2.append(r2(y_test, y_pred))\n",
        "  MAE.append(mae(y_test, y_pred))"
      ],
      "metadata": {
        "id": "_1C74DcNkYNN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cheking mean_absolute_error calculated by some ML Algorithms\n",
        "\n",
        "pd.Series(MAE, index=('LR', 'KNN', 'SVR', 'Decision-tree', 'Bagging', 'Random-forest', 'Gradient-Boosting', 'Ada-Boost')).plot(grid=True, figsize=(8,5))\n",
        "plt.xticks(rotation = 60)\n",
        "plt.ylabel('MAE')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "AOLodZVDmHfh",
        "outputId": "708c822f-2c3f-49f8-8a33-ad0309ce749b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-cBRMTdmPOk",
        "outputId": "b155ccc7-8d82-41ed-f4b6-a67b05e28597"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.651351711013849,\n",
              " 1.1625287356321838,\n",
              " 1.3327024387777022,\n",
              " 1.6882503192848022,\n",
              " 1.8402041142127348,\n",
              " 1.81312207474305,\n",
              " 1.4427008490651811,\n",
              " 1.687547892720306]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cheking r2_score calculated by some ML Algorithms\n",
        "\n",
        "pd.Series(R2, index=('LR', 'KNN', 'SVR', 'Decision-tree', 'Bagging', 'Random-forest', 'Gradient-Boosting', 'Ada-Boost')).plot(grid=True, figsize=(8,5))\n",
        "plt.xticks(rotation = 60)\n",
        "plt.ylabel('R2')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "-HxM9njekYLB",
        "outputId": "ad564806-1f38-4d51-dc9b-ae78c039b895"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3MqtuDslx5p",
        "outputId": "f9c8dc7e-a00f-4a3f-b1b9-965e2efaf610"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0474262652989923,\n",
              " 0.7688413527068171,\n",
              " 0.3439996971568996,\n",
              " 0.2670090418104609,\n",
              " 0.15682924869879045,\n",
              " 0.032591433615752785,\n",
              " 0.49371061971236363,\n",
              " 0.26724227852207705]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(R2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2fE6Y6IlziN",
        "outputId": "251dc085-bd24-4aa6-c094-fb3f9594dd19"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7688413527068171"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I can easily see, KNN gives lowest MAE and highest R2 score."
      ],
      "metadata": {
        "id": "fzVOGVNAl5mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN model's parameter tuning"
      ],
      "metadata": {
        "id": "MbdQfz1IQb4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e = []\n",
        "for i in np.arange(1,11):   \n",
        "  knn = KNeighborsRegressor(n_neighbors = i)\n",
        "  knn.fit(x_train, y_train)\n",
        "  y_pred = knn.predict(x_test)\n",
        "\n",
        "  e.append(r2(y_test, y_pred))"
      ],
      "metadata": {
        "id": "4EcOzqwzpmTz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(e, index=np.arange(1,11)).plot(grid= True, figsize=(8,5))\n",
        "plt.xlabel('n_neighbors')\n",
        "plt.ylabel('R2')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "XVgDHJIOqXn9",
        "outputId": "50313224-d6eb-4da3-d53a-a3cb3bffa4cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# knn gives highest R2 score at n_neighbors=3\n",
        "knn = KNeighborsRegressor(n_neighbors = 3)\n",
        "\n",
        "knn.fit(x_train, y_train)\n",
        "y_pred = knn.predict(x_test)\n",
        "\n",
        "print('R2 :', r2(y_test, y_pred))\n",
        "print('MAE :', mae(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMx4M4egmsoI",
        "outputId": "7c221a8e-835e-4252-8305-7029ba698c08"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 : 0.9753892492740357\n",
            "MAE : 0.4563218390804599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# original_x_test\n",
        "# conc(ppm),\tad_dose(g/L),\tph_value,\ttemperature(⁰C),\ttime \n",
        "\n",
        "original_x_test = mms.inverse_transform(x_test)\n",
        "original_x_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fwlxf08jAgq",
        "outputId": "4128cdae-95ee-4c80-9869-19dab589f5de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 30. ,   1. ,   6. ,  30. ,  30. ],\n",
              "       [ 30. ,   0.5,   6. ,  30. ,  30. ],\n",
              "       [ 10. ,   1. ,   6. ,  30. ,  60. ],\n",
              "       [ 30. ,   1.5,   6. ,  30. ,  15. ],\n",
              "       [ 60. ,   1. ,   6. ,  30. , 120. ],\n",
              "       [ 30. ,   1. ,   6. ,  30. , 120. ],\n",
              "       [ 30. ,   1.5,   6. ,  30. ,  45. ],\n",
              "       [ 30. ,   1. ,  10. ,  30. ,  60. ],\n",
              "       [ 30. ,   1.5,   6. ,  30. ,  60. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate actual '%removal'\n",
        "testt = []\n",
        "for i in y_test:\n",
        "  testt.append((30-i)*(100/30))\n",
        "testt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_7K4TK7nD48",
        "outputId": "1b4b5fa8-08d4-44a3-a7dd-38c54940c3f7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[91.11494252873564,\n",
              " 57.6551724137931,\n",
              " 98.39080459770115,\n",
              " 96.88505747126437,\n",
              " 88.95402298850576,\n",
              " 98.0,\n",
              " 97.816091954023,\n",
              " 98.48275862068965,\n",
              " 98.65517241379311]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate ML model predicted '%removal'\n",
        "predd = []\n",
        "for i in y_pred:\n",
        "  predd.append((30-i)*(100/30))\n",
        "predd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnPjNuJynIYg",
        "outputId": "34c71441-9f15-4e07-a3c7-51abf4d71472"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[94.36781609195403,\n",
              " 55.39463601532567,\n",
              " 98.37164750957855,\n",
              " 93.97318007662835,\n",
              " 86.00766283524904,\n",
              " 98.09578544061303,\n",
              " 97.45977011494253,\n",
              " 97.62452107279694,\n",
              " 97.66666666666667]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare actual and predicted '%removal'\n",
        "\n",
        "print('percent removal error :', mae(testt, predd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZqzzRyFnk1p",
        "outputId": "6ddcc451-cc22-4d58-a536-e83c3812ba8b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percent removal error : 1.5210727969348667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save best ML model for further prediction\n",
        "import pickle\n",
        "\n",
        "pickle.dump(knn,open('Nilavo_ML.pkl','wb'))"
      ],
      "metadata": {
        "id": "LGpJFsrUTagm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning\n",
        "\n",
        "ANN"
      ],
      "metadata": {
        "id": "YSJfotxWkNok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner"
      ],
      "metadata": {
        "id": "hImbril5OjyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e949b171-79f8-4a5f-f854-aba8df418726"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 40 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.8.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.8)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.2 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import keras_tuner\n",
        "from kerastuner.tuners import RandomSearch"
      ],
      "metadata": {
        "id": "yqOnY-o9NzWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb1e6cc-5272-433b-ef2d-906f26e9547e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# building model\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units= hp.Int('n', min_value=16, max_value=512, step=16), activation= 'relu', input_dim= x_train.shape[1]))\n",
        "    for i in range(hp.Int('hidden_layers', min_value=1, max_value=3)):\n",
        "        model.add(Dense(units= hp.Int('neurons'+str(i), min_value=16, max_value=256, step=16), activation= 'relu'))\n",
        "    model.add(Dense(units=1, activation='linear'))\n",
        "              \n",
        "    model.compile(optimizer = Adam(learning_rate = hp.Choice('lr', [1e-2, 1e-3, 1e-4])), loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "-QQEp7cLOr2v"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = RandomSearch(build_model, objective='val_mean_absolute_error', max_trials=10, executions_per_trial=3, directory='output', project_name='tartazine_removal')"
      ],
      "metadata": {
        "id": "2GE8pXXxxVk0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search best model\n",
        "tuner.search(x_train, y_train, epochs=20, validation_data=(x_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv9yy6YAzIYB",
        "outputId": "f7a774bb-d6ec-43cf-91b6-1a69d159f555"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 06s]\n",
            "val_mean_absolute_error: 1.521426518758138\n",
            "\n",
            "Best val_mean_absolute_error So Far: 0.40839578708012897\n",
            "Total elapsed time: 00h 01m 35s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "XbgBUgY600VW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my DL model's details\n",
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pPV4Wr_04PY",
        "outputId": "a5f41479-82d3-49fc-cee0-4f00d13f9fef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 48)                288       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 192)               6336      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                3088      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,297\n",
            "Trainable params: 11,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making prediction\n",
        "\n",
        "y_pred = best_model.predict(x_test)"
      ],
      "metadata": {
        "id": "jLird46k1wWf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot actual conc vs predicted conc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('y_test')\n",
        "plt.ylabel('y_pred')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "fKzLD8tIOTUF",
        "outputId": "37f32606-e907-4a86-e85d-1bdb9ed9a7cf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# statistical results of Deep Learning model predicted conc\n",
        "print('R2 :', r2(y_test, y_pred))\n",
        "print('MAE :', mae(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzmnNP9-OQzP",
        "outputId": "f9631648-95c6-45ef-b9c2-0acf423dca5d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 : 0.9893832565588225\n",
            "MAE : 0.2660412106249068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# original_x_test [ conc(ppm),\tad_dose(g/L),\tph_value,\ttemperature(⁰C),\ttime ]\n",
        "\n",
        "original_x_test = mms.inverse_transform(x_test)\n",
        "original_x_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m109ycU-jhZP",
        "outputId": "c79c0490-30e5-42e5-93ca-7a27f6b7b3c8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 30. ,   1. ,   6. ,  30. ,  30. ],\n",
              "       [ 30. ,   0.5,   6. ,  30. ,  30. ],\n",
              "       [ 10. ,   1. ,   6. ,  30. ,  60. ],\n",
              "       [ 30. ,   1.5,   6. ,  30. ,  15. ],\n",
              "       [ 60. ,   1. ,   6. ,  30. , 120. ],\n",
              "       [ 30. ,   1. ,   6. ,  30. , 120. ],\n",
              "       [ 30. ,   1.5,   6. ,  30. ,  45. ],\n",
              "       [ 30. ,   1. ,  10. ,  30. ,  60. ],\n",
              "       [ 30. ,   1.5,   6. ,  30. ,  60. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate actual '%removal'\n",
        "testt = []\n",
        "for i in y_test:\n",
        "  testt.append((30-i)*(100/30))\n",
        "testt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GYkhKZbSZVM",
        "outputId": "d927fa32-89c1-4737-fa13-721a1c732bd8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[91.11494252873564,\n",
              " 57.6551724137931,\n",
              " 98.39080459770115,\n",
              " 96.88505747126437,\n",
              " 88.95402298850576,\n",
              " 98.0,\n",
              " 97.816091954023,\n",
              " 98.48275862068965,\n",
              " 98.65517241379311]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate DL model predicted '%removal'\n",
        "predd = []\n",
        "for i in y_pred:\n",
        "  predd.append((30-i[0])*(100/30))\n",
        "predd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKTcUgQXTHNc",
        "outputId": "2fba3684-e4c0-4567-da2c-d9fec75b91e2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[91.02919975916545,\n",
              " 60.18394470214844,\n",
              " 98.23464949925742,\n",
              " 97.17094838619232,\n",
              " 91.37181758880615,\n",
              " 98.33879878123602,\n",
              " 98.14130147298178,\n",
              " 96.97088559468588,\n",
              " 98.3241730928421]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare actual and predicted '%removal'\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "print('percent removal error :', mae(testt, predd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lwLtFYaUAvm",
        "outputId": "fdaedb2b-7cd0-4be0-e524-058dd0304259"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percent removal error : 0.886804035416354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save best DL model for further prediction\n",
        "import pickle\n",
        "\n",
        "pickle.dump(best_model,open('Nilavo_DL.pkl','wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQZjSrg1Tspq",
        "outputId": "c847d47d-3913-4f0f-dc5f-061839233fe3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://60270a94-d89d-4069-9c79-55d0b9b6b2ac/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Data Using GAN "
      ],
      "metadata": {
        "id": "IY_fap4Kgye3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_cols = ['conc(ppm)',\t'ad_dose(g/L)',\t'ph_value',\t'temperature(⁰C)',\t'time',\t'conc2(=absorbance/0.029)']"
      ],
      "metadata": {
        "id": "CsDkfM-Dre7U"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "CZzL43zBkUol"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN():\n",
        "    \n",
        "    def __init__(self, gan_args):\n",
        "        [self.batch_size, lr, self.noise_dim,\n",
        "         self.data_dim, layers_dim] = gan_args\n",
        "\n",
        "        self.generator = Generator(self.batch_size).\\\n",
        "            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n",
        "\n",
        "        self.discriminator = Discriminator(self.batch_size).\\\n",
        "            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n",
        "\n",
        "        optimizer = Adam(lr, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.noise_dim,))\n",
        "        record = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(record)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def get_data_batch(self, train, batch_size, seed=0):\n",
        "        # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n",
        "        # np.random.seed(seed)\n",
        "        # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n",
        "        # iterate through shuffled indices, so every sample gets covered evenly\n",
        "\n",
        "        start_i = (batch_size * seed) % len(train)\n",
        "        stop_i = start_i + batch_size\n",
        "        shuffle_seed = (batch_size * seed) // len(train)\n",
        "        np.random.seed(shuffle_seed)\n",
        "        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time\n",
        "        train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set\n",
        "        x = train.loc[train_ix[start_i: stop_i]].values\n",
        "        return np.reshape(x, (batch_size, -1))\n",
        "        \n",
        "    def train(self, data, train_arguments):\n",
        "        [cache_prefix, epochs, sample_interval] = train_arguments\n",
        "        \n",
        "        data_cols = data.columns\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((self.batch_size, 1))\n",
        "        fake = np.zeros((self.batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):    \n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            batch_data = self.get_data_batch(data, self.batch_size)\n",
        "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
        "\n",
        "            # Generate a batch of new images\n",
        "            gen_data = self.generator.predict(noise)\n",
        "    \n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    \n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
        "            # Train the generator (to have the discriminator label samples as valid)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "    \n",
        "            # Plot the progress\n",
        "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "    \n",
        "            # If at save interval => save generated events\n",
        "            if epoch % sample_interval == 0:\n",
        "                #Test here data generation step\n",
        "                # save model checkpoints\n",
        "                model_checkpoint_base_name = 'model/' + cache_prefix + '_{}_model_weights_step_{}.h5'\n",
        "                self.generator.save_weights(model_checkpoint_base_name.format('generator', epoch))\n",
        "                self.discriminator.save_weights(model_checkpoint_base_name.format('discriminator', epoch))\n",
        "\n",
        "                #Here is generating the data\n",
        "                z = tf.random.normal((432, self.noise_dim))\n",
        "                gen_data = self.generator(z)\n",
        "                print('generated_data')\n",
        "\n",
        "    def save(self, path, name):\n",
        "        assert os.path.isdir(path) == True, \\\n",
        "            \"Please provide a valid path. Path must be a directory.\"\n",
        "        model_path = os.path.join(path, name)\n",
        "        self.generator.save_weights(model_path)  # Load the generator\n",
        "        return\n",
        "    \n",
        "    def load(self, path):\n",
        "        assert os.path.isdir(path) == True, \\\n",
        "            \"Please provide a valid path. Path must be a directory.\"\n",
        "        self.generator = Generator(self.batch_size)\n",
        "        self.generator = self.generator.load_weights(path)\n",
        "        return self.generator\n",
        "    \n",
        "class Generator():\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size=batch_size\n",
        "        \n",
        "    def build_model(self, input_shape, dim, data_dim):\n",
        "        input= Input(shape=input_shape, batch_size=self.batch_size)\n",
        "        x = Dense(dim, activation='relu')(input)\n",
        "        x = Dense(dim * 2, activation='relu')(x)\n",
        "        x = Dense(dim * 4, activation='relu')(x)\n",
        "        x = Dense(data_dim)(x)\n",
        "        return Model(inputs=input, outputs=x)\n",
        "\n",
        "class Discriminator():\n",
        "    def __init__(self,batch_size):\n",
        "        self.batch_size=batch_size\n",
        "    \n",
        "    def build_model(self, input_shape, dim):\n",
        "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
        "        x = Dense(dim * 4, activation='relu')(input)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(dim * 2, activation='relu')(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(dim, activation='relu')(x)\n",
        "        x = Dense(1, activation='sigmoid')(x)\n",
        "        return Model(inputs=input, outputs=x)"
      ],
      "metadata": {
        "id": "FH1Eypi3gwMc"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_dim = 32\n",
        "dim = 128\n",
        "batch_size = 32\n",
        "\n",
        "log_step = 100\n",
        "epochs = 5000+1\n",
        "learning_rate = 5e-4\n",
        "models_dir = 'model'\n",
        "\n",
        "\n",
        "\n",
        "gan_args = [batch_size, learning_rate, noise_dim, df.shape[1], dim]\n",
        "train_args = ['', epochs, log_step]"
      ],
      "metadata": {
        "id": "HmgrlXNpknrX"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model"
      ],
      "metadata": {
        "id": "q9ckl_msk0VU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4898c3a9-688e-409c-cbdd-d7b469296afd"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GAN\n",
        "\n",
        "#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n",
        "synthesizer = model(gan_args)\n",
        "synthesizer.train(df,train_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsW4ZJumk4MA",
        "outputId": "c459c6ad-1a07-4cf0-b91a-63758fac429c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "57 [D loss: 0.433833, acc.: 73.44%] [G loss: 0.851864]\n",
            "58 [D loss: 0.428712, acc.: 79.69%] [G loss: 0.978380]\n",
            "59 [D loss: 0.410219, acc.: 90.62%] [G loss: 1.065670]\n",
            "60 [D loss: 0.421979, acc.: 82.81%] [G loss: 1.050456]\n",
            "61 [D loss: 0.379761, acc.: 87.50%] [G loss: 1.107617]\n",
            "62 [D loss: 0.444246, acc.: 84.38%] [G loss: 1.041845]\n",
            "63 [D loss: 0.439537, acc.: 85.94%] [G loss: 0.998508]\n",
            "64 [D loss: 0.397385, acc.: 92.19%] [G loss: 1.057029]\n",
            "65 [D loss: 0.341223, acc.: 93.75%] [G loss: 1.177799]\n",
            "66 [D loss: 0.322706, acc.: 93.75%] [G loss: 1.230767]\n",
            "67 [D loss: 0.402096, acc.: 85.94%] [G loss: 1.059270]\n",
            "68 [D loss: 0.483878, acc.: 56.25%] [G loss: 1.000602]\n",
            "69 [D loss: 0.493831, acc.: 64.06%] [G loss: 0.924951]\n",
            "70 [D loss: 0.446029, acc.: 65.62%] [G loss: 0.892148]\n",
            "71 [D loss: 0.518050, acc.: 50.00%] [G loss: 0.771796]\n",
            "72 [D loss: 0.552323, acc.: 50.00%] [G loss: 0.826037]\n",
            "73 [D loss: 0.537353, acc.: 51.56%] [G loss: 0.905307]\n",
            "74 [D loss: 0.572079, acc.: 48.44%] [G loss: 0.846586]\n",
            "75 [D loss: 0.421702, acc.: 68.75%] [G loss: 1.057349]\n",
            "76 [D loss: 0.354854, acc.: 93.75%] [G loss: 1.267506]\n",
            "77 [D loss: 0.532085, acc.: 60.94%] [G loss: 0.991421]\n",
            "78 [D loss: 0.246374, acc.: 98.44%] [G loss: 1.616839]\n",
            "79 [D loss: 0.551531, acc.: 68.75%] [G loss: 0.907350]\n",
            "80 [D loss: 0.370251, acc.: 75.00%] [G loss: 1.046020]\n",
            "81 [D loss: 0.479272, acc.: 56.25%] [G loss: 0.860224]\n",
            "82 [D loss: 0.485876, acc.: 54.69%] [G loss: 0.839020]\n",
            "83 [D loss: 0.476518, acc.: 57.81%] [G loss: 0.884204]\n",
            "84 [D loss: 0.422699, acc.: 76.56%] [G loss: 0.905171]\n",
            "85 [D loss: 0.405233, acc.: 87.50%] [G loss: 0.976223]\n",
            "86 [D loss: 0.357873, acc.: 93.75%] [G loss: 1.058637]\n",
            "87 [D loss: 0.353898, acc.: 93.75%] [G loss: 1.151762]\n",
            "88 [D loss: 0.339937, acc.: 96.88%] [G loss: 1.276630]\n",
            "89 [D loss: 0.387832, acc.: 84.38%] [G loss: 1.193363]\n",
            "90 [D loss: 0.392885, acc.: 81.25%] [G loss: 1.012493]\n",
            "91 [D loss: 0.424260, acc.: 81.25%] [G loss: 0.892328]\n",
            "92 [D loss: 0.495984, acc.: 59.38%] [G loss: 0.980798]\n",
            "93 [D loss: 0.405705, acc.: 82.81%] [G loss: 1.170220]\n",
            "94 [D loss: 0.392398, acc.: 90.62%] [G loss: 1.209274]\n",
            "95 [D loss: 0.366277, acc.: 85.94%] [G loss: 1.313671]\n",
            "96 [D loss: 0.334676, acc.: 96.88%] [G loss: 1.341162]\n",
            "97 [D loss: 0.393016, acc.: 85.94%] [G loss: 1.211614]\n",
            "98 [D loss: 0.377611, acc.: 87.50%] [G loss: 1.165882]\n",
            "99 [D loss: 0.452329, acc.: 81.25%] [G loss: 1.071846]\n",
            "100 [D loss: 0.351104, acc.: 84.38%] [G loss: 1.100113]\n",
            "generated_data\n",
            "101 [D loss: 0.422367, acc.: 78.12%] [G loss: 1.135680]\n",
            "102 [D loss: 0.404351, acc.: 81.25%] [G loss: 1.297060]\n",
            "103 [D loss: 0.390910, acc.: 87.50%] [G loss: 1.362650]\n",
            "104 [D loss: 0.369167, acc.: 87.50%] [G loss: 1.243302]\n",
            "105 [D loss: 0.506842, acc.: 67.19%] [G loss: 0.881536]\n",
            "106 [D loss: 0.309590, acc.: 90.62%] [G loss: 1.277821]\n",
            "107 [D loss: 0.452595, acc.: 87.50%] [G loss: 0.973469]\n",
            "108 [D loss: 0.279371, acc.: 96.88%] [G loss: 1.355296]\n",
            "109 [D loss: 0.253357, acc.: 95.31%] [G loss: 1.498368]\n",
            "110 [D loss: 0.266017, acc.: 93.75%] [G loss: 1.335870]\n",
            "111 [D loss: 0.377786, acc.: 81.25%] [G loss: 1.474056]\n",
            "112 [D loss: 0.844846, acc.: 50.00%] [G loss: 0.658707]\n",
            "113 [D loss: 0.605973, acc.: 68.75%] [G loss: 0.924212]\n",
            "114 [D loss: 0.544296, acc.: 68.75%] [G loss: 0.914518]\n",
            "115 [D loss: 0.487493, acc.: 71.88%] [G loss: 0.898535]\n",
            "116 [D loss: 0.631006, acc.: 57.81%] [G loss: 0.985945]\n",
            "117 [D loss: 0.566066, acc.: 57.81%] [G loss: 1.037501]\n",
            "118 [D loss: 0.414058, acc.: 79.69%] [G loss: 1.307646]\n",
            "119 [D loss: 0.507756, acc.: 75.00%] [G loss: 1.136981]\n",
            "120 [D loss: 0.446765, acc.: 79.69%] [G loss: 1.159150]\n",
            "121 [D loss: 0.462667, acc.: 76.56%] [G loss: 1.052918]\n",
            "122 [D loss: 0.503888, acc.: 68.75%] [G loss: 1.049514]\n",
            "123 [D loss: 0.537581, acc.: 65.62%] [G loss: 1.021605]\n",
            "124 [D loss: 0.595757, acc.: 57.81%] [G loss: 0.876991]\n",
            "125 [D loss: 0.673346, acc.: 51.56%] [G loss: 0.854150]\n",
            "126 [D loss: 0.639288, acc.: 53.12%] [G loss: 0.998143]\n",
            "127 [D loss: 0.573546, acc.: 68.75%] [G loss: 0.901292]\n",
            "128 [D loss: 0.576920, acc.: 62.50%] [G loss: 0.843031]\n",
            "129 [D loss: 0.677851, acc.: 53.12%] [G loss: 0.827432]\n",
            "130 [D loss: 0.568903, acc.: 62.50%] [G loss: 0.918468]\n",
            "131 [D loss: 0.645467, acc.: 54.69%] [G loss: 0.862947]\n",
            "132 [D loss: 0.606597, acc.: 60.94%] [G loss: 0.745754]\n",
            "133 [D loss: 0.586256, acc.: 67.19%] [G loss: 0.821762]\n",
            "134 [D loss: 0.507251, acc.: 75.00%] [G loss: 0.966161]\n",
            "135 [D loss: 0.652891, acc.: 50.00%] [G loss: 0.910410]\n",
            "136 [D loss: 0.614470, acc.: 68.75%] [G loss: 0.775114]\n",
            "137 [D loss: 0.675146, acc.: 50.00%] [G loss: 0.784910]\n",
            "138 [D loss: 0.672130, acc.: 53.12%] [G loss: 0.793491]\n",
            "139 [D loss: 0.648319, acc.: 45.31%] [G loss: 0.751093]\n",
            "140 [D loss: 0.695342, acc.: 45.31%] [G loss: 0.792223]\n",
            "141 [D loss: 0.676203, acc.: 59.38%] [G loss: 0.859356]\n",
            "142 [D loss: 0.590036, acc.: 65.62%] [G loss: 0.885047]\n",
            "143 [D loss: 0.733191, acc.: 60.94%] [G loss: 0.941969]\n",
            "144 [D loss: 0.651290, acc.: 56.25%] [G loss: 1.078143]\n",
            "145 [D loss: 0.636265, acc.: 71.88%] [G loss: 1.073212]\n",
            "146 [D loss: 0.630046, acc.: 70.31%] [G loss: 0.883068]\n",
            "147 [D loss: 0.725645, acc.: 42.19%] [G loss: 0.856933]\n",
            "148 [D loss: 0.703945, acc.: 57.81%] [G loss: 0.892005]\n",
            "149 [D loss: 0.719667, acc.: 57.81%] [G loss: 0.933020]\n",
            "150 [D loss: 0.750459, acc.: 46.88%] [G loss: 1.123157]\n",
            "151 [D loss: 0.765562, acc.: 43.75%] [G loss: 0.889916]\n",
            "152 [D loss: 0.670199, acc.: 57.81%] [G loss: 0.810858]\n",
            "153 [D loss: 0.709594, acc.: 48.44%] [G loss: 0.881575]\n",
            "154 [D loss: 0.653755, acc.: 62.50%] [G loss: 1.064821]\n",
            "155 [D loss: 0.648003, acc.: 62.50%] [G loss: 0.997631]\n",
            "156 [D loss: 0.645103, acc.: 56.25%] [G loss: 1.009284]\n",
            "157 [D loss: 0.742046, acc.: 53.12%] [G loss: 0.856078]\n",
            "158 [D loss: 0.673829, acc.: 59.38%] [G loss: 0.874032]\n",
            "159 [D loss: 0.666491, acc.: 57.81%] [G loss: 0.871799]\n",
            "160 [D loss: 0.650147, acc.: 62.50%] [G loss: 0.916396]\n",
            "161 [D loss: 0.703217, acc.: 46.88%] [G loss: 0.853785]\n",
            "162 [D loss: 0.775927, acc.: 39.06%] [G loss: 0.857249]\n",
            "163 [D loss: 0.663456, acc.: 57.81%] [G loss: 0.866377]\n",
            "164 [D loss: 0.690898, acc.: 56.25%] [G loss: 0.895566]\n",
            "165 [D loss: 0.669850, acc.: 56.25%] [G loss: 0.943515]\n",
            "166 [D loss: 0.625661, acc.: 64.06%] [G loss: 0.944348]\n",
            "167 [D loss: 0.679239, acc.: 54.69%] [G loss: 0.919675]\n",
            "168 [D loss: 0.692329, acc.: 50.00%] [G loss: 0.930651]\n",
            "169 [D loss: 0.648370, acc.: 59.38%] [G loss: 0.823615]\n",
            "170 [D loss: 0.749381, acc.: 39.06%] [G loss: 0.931700]\n",
            "171 [D loss: 0.683379, acc.: 51.56%] [G loss: 0.961904]\n",
            "172 [D loss: 0.627089, acc.: 62.50%] [G loss: 1.041090]\n",
            "173 [D loss: 0.629154, acc.: 59.38%] [G loss: 1.083880]\n",
            "174 [D loss: 0.666995, acc.: 54.69%] [G loss: 1.020441]\n",
            "175 [D loss: 0.626849, acc.: 53.12%] [G loss: 1.024386]\n",
            "176 [D loss: 0.603443, acc.: 67.19%] [G loss: 1.030348]\n",
            "177 [D loss: 0.701244, acc.: 40.62%] [G loss: 0.934597]\n",
            "178 [D loss: 0.709925, acc.: 48.44%] [G loss: 1.027021]\n",
            "179 [D loss: 0.654998, acc.: 59.38%] [G loss: 1.045151]\n",
            "180 [D loss: 0.670169, acc.: 56.25%] [G loss: 1.082108]\n",
            "181 [D loss: 0.662208, acc.: 57.81%] [G loss: 1.018898]\n",
            "182 [D loss: 0.692836, acc.: 50.00%] [G loss: 0.955470]\n",
            "183 [D loss: 0.739243, acc.: 46.88%] [G loss: 0.907705]\n",
            "184 [D loss: 0.729945, acc.: 50.00%] [G loss: 1.009497]\n",
            "185 [D loss: 0.665803, acc.: 56.25%] [G loss: 1.093588]\n",
            "186 [D loss: 0.665904, acc.: 57.81%] [G loss: 1.002339]\n",
            "187 [D loss: 0.673059, acc.: 53.12%] [G loss: 1.085037]\n",
            "188 [D loss: 0.620916, acc.: 64.06%] [G loss: 1.010154]\n",
            "189 [D loss: 0.675385, acc.: 48.44%] [G loss: 1.067249]\n",
            "190 [D loss: 0.667345, acc.: 57.81%] [G loss: 1.067227]\n",
            "191 [D loss: 0.700120, acc.: 56.25%] [G loss: 1.065014]\n",
            "192 [D loss: 0.662869, acc.: 62.50%] [G loss: 1.013885]\n",
            "193 [D loss: 0.615656, acc.: 68.75%] [G loss: 1.122337]\n",
            "194 [D loss: 0.587286, acc.: 68.75%] [G loss: 1.099670]\n",
            "195 [D loss: 0.550435, acc.: 71.88%] [G loss: 1.135312]\n",
            "196 [D loss: 0.652259, acc.: 54.69%] [G loss: 1.135136]\n",
            "197 [D loss: 0.555430, acc.: 81.25%] [G loss: 1.081626]\n",
            "198 [D loss: 0.609670, acc.: 59.38%] [G loss: 1.043603]\n",
            "199 [D loss: 0.648028, acc.: 57.81%] [G loss: 1.049767]\n",
            "200 [D loss: 0.684699, acc.: 50.00%] [G loss: 1.083839]\n",
            "generated_data\n",
            "201 [D loss: 0.616011, acc.: 60.94%] [G loss: 1.023865]\n",
            "202 [D loss: 0.655696, acc.: 65.62%] [G loss: 0.949897]\n",
            "203 [D loss: 0.567274, acc.: 71.88%] [G loss: 0.969703]\n",
            "204 [D loss: 0.577503, acc.: 67.19%] [G loss: 1.088158]\n",
            "205 [D loss: 0.627905, acc.: 54.69%] [G loss: 1.079099]\n",
            "206 [D loss: 0.722878, acc.: 39.06%] [G loss: 0.901482]\n",
            "207 [D loss: 0.800717, acc.: 29.69%] [G loss: 0.932892]\n",
            "208 [D loss: 0.684789, acc.: 51.56%] [G loss: 0.914353]\n",
            "209 [D loss: 0.644740, acc.: 60.94%] [G loss: 0.957487]\n",
            "210 [D loss: 0.623216, acc.: 68.75%] [G loss: 1.013125]\n",
            "211 [D loss: 0.526715, acc.: 81.25%] [G loss: 1.008018]\n",
            "212 [D loss: 0.445868, acc.: 89.06%] [G loss: 1.237027]\n",
            "213 [D loss: 0.419173, acc.: 90.62%] [G loss: 1.337648]\n",
            "214 [D loss: 0.495840, acc.: 85.94%] [G loss: 1.508475]\n",
            "215 [D loss: 0.526632, acc.: 70.31%] [G loss: 1.541932]\n",
            "216 [D loss: 0.917008, acc.: 42.19%] [G loss: 1.158523]\n",
            "217 [D loss: 0.902218, acc.: 40.62%] [G loss: 0.912817]\n",
            "218 [D loss: 0.877495, acc.: 28.12%] [G loss: 1.023211]\n",
            "219 [D loss: 0.618712, acc.: 62.50%] [G loss: 1.125329]\n",
            "220 [D loss: 0.482078, acc.: 84.38%] [G loss: 1.715843]\n",
            "221 [D loss: 0.393199, acc.: 90.62%] [G loss: 1.603629]\n",
            "222 [D loss: 0.333951, acc.: 93.75%] [G loss: 1.660883]\n",
            "223 [D loss: 0.518467, acc.: 68.75%] [G loss: 1.444982]\n",
            "224 [D loss: 0.700871, acc.: 53.12%] [G loss: 1.095971]\n",
            "225 [D loss: 0.872921, acc.: 32.81%] [G loss: 0.945044]\n",
            "226 [D loss: 0.918997, acc.: 26.56%] [G loss: 0.940065]\n",
            "227 [D loss: 0.734018, acc.: 51.56%] [G loss: 0.947093]\n",
            "228 [D loss: 0.643457, acc.: 57.81%] [G loss: 1.032977]\n",
            "229 [D loss: 0.462316, acc.: 84.38%] [G loss: 1.232150]\n",
            "230 [D loss: 0.454217, acc.: 89.06%] [G loss: 1.462730]\n",
            "231 [D loss: 0.333271, acc.: 90.62%] [G loss: 1.395398]\n",
            "232 [D loss: 0.368768, acc.: 84.38%] [G loss: 1.542919]\n",
            "233 [D loss: 0.370905, acc.: 90.62%] [G loss: 1.495026]\n",
            "234 [D loss: 0.431687, acc.: 84.38%] [G loss: 1.403816]\n",
            "235 [D loss: 0.663905, acc.: 59.38%] [G loss: 1.179657]\n",
            "236 [D loss: 0.883698, acc.: 42.19%] [G loss: 0.951686]\n",
            "237 [D loss: 0.834127, acc.: 35.94%] [G loss: 0.837102]\n",
            "238 [D loss: 0.786253, acc.: 40.62%] [G loss: 0.873533]\n",
            "239 [D loss: 0.676933, acc.: 60.94%] [G loss: 0.937749]\n",
            "240 [D loss: 0.634496, acc.: 67.19%] [G loss: 1.074958]\n",
            "241 [D loss: 0.603981, acc.: 70.31%] [G loss: 1.215823]\n",
            "242 [D loss: 0.586548, acc.: 68.75%] [G loss: 1.197710]\n",
            "243 [D loss: 0.535873, acc.: 81.25%] [G loss: 1.293808]\n",
            "244 [D loss: 0.542490, acc.: 84.38%] [G loss: 1.120093]\n",
            "245 [D loss: 0.475171, acc.: 87.50%] [G loss: 1.088571]\n",
            "246 [D loss: 0.540383, acc.: 78.12%] [G loss: 1.090416]\n",
            "247 [D loss: 0.471692, acc.: 82.81%] [G loss: 1.085317]\n",
            "248 [D loss: 0.467314, acc.: 84.38%] [G loss: 1.118008]\n",
            "249 [D loss: 0.737134, acc.: 48.44%] [G loss: 0.972771]\n",
            "250 [D loss: 0.624897, acc.: 57.81%] [G loss: 0.930113]\n",
            "251 [D loss: 0.702890, acc.: 46.88%] [G loss: 0.789980]\n",
            "252 [D loss: 0.633215, acc.: 60.94%] [G loss: 0.801280]\n",
            "253 [D loss: 0.591627, acc.: 64.06%] [G loss: 1.007257]\n",
            "254 [D loss: 0.637319, acc.: 59.38%] [G loss: 0.938164]\n",
            "255 [D loss: 0.590242, acc.: 67.19%] [G loss: 0.911589]\n",
            "256 [D loss: 0.529059, acc.: 71.88%] [G loss: 0.981134]\n",
            "257 [D loss: 0.533914, acc.: 79.69%] [G loss: 0.955444]\n",
            "258 [D loss: 0.539723, acc.: 68.75%] [G loss: 1.071333]\n",
            "259 [D loss: 0.515232, acc.: 78.12%] [G loss: 1.044109]\n",
            "260 [D loss: 0.509877, acc.: 81.25%] [G loss: 1.095722]\n",
            "261 [D loss: 0.545182, acc.: 73.44%] [G loss: 1.117483]\n",
            "262 [D loss: 0.518521, acc.: 75.00%] [G loss: 1.062305]\n",
            "263 [D loss: 0.518554, acc.: 71.88%] [G loss: 1.033935]\n",
            "264 [D loss: 0.606996, acc.: 59.38%] [G loss: 1.061676]\n",
            "265 [D loss: 0.567183, acc.: 70.31%] [G loss: 1.085477]\n",
            "266 [D loss: 0.513491, acc.: 76.56%] [G loss: 1.249826]\n",
            "267 [D loss: 0.494133, acc.: 79.69%] [G loss: 1.157637]\n",
            "268 [D loss: 0.520339, acc.: 81.25%] [G loss: 1.260213]\n",
            "269 [D loss: 0.629709, acc.: 60.94%] [G loss: 1.011032]\n",
            "270 [D loss: 0.598446, acc.: 65.62%] [G loss: 0.883292]\n",
            "271 [D loss: 0.558210, acc.: 64.06%] [G loss: 0.928574]\n",
            "272 [D loss: 0.508973, acc.: 76.56%] [G loss: 0.926563]\n",
            "273 [D loss: 0.670755, acc.: 65.62%] [G loss: 0.976916]\n",
            "274 [D loss: 0.567455, acc.: 68.75%] [G loss: 0.906016]\n",
            "275 [D loss: 0.545052, acc.: 73.44%] [G loss: 1.035928]\n",
            "276 [D loss: 0.531520, acc.: 75.00%] [G loss: 1.000600]\n",
            "277 [D loss: 0.657640, acc.: 59.38%] [G loss: 0.977549]\n",
            "278 [D loss: 0.631131, acc.: 62.50%] [G loss: 0.950535]\n",
            "279 [D loss: 0.674055, acc.: 54.69%] [G loss: 1.132216]\n",
            "280 [D loss: 0.660716, acc.: 62.50%] [G loss: 1.001642]\n",
            "281 [D loss: 0.642855, acc.: 59.38%] [G loss: 0.875316]\n",
            "282 [D loss: 0.713360, acc.: 56.25%] [G loss: 0.878615]\n",
            "283 [D loss: 0.609736, acc.: 56.25%] [G loss: 1.068613]\n",
            "284 [D loss: 0.563683, acc.: 67.19%] [G loss: 1.262022]\n",
            "285 [D loss: 0.567079, acc.: 68.75%] [G loss: 1.151251]\n",
            "286 [D loss: 0.652174, acc.: 46.88%] [G loss: 1.002122]\n",
            "287 [D loss: 0.680683, acc.: 45.31%] [G loss: 0.964159]\n",
            "288 [D loss: 0.696965, acc.: 43.75%] [G loss: 0.989264]\n",
            "289 [D loss: 0.674893, acc.: 56.25%] [G loss: 0.931706]\n",
            "290 [D loss: 0.614206, acc.: 59.38%] [G loss: 0.998570]\n",
            "291 [D loss: 0.543149, acc.: 76.56%] [G loss: 1.007232]\n",
            "292 [D loss: 0.483138, acc.: 87.50%] [G loss: 1.017616]\n",
            "293 [D loss: 0.527579, acc.: 82.81%] [G loss: 1.167357]\n",
            "294 [D loss: 0.479224, acc.: 81.25%] [G loss: 1.130538]\n",
            "295 [D loss: 0.582145, acc.: 60.94%] [G loss: 1.110700]\n",
            "296 [D loss: 0.609662, acc.: 75.00%] [G loss: 1.132375]\n",
            "297 [D loss: 0.744443, acc.: 43.75%] [G loss: 0.984236]\n",
            "298 [D loss: 0.751866, acc.: 45.31%] [G loss: 1.039104]\n",
            "299 [D loss: 0.559963, acc.: 73.44%] [G loss: 1.034558]\n",
            "300 [D loss: 0.512176, acc.: 76.56%] [G loss: 1.231375]\n",
            "generated_data\n",
            "301 [D loss: 0.464654, acc.: 79.69%] [G loss: 1.428821]\n",
            "302 [D loss: 0.517457, acc.: 70.31%] [G loss: 1.415604]\n",
            "303 [D loss: 0.761160, acc.: 45.31%] [G loss: 1.081066]\n",
            "304 [D loss: 0.715585, acc.: 50.00%] [G loss: 1.006276]\n",
            "305 [D loss: 0.840064, acc.: 39.06%] [G loss: 1.052622]\n",
            "306 [D loss: 0.675516, acc.: 54.69%] [G loss: 1.081407]\n",
            "307 [D loss: 0.640201, acc.: 56.25%] [G loss: 1.040159]\n",
            "308 [D loss: 0.569570, acc.: 73.44%] [G loss: 1.090868]\n",
            "309 [D loss: 0.575687, acc.: 81.25%] [G loss: 1.019658]\n",
            "310 [D loss: 0.625251, acc.: 67.19%] [G loss: 0.940299]\n",
            "311 [D loss: 0.655381, acc.: 60.94%] [G loss: 0.923634]\n",
            "312 [D loss: 0.686539, acc.: 50.00%] [G loss: 0.895322]\n",
            "313 [D loss: 0.678586, acc.: 56.25%] [G loss: 0.863322]\n",
            "314 [D loss: 0.640835, acc.: 60.94%] [G loss: 0.891951]\n",
            "315 [D loss: 0.611186, acc.: 64.06%] [G loss: 0.920899]\n",
            "316 [D loss: 0.627859, acc.: 62.50%] [G loss: 0.956195]\n",
            "317 [D loss: 0.622446, acc.: 64.06%] [G loss: 1.027675]\n",
            "318 [D loss: 0.630346, acc.: 54.69%] [G loss: 0.984466]\n",
            "319 [D loss: 0.598917, acc.: 60.94%] [G loss: 0.924300]\n",
            "320 [D loss: 0.611016, acc.: 64.06%] [G loss: 1.015416]\n",
            "321 [D loss: 0.612867, acc.: 59.38%] [G loss: 0.987456]\n",
            "322 [D loss: 0.593889, acc.: 65.62%] [G loss: 1.017166]\n",
            "323 [D loss: 0.601088, acc.: 62.50%] [G loss: 0.990410]\n",
            "324 [D loss: 0.603449, acc.: 65.62%] [G loss: 0.860254]\n",
            "325 [D loss: 0.620156, acc.: 62.50%] [G loss: 0.815186]\n",
            "326 [D loss: 0.569273, acc.: 67.19%] [G loss: 0.871710]\n",
            "327 [D loss: 0.551819, acc.: 73.44%] [G loss: 0.979086]\n",
            "328 [D loss: 0.615089, acc.: 62.50%] [G loss: 1.070354]\n",
            "329 [D loss: 0.592421, acc.: 67.19%] [G loss: 1.006303]\n",
            "330 [D loss: 0.583052, acc.: 64.06%] [G loss: 1.062139]\n",
            "331 [D loss: 0.563368, acc.: 71.88%] [G loss: 0.997424]\n",
            "332 [D loss: 0.561067, acc.: 73.44%] [G loss: 0.972732]\n",
            "333 [D loss: 0.588388, acc.: 62.50%] [G loss: 0.974403]\n",
            "334 [D loss: 0.621515, acc.: 62.50%] [G loss: 0.991869]\n",
            "335 [D loss: 0.542170, acc.: 71.88%] [G loss: 0.982144]\n",
            "336 [D loss: 0.583178, acc.: 53.12%] [G loss: 1.056346]\n",
            "337 [D loss: 0.564608, acc.: 67.19%] [G loss: 1.027357]\n",
            "338 [D loss: 0.592598, acc.: 62.50%] [G loss: 1.098524]\n",
            "339 [D loss: 0.642959, acc.: 50.00%] [G loss: 0.967838]\n",
            "340 [D loss: 0.582433, acc.: 68.75%] [G loss: 0.936526]\n",
            "341 [D loss: 0.703708, acc.: 50.00%] [G loss: 1.065393]\n",
            "342 [D loss: 0.588784, acc.: 68.75%] [G loss: 1.024664]\n",
            "343 [D loss: 0.619340, acc.: 60.94%] [G loss: 1.031721]\n",
            "344 [D loss: 0.654841, acc.: 60.94%] [G loss: 0.941356]\n",
            "345 [D loss: 0.629750, acc.: 53.12%] [G loss: 0.917612]\n",
            "346 [D loss: 0.651797, acc.: 56.25%] [G loss: 1.098122]\n",
            "347 [D loss: 0.606193, acc.: 64.06%] [G loss: 1.270207]\n",
            "348 [D loss: 0.577754, acc.: 62.50%] [G loss: 1.274769]\n",
            "349 [D loss: 0.519529, acc.: 73.44%] [G loss: 1.227988]\n",
            "350 [D loss: 0.480183, acc.: 79.69%] [G loss: 1.363474]\n",
            "351 [D loss: 0.583333, acc.: 65.62%] [G loss: 1.264108]\n",
            "352 [D loss: 0.689321, acc.: 57.81%] [G loss: 1.100304]\n",
            "353 [D loss: 0.595329, acc.: 65.62%] [G loss: 1.100973]\n",
            "354 [D loss: 0.582433, acc.: 65.62%] [G loss: 1.108344]\n",
            "355 [D loss: 0.554083, acc.: 68.75%] [G loss: 1.004622]\n",
            "356 [D loss: 0.562217, acc.: 65.62%] [G loss: 1.062753]\n",
            "357 [D loss: 0.612749, acc.: 64.06%] [G loss: 1.032838]\n",
            "358 [D loss: 0.545043, acc.: 68.75%] [G loss: 1.049301]\n",
            "359 [D loss: 0.619337, acc.: 59.38%] [G loss: 0.977226]\n",
            "360 [D loss: 0.608826, acc.: 60.94%] [G loss: 0.956143]\n",
            "361 [D loss: 0.654098, acc.: 60.94%] [G loss: 0.983383]\n",
            "362 [D loss: 0.641756, acc.: 54.69%] [G loss: 0.931381]\n",
            "363 [D loss: 0.670391, acc.: 48.44%] [G loss: 0.858487]\n",
            "364 [D loss: 0.633986, acc.: 53.12%] [G loss: 0.870462]\n",
            "365 [D loss: 0.616349, acc.: 56.25%] [G loss: 0.912050]\n",
            "366 [D loss: 0.565879, acc.: 64.06%] [G loss: 1.061186]\n",
            "367 [D loss: 0.592805, acc.: 64.06%] [G loss: 1.062668]\n",
            "368 [D loss: 0.591505, acc.: 62.50%] [G loss: 1.131467]\n",
            "369 [D loss: 0.621946, acc.: 57.81%] [G loss: 0.989477]\n",
            "370 [D loss: 0.574677, acc.: 64.06%] [G loss: 1.021428]\n",
            "371 [D loss: 0.596796, acc.: 62.50%] [G loss: 0.935086]\n",
            "372 [D loss: 0.598459, acc.: 70.31%] [G loss: 0.906169]\n",
            "373 [D loss: 0.595639, acc.: 65.62%] [G loss: 0.930600]\n",
            "374 [D loss: 0.570367, acc.: 65.62%] [G loss: 1.057459]\n",
            "375 [D loss: 0.551150, acc.: 68.75%] [G loss: 1.082791]\n",
            "376 [D loss: 0.567474, acc.: 73.44%] [G loss: 0.957569]\n",
            "377 [D loss: 0.554078, acc.: 67.19%] [G loss: 0.910244]\n",
            "378 [D loss: 0.567653, acc.: 68.75%] [G loss: 0.952236]\n",
            "379 [D loss: 0.605757, acc.: 70.31%] [G loss: 0.940501]\n",
            "380 [D loss: 0.554897, acc.: 71.88%] [G loss: 0.993595]\n",
            "381 [D loss: 0.593069, acc.: 62.50%] [G loss: 1.050083]\n",
            "382 [D loss: 0.620705, acc.: 60.94%] [G loss: 1.034662]\n",
            "383 [D loss: 0.566246, acc.: 67.19%] [G loss: 1.067750]\n",
            "384 [D loss: 0.563410, acc.: 68.75%] [G loss: 1.186633]\n",
            "385 [D loss: 0.571450, acc.: 62.50%] [G loss: 1.108308]\n",
            "386 [D loss: 0.564712, acc.: 65.62%] [G loss: 1.032220]\n",
            "387 [D loss: 0.588479, acc.: 68.75%] [G loss: 1.067564]\n",
            "388 [D loss: 0.600596, acc.: 67.19%] [G loss: 1.004647]\n",
            "389 [D loss: 0.555312, acc.: 68.75%] [G loss: 1.017491]\n",
            "390 [D loss: 0.576770, acc.: 70.31%] [G loss: 0.964988]\n",
            "391 [D loss: 0.541953, acc.: 73.44%] [G loss: 0.991056]\n",
            "392 [D loss: 0.533075, acc.: 71.88%] [G loss: 1.017918]\n",
            "393 [D loss: 0.561649, acc.: 68.75%] [G loss: 0.955657]\n",
            "394 [D loss: 0.630895, acc.: 54.69%] [G loss: 0.915170]\n",
            "395 [D loss: 0.593305, acc.: 60.94%] [G loss: 0.910976]\n",
            "396 [D loss: 0.592231, acc.: 67.19%] [G loss: 1.068764]\n",
            "397 [D loss: 0.616841, acc.: 57.81%] [G loss: 1.110643]\n",
            "398 [D loss: 0.582644, acc.: 65.62%] [G loss: 1.009754]\n",
            "399 [D loss: 0.565656, acc.: 67.19%] [G loss: 1.129438]\n",
            "400 [D loss: 0.595801, acc.: 62.50%] [G loss: 1.054703]\n",
            "generated_data\n",
            "401 [D loss: 0.572899, acc.: 64.06%] [G loss: 1.018462]\n",
            "402 [D loss: 0.599668, acc.: 57.81%] [G loss: 0.992332]\n",
            "403 [D loss: 0.643090, acc.: 54.69%] [G loss: 1.069087]\n",
            "404 [D loss: 0.579017, acc.: 67.19%] [G loss: 1.186275]\n",
            "405 [D loss: 0.574925, acc.: 62.50%] [G loss: 1.031399]\n",
            "406 [D loss: 0.571675, acc.: 64.06%] [G loss: 1.092023]\n",
            "407 [D loss: 0.566532, acc.: 68.75%] [G loss: 1.095333]\n",
            "408 [D loss: 0.579720, acc.: 64.06%] [G loss: 1.010211]\n",
            "409 [D loss: 0.586544, acc.: 60.94%] [G loss: 1.055656]\n",
            "410 [D loss: 0.558072, acc.: 70.31%] [G loss: 0.979433]\n",
            "411 [D loss: 0.570144, acc.: 68.75%] [G loss: 0.970460]\n",
            "412 [D loss: 0.560936, acc.: 68.75%] [G loss: 0.866771]\n",
            "413 [D loss: 0.555704, acc.: 70.31%] [G loss: 0.939994]\n",
            "414 [D loss: 0.528780, acc.: 71.88%] [G loss: 1.000320]\n",
            "415 [D loss: 0.506483, acc.: 73.44%] [G loss: 1.050942]\n",
            "416 [D loss: 0.555380, acc.: 64.06%] [G loss: 1.035613]\n",
            "417 [D loss: 0.558793, acc.: 64.06%] [G loss: 1.174493]\n",
            "418 [D loss: 0.554425, acc.: 70.31%] [G loss: 1.066187]\n",
            "419 [D loss: 0.706455, acc.: 54.69%] [G loss: 1.179386]\n",
            "420 [D loss: 0.583377, acc.: 57.81%] [G loss: 1.158204]\n",
            "421 [D loss: 0.630110, acc.: 70.31%] [G loss: 1.195527]\n",
            "422 [D loss: 0.611867, acc.: 64.06%] [G loss: 1.046850]\n",
            "423 [D loss: 0.589083, acc.: 67.19%] [G loss: 1.016041]\n",
            "424 [D loss: 0.599709, acc.: 62.50%] [G loss: 1.040189]\n",
            "425 [D loss: 0.551801, acc.: 70.31%] [G loss: 0.986089]\n",
            "426 [D loss: 0.557252, acc.: 68.75%] [G loss: 0.972719]\n",
            "427 [D loss: 0.539184, acc.: 67.19%] [G loss: 1.040315]\n",
            "428 [D loss: 0.563539, acc.: 64.06%] [G loss: 1.060921]\n",
            "429 [D loss: 0.563447, acc.: 70.31%] [G loss: 1.098573]\n",
            "430 [D loss: 0.597768, acc.: 64.06%] [G loss: 1.046957]\n",
            "431 [D loss: 0.596146, acc.: 70.31%] [G loss: 1.039613]\n",
            "432 [D loss: 0.630103, acc.: 65.62%] [G loss: 0.975669]\n",
            "433 [D loss: 0.631354, acc.: 62.50%] [G loss: 1.027846]\n",
            "434 [D loss: 0.554971, acc.: 64.06%] [G loss: 1.175517]\n",
            "435 [D loss: 0.524103, acc.: 65.62%] [G loss: 1.135995]\n",
            "436 [D loss: 0.543692, acc.: 67.19%] [G loss: 1.158490]\n",
            "437 [D loss: 0.584428, acc.: 62.50%] [G loss: 1.077356]\n",
            "438 [D loss: 0.545868, acc.: 73.44%] [G loss: 1.055439]\n",
            "439 [D loss: 0.545802, acc.: 68.75%] [G loss: 1.194503]\n",
            "440 [D loss: 0.554204, acc.: 71.88%] [G loss: 1.151590]\n",
            "441 [D loss: 0.488219, acc.: 81.25%] [G loss: 1.164837]\n",
            "442 [D loss: 0.556214, acc.: 78.12%] [G loss: 1.104218]\n",
            "443 [D loss: 0.526031, acc.: 76.56%] [G loss: 1.164916]\n",
            "444 [D loss: 0.564752, acc.: 70.31%] [G loss: 0.999061]\n",
            "445 [D loss: 0.524855, acc.: 75.00%] [G loss: 0.921181]\n",
            "446 [D loss: 0.575627, acc.: 59.38%] [G loss: 0.876114]\n",
            "447 [D loss: 0.590666, acc.: 64.06%] [G loss: 0.940462]\n",
            "448 [D loss: 0.549575, acc.: 68.75%] [G loss: 0.982931]\n",
            "449 [D loss: 0.552314, acc.: 76.56%] [G loss: 0.947825]\n",
            "450 [D loss: 0.538929, acc.: 73.44%] [G loss: 1.001843]\n",
            "451 [D loss: 0.505179, acc.: 76.56%] [G loss: 1.009281]\n",
            "452 [D loss: 0.513860, acc.: 76.56%] [G loss: 1.010711]\n",
            "453 [D loss: 0.536199, acc.: 73.44%] [G loss: 1.133634]\n",
            "454 [D loss: 0.609175, acc.: 60.94%] [G loss: 0.956253]\n",
            "455 [D loss: 0.640198, acc.: 53.12%] [G loss: 0.943917]\n",
            "456 [D loss: 0.675819, acc.: 53.12%] [G loss: 1.050524]\n",
            "457 [D loss: 0.556656, acc.: 64.06%] [G loss: 1.280113]\n",
            "458 [D loss: 0.527236, acc.: 68.75%] [G loss: 1.259545]\n",
            "459 [D loss: 0.559456, acc.: 67.19%] [G loss: 1.150337]\n",
            "460 [D loss: 0.577192, acc.: 64.06%] [G loss: 1.049269]\n",
            "461 [D loss: 0.549832, acc.: 73.44%] [G loss: 1.161468]\n",
            "462 [D loss: 0.692164, acc.: 42.19%] [G loss: 0.872516]\n",
            "463 [D loss: 0.620488, acc.: 48.44%] [G loss: 0.911880]\n",
            "464 [D loss: 0.566502, acc.: 70.31%] [G loss: 0.964365]\n",
            "465 [D loss: 0.533362, acc.: 68.75%] [G loss: 0.964647]\n",
            "466 [D loss: 0.513563, acc.: 78.12%] [G loss: 1.039683]\n",
            "467 [D loss: 0.464849, acc.: 87.50%] [G loss: 1.241386]\n",
            "468 [D loss: 0.443234, acc.: 89.06%] [G loss: 1.419080]\n",
            "469 [D loss: 0.429953, acc.: 89.06%] [G loss: 1.397242]\n",
            "470 [D loss: 0.498913, acc.: 78.12%] [G loss: 1.321833]\n",
            "471 [D loss: 0.487189, acc.: 81.25%] [G loss: 1.132166]\n",
            "472 [D loss: 0.771388, acc.: 53.12%] [G loss: 0.867805]\n",
            "473 [D loss: 0.730849, acc.: 54.69%] [G loss: 0.682161]\n",
            "474 [D loss: 0.677275, acc.: 50.00%] [G loss: 0.909225]\n",
            "475 [D loss: 0.594771, acc.: 67.19%] [G loss: 1.148359]\n",
            "476 [D loss: 0.534718, acc.: 65.62%] [G loss: 1.303825]\n",
            "477 [D loss: 0.587090, acc.: 67.19%] [G loss: 1.093449]\n",
            "478 [D loss: 0.627832, acc.: 59.38%] [G loss: 0.941358]\n",
            "479 [D loss: 0.583014, acc.: 64.06%] [G loss: 0.914760]\n",
            "480 [D loss: 0.566871, acc.: 70.31%] [G loss: 0.937899]\n",
            "481 [D loss: 0.563510, acc.: 65.62%] [G loss: 0.901146]\n",
            "482 [D loss: 0.576420, acc.: 62.50%] [G loss: 0.898637]\n",
            "483 [D loss: 0.583349, acc.: 59.38%] [G loss: 0.855114]\n",
            "484 [D loss: 0.590410, acc.: 67.19%] [G loss: 0.880760]\n",
            "485 [D loss: 0.613192, acc.: 54.69%] [G loss: 0.872062]\n",
            "486 [D loss: 0.568504, acc.: 64.06%] [G loss: 0.938151]\n",
            "487 [D loss: 0.569157, acc.: 68.75%] [G loss: 1.001077]\n",
            "488 [D loss: 0.554571, acc.: 67.19%] [G loss: 1.098969]\n",
            "489 [D loss: 0.567721, acc.: 65.62%] [G loss: 0.994655]\n",
            "490 [D loss: 0.625371, acc.: 57.81%] [G loss: 0.878574]\n",
            "491 [D loss: 0.659745, acc.: 56.25%] [G loss: 0.934135]\n",
            "492 [D loss: 0.567153, acc.: 68.75%] [G loss: 0.922562]\n",
            "493 [D loss: 0.552307, acc.: 75.00%] [G loss: 0.900292]\n",
            "494 [D loss: 0.600121, acc.: 64.06%] [G loss: 0.923520]\n",
            "495 [D loss: 0.560866, acc.: 64.06%] [G loss: 0.990789]\n",
            "496 [D loss: 0.570560, acc.: 68.75%] [G loss: 0.988658]\n",
            "497 [D loss: 0.636142, acc.: 62.50%] [G loss: 0.932337]\n",
            "498 [D loss: 0.624663, acc.: 60.94%] [G loss: 0.948360]\n",
            "499 [D loss: 0.592235, acc.: 59.38%] [G loss: 1.037090]\n",
            "500 [D loss: 0.555471, acc.: 68.75%] [G loss: 1.059755]\n",
            "generated_data\n",
            "501 [D loss: 0.603889, acc.: 64.06%] [G loss: 1.078268]\n",
            "502 [D loss: 0.611420, acc.: 64.06%] [G loss: 1.004694]\n",
            "503 [D loss: 0.612165, acc.: 59.38%] [G loss: 0.933968]\n",
            "504 [D loss: 0.606419, acc.: 60.94%] [G loss: 0.977987]\n",
            "505 [D loss: 0.681319, acc.: 57.81%] [G loss: 0.857131]\n",
            "506 [D loss: 0.575515, acc.: 67.19%] [G loss: 0.882454]\n",
            "507 [D loss: 0.631382, acc.: 60.94%] [G loss: 0.959910]\n",
            "508 [D loss: 0.579903, acc.: 70.31%] [G loss: 1.058493]\n",
            "509 [D loss: 0.623344, acc.: 57.81%] [G loss: 0.913650]\n",
            "510 [D loss: 0.600382, acc.: 62.50%] [G loss: 0.885414]\n",
            "511 [D loss: 0.615568, acc.: 60.94%] [G loss: 0.880122]\n",
            "512 [D loss: 0.656152, acc.: 50.00%] [G loss: 0.906919]\n",
            "513 [D loss: 0.623287, acc.: 59.38%] [G loss: 0.821994]\n",
            "514 [D loss: 0.625238, acc.: 54.69%] [G loss: 0.876733]\n",
            "515 [D loss: 0.635258, acc.: 65.62%] [G loss: 1.111150]\n",
            "516 [D loss: 0.588385, acc.: 68.75%] [G loss: 0.954126]\n",
            "517 [D loss: 0.620076, acc.: 64.06%] [G loss: 0.841232]\n",
            "518 [D loss: 0.632431, acc.: 59.38%] [G loss: 0.814931]\n",
            "519 [D loss: 0.621781, acc.: 59.38%] [G loss: 0.909292]\n",
            "520 [D loss: 0.635630, acc.: 60.94%] [G loss: 0.884266]\n",
            "521 [D loss: 0.634793, acc.: 64.06%] [G loss: 0.899037]\n",
            "522 [D loss: 0.621312, acc.: 57.81%] [G loss: 0.901860]\n",
            "523 [D loss: 0.617798, acc.: 59.38%] [G loss: 0.854790]\n",
            "524 [D loss: 0.644361, acc.: 60.94%] [G loss: 0.914523]\n",
            "525 [D loss: 0.669218, acc.: 57.81%] [G loss: 0.863343]\n",
            "526 [D loss: 0.631018, acc.: 59.38%] [G loss: 0.836962]\n",
            "527 [D loss: 0.672244, acc.: 57.81%] [G loss: 0.908606]\n",
            "528 [D loss: 0.739982, acc.: 53.12%] [G loss: 0.834121]\n",
            "529 [D loss: 0.714441, acc.: 51.56%] [G loss: 0.910288]\n",
            "530 [D loss: 0.689866, acc.: 53.12%] [G loss: 0.988377]\n",
            "531 [D loss: 0.641984, acc.: 59.38%] [G loss: 1.089399]\n",
            "532 [D loss: 0.638148, acc.: 60.94%] [G loss: 0.887371]\n",
            "533 [D loss: 0.659083, acc.: 59.38%] [G loss: 0.803089]\n",
            "534 [D loss: 0.714002, acc.: 42.19%] [G loss: 0.992661]\n",
            "535 [D loss: 0.614302, acc.: 60.94%] [G loss: 0.940643]\n",
            "536 [D loss: 0.771617, acc.: 51.56%] [G loss: 0.906793]\n",
            "537 [D loss: 0.629857, acc.: 56.25%] [G loss: 1.036920]\n",
            "538 [D loss: 0.659687, acc.: 54.69%] [G loss: 0.990919]\n",
            "539 [D loss: 0.650461, acc.: 57.81%] [G loss: 1.025863]\n",
            "540 [D loss: 0.613786, acc.: 60.94%] [G loss: 1.065964]\n",
            "541 [D loss: 0.669764, acc.: 57.81%] [G loss: 1.034695]\n",
            "542 [D loss: 0.657724, acc.: 54.69%] [G loss: 1.087454]\n",
            "543 [D loss: 0.647811, acc.: 62.50%] [G loss: 0.950746]\n",
            "544 [D loss: 0.673420, acc.: 51.56%] [G loss: 0.987983]\n",
            "545 [D loss: 0.635799, acc.: 59.38%] [G loss: 0.965688]\n",
            "546 [D loss: 0.639380, acc.: 59.38%] [G loss: 1.017133]\n",
            "547 [D loss: 0.673294, acc.: 53.12%] [G loss: 0.962518]\n",
            "548 [D loss: 0.649085, acc.: 56.25%] [G loss: 1.020719]\n",
            "549 [D loss: 0.641018, acc.: 56.25%] [G loss: 0.958140]\n",
            "550 [D loss: 0.634922, acc.: 62.50%] [G loss: 1.003430]\n",
            "551 [D loss: 0.628830, acc.: 62.50%] [G loss: 0.894462]\n",
            "552 [D loss: 0.644124, acc.: 57.81%] [G loss: 0.876115]\n",
            "553 [D loss: 0.645812, acc.: 54.69%] [G loss: 0.951230]\n",
            "554 [D loss: 0.660320, acc.: 56.25%] [G loss: 0.952703]\n",
            "555 [D loss: 0.653245, acc.: 54.69%] [G loss: 1.045509]\n",
            "556 [D loss: 0.622940, acc.: 64.06%] [G loss: 1.026003]\n",
            "557 [D loss: 0.638231, acc.: 59.38%] [G loss: 0.879418]\n",
            "558 [D loss: 0.622360, acc.: 56.25%] [G loss: 0.876617]\n",
            "559 [D loss: 0.627873, acc.: 57.81%] [G loss: 0.896751]\n",
            "560 [D loss: 0.640656, acc.: 57.81%] [G loss: 0.930116]\n",
            "561 [D loss: 0.635824, acc.: 60.94%] [G loss: 1.005925]\n",
            "562 [D loss: 0.631399, acc.: 60.94%] [G loss: 1.106268]\n",
            "563 [D loss: 0.633570, acc.: 64.06%] [G loss: 0.968695]\n",
            "564 [D loss: 0.618721, acc.: 62.50%] [G loss: 0.912819]\n",
            "565 [D loss: 0.593889, acc.: 65.62%] [G loss: 0.994016]\n",
            "566 [D loss: 0.594196, acc.: 67.19%] [G loss: 0.975602]\n",
            "567 [D loss: 0.618238, acc.: 60.94%] [G loss: 0.919489]\n",
            "568 [D loss: 0.591429, acc.: 64.06%] [G loss: 0.870866]\n",
            "569 [D loss: 0.626384, acc.: 56.25%] [G loss: 0.947468]\n",
            "570 [D loss: 0.601646, acc.: 64.06%] [G loss: 0.916467]\n",
            "571 [D loss: 0.699243, acc.: 57.81%] [G loss: 0.965834]\n",
            "572 [D loss: 0.645188, acc.: 64.06%] [G loss: 0.883157]\n",
            "573 [D loss: 0.676595, acc.: 51.56%] [G loss: 1.082593]\n",
            "574 [D loss: 0.621180, acc.: 62.50%] [G loss: 1.001142]\n",
            "575 [D loss: 0.709738, acc.: 54.69%] [G loss: 1.035957]\n",
            "576 [D loss: 0.644377, acc.: 57.81%] [G loss: 1.036719]\n",
            "577 [D loss: 0.643372, acc.: 56.25%] [G loss: 0.962400]\n",
            "578 [D loss: 0.612250, acc.: 65.62%] [G loss: 0.937191]\n",
            "579 [D loss: 0.606198, acc.: 60.94%] [G loss: 0.836742]\n",
            "580 [D loss: 0.636879, acc.: 57.81%] [G loss: 0.958715]\n",
            "581 [D loss: 0.708155, acc.: 50.00%] [G loss: 0.908249]\n",
            "582 [D loss: 0.638093, acc.: 60.94%] [G loss: 0.852891]\n",
            "583 [D loss: 0.616715, acc.: 56.25%] [G loss: 1.030696]\n",
            "584 [D loss: 0.711540, acc.: 48.44%] [G loss: 0.887816]\n",
            "585 [D loss: 0.651478, acc.: 50.00%] [G loss: 0.984685]\n",
            "586 [D loss: 0.618028, acc.: 59.38%] [G loss: 0.896368]\n",
            "587 [D loss: 0.626344, acc.: 57.81%] [G loss: 0.964154]\n",
            "588 [D loss: 0.642887, acc.: 54.69%] [G loss: 0.911803]\n",
            "589 [D loss: 0.670842, acc.: 54.69%] [G loss: 0.877944]\n",
            "590 [D loss: 0.655265, acc.: 59.38%] [G loss: 0.934714]\n",
            "591 [D loss: 0.664646, acc.: 57.81%] [G loss: 0.959971]\n",
            "592 [D loss: 0.635273, acc.: 56.25%] [G loss: 0.925319]\n",
            "593 [D loss: 0.628126, acc.: 56.25%] [G loss: 0.924137]\n",
            "594 [D loss: 0.638420, acc.: 56.25%] [G loss: 0.910596]\n",
            "595 [D loss: 0.647158, acc.: 53.12%] [G loss: 0.949377]\n",
            "596 [D loss: 0.641950, acc.: 57.81%] [G loss: 0.890973]\n",
            "597 [D loss: 0.618537, acc.: 60.94%] [G loss: 0.922365]\n",
            "598 [D loss: 0.683679, acc.: 57.81%] [G loss: 0.925447]\n",
            "599 [D loss: 0.657601, acc.: 56.25%] [G loss: 0.908535]\n",
            "600 [D loss: 0.714603, acc.: 56.25%] [G loss: 0.946960]\n",
            "generated_data\n",
            "601 [D loss: 0.609917, acc.: 60.94%] [G loss: 1.108337]\n",
            "602 [D loss: 0.628923, acc.: 56.25%] [G loss: 0.993754]\n",
            "603 [D loss: 0.622835, acc.: 56.25%] [G loss: 0.934979]\n",
            "604 [D loss: 0.627481, acc.: 57.81%] [G loss: 0.948311]\n",
            "605 [D loss: 0.627392, acc.: 65.62%] [G loss: 0.947816]\n",
            "606 [D loss: 0.633337, acc.: 59.38%] [G loss: 0.981851]\n",
            "607 [D loss: 0.622620, acc.: 59.38%] [G loss: 0.920489]\n",
            "608 [D loss: 0.651353, acc.: 56.25%] [G loss: 0.992287]\n",
            "609 [D loss: 0.616387, acc.: 57.81%] [G loss: 1.011976]\n",
            "610 [D loss: 0.600565, acc.: 59.38%] [G loss: 0.928176]\n",
            "611 [D loss: 0.674721, acc.: 53.12%] [G loss: 0.991734]\n",
            "612 [D loss: 0.665337, acc.: 53.12%] [G loss: 0.957706]\n",
            "613 [D loss: 0.637258, acc.: 53.12%] [G loss: 1.061516]\n",
            "614 [D loss: 0.660797, acc.: 57.81%] [G loss: 1.058998]\n",
            "615 [D loss: 0.679108, acc.: 48.44%] [G loss: 0.962467]\n",
            "616 [D loss: 0.661846, acc.: 59.38%] [G loss: 0.943526]\n",
            "617 [D loss: 0.616206, acc.: 60.94%] [G loss: 0.921758]\n",
            "618 [D loss: 0.653710, acc.: 57.81%] [G loss: 0.844759]\n",
            "619 [D loss: 0.694174, acc.: 46.88%] [G loss: 0.939474]\n",
            "620 [D loss: 0.634210, acc.: 51.56%] [G loss: 0.953336]\n",
            "621 [D loss: 0.640241, acc.: 56.25%] [G loss: 0.910633]\n",
            "622 [D loss: 0.630875, acc.: 56.25%] [G loss: 0.986583]\n",
            "623 [D loss: 0.629729, acc.: 56.25%] [G loss: 0.911755]\n",
            "624 [D loss: 0.656050, acc.: 56.25%] [G loss: 0.914050]\n",
            "625 [D loss: 0.616321, acc.: 59.38%] [G loss: 0.977329]\n",
            "626 [D loss: 0.644707, acc.: 54.69%] [G loss: 0.920075]\n",
            "627 [D loss: 0.650371, acc.: 53.12%] [G loss: 0.930997]\n",
            "628 [D loss: 0.613647, acc.: 62.50%] [G loss: 0.993729]\n",
            "629 [D loss: 0.623247, acc.: 59.38%] [G loss: 0.853150]\n",
            "630 [D loss: 0.659159, acc.: 54.69%] [G loss: 0.943858]\n",
            "631 [D loss: 0.654468, acc.: 53.12%] [G loss: 0.938203]\n",
            "632 [D loss: 0.680796, acc.: 48.44%] [G loss: 0.873424]\n",
            "633 [D loss: 0.626749, acc.: 60.94%] [G loss: 0.945938]\n",
            "634 [D loss: 0.640941, acc.: 57.81%] [G loss: 1.011136]\n",
            "635 [D loss: 0.636842, acc.: 54.69%] [G loss: 1.021623]\n",
            "636 [D loss: 0.618821, acc.: 60.94%] [G loss: 0.998263]\n",
            "637 [D loss: 0.659255, acc.: 54.69%] [G loss: 0.955637]\n",
            "638 [D loss: 0.681924, acc.: 53.12%] [G loss: 0.931860]\n",
            "639 [D loss: 0.633207, acc.: 54.69%] [G loss: 0.971270]\n",
            "640 [D loss: 0.650967, acc.: 56.25%] [G loss: 0.914553]\n",
            "641 [D loss: 0.639745, acc.: 54.69%] [G loss: 0.998243]\n",
            "642 [D loss: 0.678797, acc.: 53.12%] [G loss: 1.003968]\n",
            "643 [D loss: 0.641741, acc.: 57.81%] [G loss: 0.915098]\n",
            "644 [D loss: 0.622614, acc.: 57.81%] [G loss: 0.931086]\n",
            "645 [D loss: 0.632571, acc.: 59.38%] [G loss: 0.920211]\n",
            "646 [D loss: 0.616044, acc.: 62.50%] [G loss: 0.994189]\n",
            "647 [D loss: 0.613275, acc.: 57.81%] [G loss: 0.825942]\n",
            "648 [D loss: 0.656122, acc.: 53.12%] [G loss: 0.930759]\n",
            "649 [D loss: 0.629787, acc.: 54.69%] [G loss: 0.954323]\n",
            "650 [D loss: 0.632063, acc.: 53.12%] [G loss: 0.984835]\n",
            "651 [D loss: 0.660886, acc.: 50.00%] [G loss: 0.992339]\n",
            "652 [D loss: 0.634208, acc.: 62.50%] [G loss: 1.014014]\n",
            "653 [D loss: 0.639140, acc.: 57.81%] [G loss: 0.902408]\n",
            "654 [D loss: 0.634834, acc.: 56.25%] [G loss: 0.968945]\n",
            "655 [D loss: 0.638828, acc.: 59.38%] [G loss: 0.958581]\n",
            "656 [D loss: 0.646825, acc.: 59.38%] [G loss: 0.965922]\n",
            "657 [D loss: 0.621791, acc.: 60.94%] [G loss: 0.926034]\n",
            "658 [D loss: 0.604191, acc.: 62.50%] [G loss: 0.908172]\n",
            "659 [D loss: 0.613517, acc.: 60.94%] [G loss: 0.897481]\n",
            "660 [D loss: 0.626686, acc.: 54.69%] [G loss: 0.898829]\n",
            "661 [D loss: 0.674134, acc.: 46.88%] [G loss: 0.940970]\n",
            "662 [D loss: 0.635152, acc.: 51.56%] [G loss: 0.963163]\n",
            "663 [D loss: 0.656909, acc.: 57.81%] [G loss: 1.025826]\n",
            "664 [D loss: 0.613207, acc.: 62.50%] [G loss: 0.937626]\n",
            "665 [D loss: 0.668026, acc.: 57.81%] [G loss: 1.030492]\n",
            "666 [D loss: 0.622028, acc.: 65.62%] [G loss: 0.993888]\n",
            "667 [D loss: 0.594595, acc.: 70.31%] [G loss: 0.939138]\n",
            "668 [D loss: 0.636768, acc.: 62.50%] [G loss: 0.902948]\n",
            "669 [D loss: 0.629444, acc.: 59.38%] [G loss: 0.830809]\n",
            "670 [D loss: 0.652884, acc.: 57.81%] [G loss: 0.846298]\n",
            "671 [D loss: 0.632806, acc.: 53.12%] [G loss: 0.952315]\n",
            "672 [D loss: 0.647175, acc.: 54.69%] [G loss: 0.925818]\n",
            "673 [D loss: 0.633776, acc.: 56.25%] [G loss: 0.989125]\n",
            "674 [D loss: 0.623422, acc.: 60.94%] [G loss: 0.973664]\n",
            "675 [D loss: 0.635718, acc.: 60.94%] [G loss: 0.886084]\n",
            "676 [D loss: 0.647579, acc.: 54.69%] [G loss: 1.002137]\n",
            "677 [D loss: 0.619111, acc.: 57.81%] [G loss: 0.957731]\n",
            "678 [D loss: 0.634095, acc.: 60.94%] [G loss: 0.991520]\n",
            "679 [D loss: 0.604812, acc.: 59.38%] [G loss: 1.050231]\n",
            "680 [D loss: 0.631669, acc.: 54.69%] [G loss: 0.822596]\n",
            "681 [D loss: 0.585901, acc.: 68.75%] [G loss: 0.909076]\n",
            "682 [D loss: 0.661605, acc.: 51.56%] [G loss: 0.933601]\n",
            "683 [D loss: 0.609633, acc.: 62.50%] [G loss: 0.998462]\n",
            "684 [D loss: 0.660913, acc.: 53.12%] [G loss: 1.064787]\n",
            "685 [D loss: 0.622634, acc.: 57.81%] [G loss: 0.909219]\n",
            "686 [D loss: 0.645422, acc.: 54.69%] [G loss: 0.895682]\n",
            "687 [D loss: 0.626683, acc.: 57.81%] [G loss: 0.947066]\n",
            "688 [D loss: 0.670938, acc.: 45.31%] [G loss: 0.951720]\n",
            "689 [D loss: 0.643411, acc.: 56.25%] [G loss: 1.091809]\n",
            "690 [D loss: 0.647954, acc.: 56.25%] [G loss: 0.890204]\n",
            "691 [D loss: 0.647137, acc.: 59.38%] [G loss: 0.851574]\n",
            "692 [D loss: 0.655624, acc.: 54.69%] [G loss: 0.978630]\n",
            "693 [D loss: 0.689757, acc.: 50.00%] [G loss: 0.886362]\n",
            "694 [D loss: 0.615638, acc.: 59.38%] [G loss: 0.939722]\n",
            "695 [D loss: 0.635210, acc.: 54.69%] [G loss: 0.904164]\n",
            "696 [D loss: 0.642829, acc.: 53.12%] [G loss: 0.873809]\n",
            "697 [D loss: 0.668957, acc.: 53.12%] [G loss: 1.027348]\n",
            "698 [D loss: 0.624758, acc.: 53.12%] [G loss: 0.988512]\n",
            "699 [D loss: 0.647121, acc.: 51.56%] [G loss: 1.058431]\n",
            "700 [D loss: 0.626129, acc.: 59.38%] [G loss: 0.951572]\n",
            "generated_data\n",
            "701 [D loss: 0.627303, acc.: 60.94%] [G loss: 0.919061]\n",
            "702 [D loss: 0.634012, acc.: 54.69%] [G loss: 0.935380]\n",
            "703 [D loss: 0.614129, acc.: 60.94%] [G loss: 0.960182]\n",
            "704 [D loss: 0.631235, acc.: 54.69%] [G loss: 0.938972]\n",
            "705 [D loss: 0.638232, acc.: 60.94%] [G loss: 0.893586]\n",
            "706 [D loss: 0.644495, acc.: 54.69%] [G loss: 0.943971]\n",
            "707 [D loss: 0.658348, acc.: 59.38%] [G loss: 0.946846]\n",
            "708 [D loss: 0.612162, acc.: 59.38%] [G loss: 0.968924]\n",
            "709 [D loss: 0.629595, acc.: 56.25%] [G loss: 0.915709]\n",
            "710 [D loss: 0.628483, acc.: 59.38%] [G loss: 0.927233]\n",
            "711 [D loss: 0.620939, acc.: 57.81%] [G loss: 0.895202]\n",
            "712 [D loss: 0.640249, acc.: 60.94%] [G loss: 0.971060]\n",
            "713 [D loss: 0.617815, acc.: 59.38%] [G loss: 0.955514]\n",
            "714 [D loss: 0.623132, acc.: 57.81%] [G loss: 0.911541]\n",
            "715 [D loss: 0.647914, acc.: 57.81%] [G loss: 0.966474]\n",
            "716 [D loss: 0.650284, acc.: 53.12%] [G loss: 0.938965]\n",
            "717 [D loss: 0.649313, acc.: 50.00%] [G loss: 0.904757]\n",
            "718 [D loss: 0.604308, acc.: 60.94%] [G loss: 0.919748]\n",
            "719 [D loss: 0.623695, acc.: 54.69%] [G loss: 0.961810]\n",
            "720 [D loss: 0.605710, acc.: 59.38%] [G loss: 0.912078]\n",
            "721 [D loss: 0.601190, acc.: 65.62%] [G loss: 0.884149]\n",
            "722 [D loss: 0.613030, acc.: 60.94%] [G loss: 0.982573]\n",
            "723 [D loss: 0.587448, acc.: 65.62%] [G loss: 0.870049]\n",
            "724 [D loss: 0.642358, acc.: 54.69%] [G loss: 0.914301]\n",
            "725 [D loss: 0.592137, acc.: 68.75%] [G loss: 0.974983]\n",
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "726 [D loss: 0.654401, acc.: 60.94%] [G loss: 0.938011]\n",
            "727 [D loss: 0.613064, acc.: 62.50%] [G loss: 0.938455]\n",
            "728 [D loss: 0.587310, acc.: 64.06%] [G loss: 0.915973]\n",
            "729 [D loss: 0.601956, acc.: 62.50%] [G loss: 0.895890]\n",
            "730 [D loss: 0.626440, acc.: 57.81%] [G loss: 0.900226]\n",
            "731 [D loss: 0.671439, acc.: 51.56%] [G loss: 0.947537]\n",
            "732 [D loss: 0.630823, acc.: 51.56%] [G loss: 0.946391]\n",
            "733 [D loss: 0.641740, acc.: 54.69%] [G loss: 1.055059]\n",
            "734 [D loss: 0.618234, acc.: 54.69%] [G loss: 0.919346]\n",
            "735 [D loss: 0.634576, acc.: 57.81%] [G loss: 0.934336]\n",
            "736 [D loss: 0.636806, acc.: 54.69%] [G loss: 0.890784]\n",
            "737 [D loss: 0.580014, acc.: 62.50%] [G loss: 0.958938]\n",
            "738 [D loss: 0.612684, acc.: 59.38%] [G loss: 0.881244]\n",
            "739 [D loss: 0.649637, acc.: 59.38%] [G loss: 0.935610]\n",
            "740 [D loss: 0.672668, acc.: 53.12%] [G loss: 0.991171]\n",
            "741 [D loss: 0.611052, acc.: 62.50%] [G loss: 0.981014]\n",
            "742 [D loss: 0.592563, acc.: 60.94%] [G loss: 0.933681]\n",
            "743 [D loss: 0.609051, acc.: 56.25%] [G loss: 0.955122]\n",
            "744 [D loss: 0.637244, acc.: 57.81%] [G loss: 0.909740]\n",
            "745 [D loss: 0.668940, acc.: 54.69%] [G loss: 0.860162]\n",
            "746 [D loss: 0.602305, acc.: 62.50%] [G loss: 0.919538]\n",
            "747 [D loss: 0.604426, acc.: 62.50%] [G loss: 0.903274]\n",
            "748 [D loss: 0.596305, acc.: 62.50%] [G loss: 0.952273]\n",
            "749 [D loss: 0.641954, acc.: 54.69%] [G loss: 0.948032]\n",
            "750 [D loss: 0.596122, acc.: 64.06%] [G loss: 0.939893]\n",
            "751 [D loss: 0.617568, acc.: 64.06%] [G loss: 0.937746]\n",
            "752 [D loss: 0.600108, acc.: 60.94%] [G loss: 1.054043]\n",
            "753 [D loss: 0.602764, acc.: 62.50%] [G loss: 0.975588]\n",
            "754 [D loss: 0.628061, acc.: 60.94%] [G loss: 0.984033]\n",
            "755 [D loss: 0.628999, acc.: 57.81%] [G loss: 1.043143]\n",
            "756 [D loss: 0.665093, acc.: 54.69%] [G loss: 0.939447]\n",
            "757 [D loss: 0.607512, acc.: 64.06%] [G loss: 1.004240]\n",
            "758 [D loss: 0.612200, acc.: 67.19%] [G loss: 0.985252]\n",
            "759 [D loss: 0.623870, acc.: 62.50%] [G loss: 0.900956]\n",
            "760 [D loss: 0.628473, acc.: 59.38%] [G loss: 0.921488]\n",
            "761 [D loss: 0.632182, acc.: 56.25%] [G loss: 0.917567]\n",
            "762 [D loss: 0.631360, acc.: 59.38%] [G loss: 0.925556]\n",
            "763 [D loss: 0.635192, acc.: 57.81%] [G loss: 1.011474]\n",
            "764 [D loss: 0.626602, acc.: 60.94%] [G loss: 0.990916]\n",
            "765 [D loss: 0.608485, acc.: 59.38%] [G loss: 0.974377]\n",
            "766 [D loss: 0.588087, acc.: 57.81%] [G loss: 0.975948]\n",
            "767 [D loss: 0.599068, acc.: 64.06%] [G loss: 0.983040]\n",
            "768 [D loss: 0.664451, acc.: 54.69%] [G loss: 0.988478]\n",
            "769 [D loss: 0.636979, acc.: 59.38%] [G loss: 0.912631]\n",
            "770 [D loss: 0.598944, acc.: 68.75%] [G loss: 0.913657]\n",
            "771 [D loss: 0.586581, acc.: 70.31%] [G loss: 0.993751]\n",
            "772 [D loss: 0.586788, acc.: 70.31%] [G loss: 0.870023]\n",
            "773 [D loss: 0.613018, acc.: 62.50%] [G loss: 0.905322]\n",
            "774 [D loss: 0.596182, acc.: 68.75%] [G loss: 0.931802]\n",
            "775 [D loss: 0.605755, acc.: 62.50%] [G loss: 0.928341]\n",
            "776 [D loss: 0.649680, acc.: 59.38%] [G loss: 0.875728]\n",
            "777 [D loss: 0.581112, acc.: 73.44%] [G loss: 0.981807]\n",
            "778 [D loss: 0.605374, acc.: 64.06%] [G loss: 0.974536]\n",
            "779 [D loss: 0.665739, acc.: 56.25%] [G loss: 0.933899]\n",
            "780 [D loss: 0.616159, acc.: 59.38%] [G loss: 0.906422]\n",
            "781 [D loss: 0.576780, acc.: 65.62%] [G loss: 0.955952]\n",
            "782 [D loss: 0.588386, acc.: 65.62%] [G loss: 0.906100]\n",
            "783 [D loss: 0.610014, acc.: 59.38%] [G loss: 0.867124]\n",
            "784 [D loss: 0.584424, acc.: 64.06%] [G loss: 0.830723]\n",
            "785 [D loss: 0.606863, acc.: 64.06%] [G loss: 0.844257]\n",
            "786 [D loss: 0.605606, acc.: 68.75%] [G loss: 0.859368]\n",
            "787 [D loss: 0.609130, acc.: 70.31%] [G loss: 0.851372]\n",
            "788 [D loss: 0.629077, acc.: 56.25%] [G loss: 0.906989]\n",
            "789 [D loss: 0.610559, acc.: 62.50%] [G loss: 0.869897]\n",
            "790 [D loss: 0.628799, acc.: 59.38%] [G loss: 0.926794]\n",
            "791 [D loss: 0.637977, acc.: 60.94%] [G loss: 0.909121]\n",
            "792 [D loss: 0.664634, acc.: 60.94%] [G loss: 0.888036]\n",
            "793 [D loss: 0.637636, acc.: 59.38%] [G loss: 0.981651]\n",
            "794 [D loss: 0.659482, acc.: 62.50%] [G loss: 0.967543]\n",
            "795 [D loss: 0.596933, acc.: 60.94%] [G loss: 0.873448]\n",
            "796 [D loss: 0.651106, acc.: 48.44%] [G loss: 0.912328]\n",
            "797 [D loss: 0.602968, acc.: 60.94%] [G loss: 0.905085]\n",
            "798 [D loss: 0.622731, acc.: 62.50%] [G loss: 0.880650]\n",
            "799 [D loss: 0.619572, acc.: 59.38%] [G loss: 0.930152]\n",
            "800 [D loss: 0.608088, acc.: 54.69%] [G loss: 0.884434]\n",
            "generated_data\n",
            "801 [D loss: 0.622626, acc.: 57.81%] [G loss: 0.859971]\n",
            "802 [D loss: 0.602608, acc.: 62.50%] [G loss: 0.901140]\n",
            "803 [D loss: 0.618450, acc.: 54.69%] [G loss: 0.960862]\n",
            "804 [D loss: 0.602336, acc.: 60.94%] [G loss: 0.882764]\n",
            "805 [D loss: 0.601711, acc.: 64.06%] [G loss: 0.931081]\n",
            "806 [D loss: 0.609710, acc.: 68.75%] [G loss: 0.931434]\n",
            "807 [D loss: 0.629067, acc.: 59.38%] [G loss: 0.944826]\n",
            "808 [D loss: 0.608578, acc.: 64.06%] [G loss: 0.906342]\n",
            "809 [D loss: 0.604571, acc.: 59.38%] [G loss: 0.896722]\n",
            "810 [D loss: 0.614411, acc.: 60.94%] [G loss: 0.895793]\n",
            "811 [D loss: 0.620711, acc.: 64.06%] [G loss: 0.951638]\n",
            "812 [D loss: 0.625217, acc.: 62.50%] [G loss: 0.904565]\n",
            "813 [D loss: 0.632985, acc.: 57.81%] [G loss: 0.903633]\n",
            "814 [D loss: 0.631289, acc.: 56.25%] [G loss: 1.013929]\n",
            "815 [D loss: 0.595808, acc.: 62.50%] [G loss: 0.888868]\n",
            "816 [D loss: 0.618976, acc.: 62.50%] [G loss: 0.910293]\n",
            "817 [D loss: 0.588391, acc.: 67.19%] [G loss: 0.864623]\n",
            "818 [D loss: 0.580582, acc.: 68.75%] [G loss: 0.893737]\n",
            "819 [D loss: 0.601343, acc.: 65.62%] [G loss: 0.883681]\n",
            "820 [D loss: 0.605009, acc.: 62.50%] [G loss: 0.833332]\n",
            "821 [D loss: 0.602006, acc.: 67.19%] [G loss: 0.840068]\n",
            "822 [D loss: 0.645194, acc.: 62.50%] [G loss: 0.933209]\n",
            "823 [D loss: 0.602018, acc.: 67.19%] [G loss: 0.964156]\n",
            "824 [D loss: 0.623086, acc.: 54.69%] [G loss: 0.930558]\n",
            "825 [D loss: 0.606826, acc.: 64.06%] [G loss: 0.862690]\n",
            "826 [D loss: 0.600584, acc.: 62.50%] [G loss: 0.830707]\n",
            "827 [D loss: 0.632578, acc.: 62.50%] [G loss: 0.810610]\n",
            "828 [D loss: 0.645105, acc.: 56.25%] [G loss: 0.870102]\n",
            "829 [D loss: 0.596278, acc.: 71.88%] [G loss: 0.968273]\n",
            "830 [D loss: 0.641040, acc.: 59.38%] [G loss: 0.787305]\n",
            "831 [D loss: 0.596865, acc.: 65.62%] [G loss: 0.882240]\n",
            "832 [D loss: 0.618176, acc.: 67.19%] [G loss: 0.906237]\n",
            "833 [D loss: 0.621811, acc.: 64.06%] [G loss: 0.960431]\n",
            "834 [D loss: 0.614819, acc.: 62.50%] [G loss: 0.883084]\n",
            "835 [D loss: 0.607274, acc.: 67.19%] [G loss: 0.916307]\n",
            "836 [D loss: 0.632573, acc.: 53.12%] [G loss: 0.868298]\n",
            "837 [D loss: 0.609867, acc.: 67.19%] [G loss: 0.892191]\n",
            "838 [D loss: 0.599537, acc.: 64.06%] [G loss: 0.793356]\n",
            "839 [D loss: 0.628354, acc.: 68.75%] [G loss: 0.875879]\n",
            "840 [D loss: 0.600234, acc.: 65.62%] [G loss: 0.989723]\n",
            "841 [D loss: 0.601438, acc.: 59.38%] [G loss: 0.839233]\n",
            "842 [D loss: 0.611808, acc.: 54.69%] [G loss: 0.827578]\n",
            "843 [D loss: 0.613860, acc.: 64.06%] [G loss: 0.864753]\n",
            "844 [D loss: 0.652048, acc.: 60.94%] [G loss: 0.847772]\n",
            "845 [D loss: 0.618562, acc.: 62.50%] [G loss: 0.881237]\n",
            "846 [D loss: 0.628427, acc.: 68.75%] [G loss: 0.879591]\n",
            "847 [D loss: 0.636663, acc.: 62.50%] [G loss: 0.862425]\n",
            "848 [D loss: 0.619555, acc.: 62.50%] [G loss: 0.894559]\n",
            "849 [D loss: 0.640191, acc.: 57.81%] [G loss: 0.869085]\n",
            "850 [D loss: 0.582751, acc.: 70.31%] [G loss: 0.931325]\n",
            "851 [D loss: 0.659740, acc.: 57.81%] [G loss: 0.901505]\n",
            "852 [D loss: 0.609784, acc.: 70.31%] [G loss: 0.811013]\n",
            "853 [D loss: 0.633901, acc.: 60.94%] [G loss: 0.855429]\n",
            "854 [D loss: 0.596848, acc.: 67.19%] [G loss: 0.939131]\n",
            "855 [D loss: 0.622177, acc.: 67.19%] [G loss: 1.019828]\n",
            "856 [D loss: 0.597132, acc.: 60.94%] [G loss: 0.854113]\n",
            "857 [D loss: 0.624562, acc.: 56.25%] [G loss: 0.907418]\n",
            "858 [D loss: 0.590788, acc.: 65.62%] [G loss: 0.884573]\n",
            "859 [D loss: 0.604492, acc.: 64.06%] [G loss: 0.860489]\n",
            "860 [D loss: 0.599678, acc.: 59.38%] [G loss: 0.803108]\n",
            "861 [D loss: 0.599523, acc.: 59.38%] [G loss: 0.850562]\n",
            "862 [D loss: 0.612408, acc.: 60.94%] [G loss: 0.905739]\n",
            "863 [D loss: 0.644709, acc.: 56.25%] [G loss: 0.852808]\n",
            "864 [D loss: 0.607924, acc.: 65.62%] [G loss: 0.877299]\n",
            "865 [D loss: 0.610399, acc.: 64.06%] [G loss: 0.901366]\n",
            "866 [D loss: 0.601176, acc.: 65.62%] [G loss: 0.927470]\n",
            "867 [D loss: 0.654036, acc.: 59.38%] [G loss: 0.847760]\n",
            "868 [D loss: 0.562207, acc.: 70.31%] [G loss: 0.939501]\n",
            "869 [D loss: 0.595434, acc.: 71.88%] [G loss: 0.898685]\n",
            "870 [D loss: 0.628352, acc.: 62.50%] [G loss: 0.882056]\n",
            "871 [D loss: 0.579140, acc.: 73.44%] [G loss: 0.890167]\n",
            "872 [D loss: 0.638726, acc.: 56.25%] [G loss: 1.001003]\n",
            "873 [D loss: 0.608175, acc.: 65.62%] [G loss: 0.934461]\n",
            "874 [D loss: 0.610335, acc.: 64.06%] [G loss: 0.964795]\n",
            "875 [D loss: 0.617251, acc.: 59.38%] [G loss: 0.904247]\n",
            "876 [D loss: 0.595103, acc.: 64.06%] [G loss: 0.872778]\n",
            "877 [D loss: 0.573160, acc.: 67.19%] [G loss: 0.909966]\n",
            "878 [D loss: 0.602439, acc.: 67.19%] [G loss: 0.839541]\n",
            "879 [D loss: 0.575075, acc.: 71.88%] [G loss: 0.881835]\n",
            "880 [D loss: 0.627744, acc.: 64.06%] [G loss: 1.032949]\n",
            "881 [D loss: 0.612061, acc.: 60.94%] [G loss: 0.928920]\n",
            "882 [D loss: 0.611032, acc.: 62.50%] [G loss: 0.884137]\n",
            "883 [D loss: 0.613793, acc.: 67.19%] [G loss: 0.873494]\n",
            "884 [D loss: 0.620269, acc.: 59.38%] [G loss: 1.009204]\n",
            "885 [D loss: 0.623530, acc.: 64.06%] [G loss: 0.960763]\n",
            "886 [D loss: 0.603107, acc.: 59.38%] [G loss: 0.850868]\n",
            "887 [D loss: 0.605422, acc.: 62.50%] [G loss: 0.850008]\n",
            "888 [D loss: 0.633102, acc.: 59.38%] [G loss: 0.931929]\n",
            "889 [D loss: 0.640777, acc.: 57.81%] [G loss: 0.958699]\n",
            "890 [D loss: 0.594588, acc.: 59.38%] [G loss: 0.904523]\n",
            "891 [D loss: 0.660205, acc.: 59.38%] [G loss: 0.973739]\n",
            "892 [D loss: 0.601401, acc.: 62.50%] [G loss: 1.000638]\n",
            "893 [D loss: 0.606768, acc.: 62.50%] [G loss: 1.000992]\n",
            "894 [D loss: 0.625682, acc.: 62.50%] [G loss: 0.876604]\n",
            "895 [D loss: 0.621004, acc.: 60.94%] [G loss: 0.948268]\n",
            "896 [D loss: 0.573523, acc.: 67.19%] [G loss: 0.934332]\n",
            "897 [D loss: 0.607393, acc.: 64.06%] [G loss: 0.885382]\n",
            "898 [D loss: 0.550534, acc.: 67.19%] [G loss: 0.924399]\n",
            "899 [D loss: 0.588161, acc.: 67.19%] [G loss: 0.932954]\n",
            "900 [D loss: 0.629131, acc.: 62.50%] [G loss: 0.863997]\n",
            "generated_data\n",
            "901 [D loss: 0.617590, acc.: 65.62%] [G loss: 0.913840]\n",
            "902 [D loss: 0.565186, acc.: 70.31%] [G loss: 1.017896]\n",
            "903 [D loss: 0.614647, acc.: 65.62%] [G loss: 1.020257]\n",
            "904 [D loss: 0.612352, acc.: 64.06%] [G loss: 0.988872]\n",
            "905 [D loss: 0.648431, acc.: 53.12%] [G loss: 1.029636]\n",
            "906 [D loss: 0.622043, acc.: 62.50%] [G loss: 0.975649]\n",
            "907 [D loss: 0.589610, acc.: 62.50%] [G loss: 0.929484]\n",
            "908 [D loss: 0.655588, acc.: 53.12%] [G loss: 0.960949]\n",
            "909 [D loss: 0.585054, acc.: 68.75%] [G loss: 0.943090]\n",
            "910 [D loss: 0.565506, acc.: 70.31%] [G loss: 0.864010]\n",
            "911 [D loss: 0.612550, acc.: 65.62%] [G loss: 0.900738]\n",
            "912 [D loss: 0.604814, acc.: 59.38%] [G loss: 0.993486]\n",
            "913 [D loss: 0.623587, acc.: 62.50%] [G loss: 0.968189]\n",
            "914 [D loss: 0.612236, acc.: 57.81%] [G loss: 0.830340]\n",
            "915 [D loss: 0.582412, acc.: 67.19%] [G loss: 0.980925]\n",
            "916 [D loss: 0.677241, acc.: 62.50%] [G loss: 0.904780]\n",
            "917 [D loss: 0.602721, acc.: 73.44%] [G loss: 0.840653]\n",
            "918 [D loss: 0.589497, acc.: 64.06%] [G loss: 0.989243]\n",
            "919 [D loss: 0.604560, acc.: 60.94%] [G loss: 1.003830]\n",
            "920 [D loss: 0.630648, acc.: 65.62%] [G loss: 0.855646]\n",
            "921 [D loss: 0.593807, acc.: 67.19%] [G loss: 0.846471]\n",
            "922 [D loss: 0.605227, acc.: 67.19%] [G loss: 0.852213]\n",
            "923 [D loss: 0.599075, acc.: 67.19%] [G loss: 0.864764]\n",
            "924 [D loss: 0.590317, acc.: 67.19%] [G loss: 0.948159]\n",
            "925 [D loss: 0.613918, acc.: 65.62%] [G loss: 0.943039]\n",
            "926 [D loss: 0.584850, acc.: 71.88%] [G loss: 0.977404]\n",
            "927 [D loss: 0.604721, acc.: 70.31%] [G loss: 0.976845]\n",
            "928 [D loss: 0.660021, acc.: 57.81%] [G loss: 0.860414]\n",
            "929 [D loss: 0.618995, acc.: 67.19%] [G loss: 0.986437]\n",
            "930 [D loss: 0.554168, acc.: 75.00%] [G loss: 0.998597]\n",
            "931 [D loss: 0.596544, acc.: 71.88%] [G loss: 1.028945]\n",
            "932 [D loss: 0.582194, acc.: 64.06%] [G loss: 0.920060]\n",
            "933 [D loss: 0.562479, acc.: 76.56%] [G loss: 0.920030]\n",
            "934 [D loss: 0.610672, acc.: 60.94%] [G loss: 0.890919]\n",
            "935 [D loss: 0.584337, acc.: 71.88%] [G loss: 0.947873]\n",
            "936 [D loss: 0.564088, acc.: 70.31%] [G loss: 0.944274]\n",
            "937 [D loss: 0.577609, acc.: 70.31%] [G loss: 0.926938]\n",
            "938 [D loss: 0.582047, acc.: 70.31%] [G loss: 0.899480]\n",
            "939 [D loss: 0.606587, acc.: 73.44%] [G loss: 0.906631]\n",
            "940 [D loss: 0.552087, acc.: 68.75%] [G loss: 0.968753]\n",
            "941 [D loss: 0.560430, acc.: 67.19%] [G loss: 0.931003]\n",
            "942 [D loss: 0.539235, acc.: 73.44%] [G loss: 0.920226]\n",
            "943 [D loss: 0.553413, acc.: 73.44%] [G loss: 0.978106]\n",
            "944 [D loss: 0.578030, acc.: 70.31%] [G loss: 1.009788]\n",
            "945 [D loss: 0.572432, acc.: 65.62%] [G loss: 0.943641]\n",
            "946 [D loss: 0.531538, acc.: 71.88%] [G loss: 0.985153]\n",
            "947 [D loss: 0.563158, acc.: 73.44%] [G loss: 0.899157]\n",
            "948 [D loss: 0.565768, acc.: 67.19%] [G loss: 0.983395]\n",
            "949 [D loss: 0.643121, acc.: 64.06%] [G loss: 0.943684]\n",
            "950 [D loss: 0.545581, acc.: 71.88%] [G loss: 1.050473]\n",
            "951 [D loss: 0.542707, acc.: 62.50%] [G loss: 0.984404]\n",
            "952 [D loss: 0.547610, acc.: 68.75%] [G loss: 0.968754]\n",
            "953 [D loss: 0.600707, acc.: 59.38%] [G loss: 0.895877]\n",
            "954 [D loss: 0.551557, acc.: 67.19%] [G loss: 0.939115]\n",
            "955 [D loss: 0.559735, acc.: 64.06%] [G loss: 0.982045]\n",
            "956 [D loss: 0.582775, acc.: 64.06%] [G loss: 1.000197]\n",
            "957 [D loss: 0.547897, acc.: 68.75%] [G loss: 0.959262]\n",
            "958 [D loss: 0.558282, acc.: 65.62%] [G loss: 0.951677]\n",
            "959 [D loss: 0.535338, acc.: 68.75%] [G loss: 0.907755]\n",
            "960 [D loss: 0.539329, acc.: 78.12%] [G loss: 0.924655]\n",
            "961 [D loss: 0.578379, acc.: 64.06%] [G loss: 0.914794]\n",
            "962 [D loss: 0.563418, acc.: 67.19%] [G loss: 0.914331]\n",
            "963 [D loss: 0.552408, acc.: 64.06%] [G loss: 0.874609]\n",
            "964 [D loss: 0.562269, acc.: 62.50%] [G loss: 0.895282]\n",
            "965 [D loss: 0.560805, acc.: 68.75%] [G loss: 0.897908]\n",
            "966 [D loss: 0.547524, acc.: 68.75%] [G loss: 1.000544]\n",
            "967 [D loss: 0.545125, acc.: 73.44%] [G loss: 0.941555]\n",
            "968 [D loss: 0.533567, acc.: 71.88%] [G loss: 0.958678]\n",
            "969 [D loss: 0.557749, acc.: 70.31%] [G loss: 0.950358]\n",
            "970 [D loss: 0.596886, acc.: 67.19%] [G loss: 0.930053]\n",
            "971 [D loss: 0.563711, acc.: 62.50%] [G loss: 0.859731]\n",
            "972 [D loss: 0.529773, acc.: 73.44%] [G loss: 0.909455]\n",
            "973 [D loss: 0.577313, acc.: 65.62%] [G loss: 0.902666]\n",
            "974 [D loss: 0.548675, acc.: 70.31%] [G loss: 0.958086]\n",
            "975 [D loss: 0.605299, acc.: 65.62%] [G loss: 0.905609]\n",
            "976 [D loss: 0.547399, acc.: 68.75%] [G loss: 0.961461]\n",
            "977 [D loss: 0.525434, acc.: 76.56%] [G loss: 0.926673]\n",
            "978 [D loss: 0.550511, acc.: 70.31%] [G loss: 0.858809]\n",
            "979 [D loss: 0.570397, acc.: 65.62%] [G loss: 0.953322]\n",
            "980 [D loss: 0.557334, acc.: 67.19%] [G loss: 0.947868]\n",
            "981 [D loss: 0.555844, acc.: 65.62%] [G loss: 0.907404]\n",
            "982 [D loss: 0.562595, acc.: 70.31%] [G loss: 0.997536]\n",
            "983 [D loss: 0.528491, acc.: 75.00%] [G loss: 1.018272]\n",
            "984 [D loss: 0.595744, acc.: 65.62%] [G loss: 1.022336]\n",
            "985 [D loss: 0.611791, acc.: 62.50%] [G loss: 0.928961]\n",
            "986 [D loss: 0.530170, acc.: 68.75%] [G loss: 1.088617]\n",
            "987 [D loss: 0.556026, acc.: 65.62%] [G loss: 1.061985]\n",
            "988 [D loss: 0.529841, acc.: 70.31%] [G loss: 1.012877]\n",
            "989 [D loss: 0.556047, acc.: 68.75%] [G loss: 1.026500]\n",
            "990 [D loss: 0.551272, acc.: 68.75%] [G loss: 0.991011]\n",
            "991 [D loss: 0.549914, acc.: 67.19%] [G loss: 1.008445]\n",
            "992 [D loss: 0.555498, acc.: 70.31%] [G loss: 0.961798]\n",
            "993 [D loss: 0.522164, acc.: 70.31%] [G loss: 0.972206]\n",
            "994 [D loss: 0.540632, acc.: 65.62%] [G loss: 0.982082]\n",
            "995 [D loss: 0.549038, acc.: 68.75%] [G loss: 0.888415]\n",
            "996 [D loss: 0.558177, acc.: 65.62%] [G loss: 0.879817]\n",
            "997 [D loss: 0.534666, acc.: 76.56%] [G loss: 0.882505]\n",
            "998 [D loss: 0.528490, acc.: 70.31%] [G loss: 0.860302]\n",
            "999 [D loss: 0.595797, acc.: 65.62%] [G loss: 0.960965]\n",
            "1000 [D loss: 0.587972, acc.: 67.19%] [G loss: 0.935434]\n",
            "generated_data\n",
            "1001 [D loss: 0.533419, acc.: 73.44%] [G loss: 1.042474]\n",
            "1002 [D loss: 0.574117, acc.: 68.75%] [G loss: 0.950428]\n",
            "1003 [D loss: 0.564323, acc.: 67.19%] [G loss: 0.875316]\n",
            "1004 [D loss: 0.548096, acc.: 68.75%] [G loss: 0.845508]\n",
            "1005 [D loss: 0.544854, acc.: 67.19%] [G loss: 0.978221]\n",
            "1006 [D loss: 0.526958, acc.: 68.75%] [G loss: 0.906690]\n",
            "1007 [D loss: 0.572113, acc.: 62.50%] [G loss: 0.973710]\n",
            "1008 [D loss: 0.569292, acc.: 62.50%] [G loss: 0.997752]\n",
            "1009 [D loss: 0.520251, acc.: 75.00%] [G loss: 1.115082]\n",
            "1010 [D loss: 0.557851, acc.: 64.06%] [G loss: 0.985082]\n",
            "1011 [D loss: 0.551711, acc.: 68.75%] [G loss: 0.927987]\n",
            "1012 [D loss: 0.561865, acc.: 71.88%] [G loss: 1.007643]\n",
            "1013 [D loss: 0.522553, acc.: 70.31%] [G loss: 1.008039]\n",
            "1014 [D loss: 0.533054, acc.: 71.88%] [G loss: 0.971188]\n",
            "1015 [D loss: 0.554112, acc.: 67.19%] [G loss: 0.900969]\n",
            "1016 [D loss: 0.537837, acc.: 67.19%] [G loss: 0.941270]\n",
            "1017 [D loss: 0.562732, acc.: 65.62%] [G loss: 0.952745]\n",
            "1018 [D loss: 0.561135, acc.: 65.62%] [G loss: 0.930288]\n",
            "1019 [D loss: 0.562796, acc.: 70.31%] [G loss: 0.925358]\n",
            "1020 [D loss: 0.534892, acc.: 68.75%] [G loss: 0.822133]\n",
            "1021 [D loss: 0.558581, acc.: 67.19%] [G loss: 0.831875]\n",
            "1022 [D loss: 0.612767, acc.: 60.94%] [G loss: 0.905701]\n",
            "1023 [D loss: 0.541979, acc.: 65.62%] [G loss: 0.920862]\n",
            "1024 [D loss: 0.583832, acc.: 64.06%] [G loss: 0.934486]\n",
            "1025 [D loss: 0.550662, acc.: 71.88%] [G loss: 0.955648]\n",
            "1026 [D loss: 0.567404, acc.: 62.50%] [G loss: 0.920513]\n",
            "1027 [D loss: 0.574990, acc.: 59.38%] [G loss: 0.953931]\n",
            "1028 [D loss: 0.544746, acc.: 70.31%] [G loss: 0.911230]\n",
            "1029 [D loss: 0.612325, acc.: 59.38%] [G loss: 1.011471]\n",
            "1030 [D loss: 0.566691, acc.: 67.19%] [G loss: 1.024534]\n",
            "1031 [D loss: 0.546093, acc.: 67.19%] [G loss: 0.951731]\n",
            "1032 [D loss: 0.559235, acc.: 62.50%] [G loss: 0.965513]\n",
            "1033 [D loss: 0.539854, acc.: 68.75%] [G loss: 1.031630]\n",
            "1034 [D loss: 0.530606, acc.: 68.75%] [G loss: 1.105727]\n",
            "1035 [D loss: 0.528909, acc.: 68.75%] [G loss: 1.019964]\n",
            "1036 [D loss: 0.486390, acc.: 78.12%] [G loss: 1.110028]\n",
            "1037 [D loss: 0.533348, acc.: 75.00%] [G loss: 0.986604]\n",
            "1038 [D loss: 0.537341, acc.: 76.56%] [G loss: 0.983861]\n",
            "1039 [D loss: 0.531571, acc.: 70.31%] [G loss: 1.069415]\n",
            "1040 [D loss: 0.566112, acc.: 67.19%] [G loss: 1.069662]\n",
            "1041 [D loss: 0.527830, acc.: 68.75%] [G loss: 0.996467]\n",
            "1042 [D loss: 0.553704, acc.: 65.62%] [G loss: 0.967679]\n",
            "1043 [D loss: 0.558504, acc.: 70.31%] [G loss: 0.959565]\n",
            "1044 [D loss: 0.544838, acc.: 70.31%] [G loss: 0.938702]\n",
            "1045 [D loss: 0.608875, acc.: 60.94%] [G loss: 0.930836]\n",
            "1046 [D loss: 0.536866, acc.: 68.75%] [G loss: 0.956442]\n",
            "1047 [D loss: 0.563998, acc.: 65.62%] [G loss: 0.926427]\n",
            "1048 [D loss: 0.565770, acc.: 65.62%] [G loss: 0.993924]\n",
            "1049 [D loss: 0.542157, acc.: 64.06%] [G loss: 0.992316]\n",
            "1050 [D loss: 0.592352, acc.: 64.06%] [G loss: 0.949147]\n",
            "1051 [D loss: 0.533199, acc.: 76.56%] [G loss: 0.944707]\n",
            "1052 [D loss: 0.547123, acc.: 65.62%] [G loss: 0.972757]\n",
            "1053 [D loss: 0.531545, acc.: 71.88%] [G loss: 0.968342]\n",
            "1054 [D loss: 0.565614, acc.: 65.62%] [G loss: 0.889731]\n",
            "1055 [D loss: 0.544880, acc.: 73.44%] [G loss: 0.968851]\n",
            "1056 [D loss: 0.551852, acc.: 65.62%] [G loss: 0.904897]\n",
            "1057 [D loss: 0.543173, acc.: 67.19%] [G loss: 0.971711]\n",
            "1058 [D loss: 0.544427, acc.: 65.62%] [G loss: 0.958131]\n",
            "1059 [D loss: 0.582775, acc.: 62.50%] [G loss: 0.939362]\n",
            "1060 [D loss: 0.537736, acc.: 68.75%] [G loss: 0.914891]\n",
            "1061 [D loss: 0.545231, acc.: 68.75%] [G loss: 0.901624]\n",
            "1062 [D loss: 0.550370, acc.: 64.06%] [G loss: 0.890976]\n",
            "1063 [D loss: 0.547841, acc.: 59.38%] [G loss: 0.835836]\n",
            "1064 [D loss: 0.533836, acc.: 68.75%] [G loss: 0.955798]\n",
            "1065 [D loss: 0.567707, acc.: 65.62%] [G loss: 0.869907]\n",
            "1066 [D loss: 0.567223, acc.: 70.31%] [G loss: 1.036142]\n",
            "1067 [D loss: 0.573665, acc.: 60.94%] [G loss: 1.006320]\n",
            "1068 [D loss: 0.533950, acc.: 71.88%] [G loss: 0.972380]\n",
            "1069 [D loss: 0.537247, acc.: 68.75%] [G loss: 0.978711]\n",
            "1070 [D loss: 0.535247, acc.: 68.75%] [G loss: 1.200744]\n",
            "1071 [D loss: 0.513998, acc.: 71.88%] [G loss: 1.009990]\n",
            "1072 [D loss: 0.562951, acc.: 68.75%] [G loss: 1.023511]\n",
            "1073 [D loss: 0.516756, acc.: 73.44%] [G loss: 1.049179]\n",
            "1074 [D loss: 0.559836, acc.: 68.75%] [G loss: 1.044870]\n",
            "1075 [D loss: 0.535842, acc.: 68.75%] [G loss: 1.025834]\n",
            "1076 [D loss: 0.550139, acc.: 60.94%] [G loss: 1.033998]\n",
            "1077 [D loss: 0.524796, acc.: 70.31%] [G loss: 1.115356]\n",
            "1078 [D loss: 0.564141, acc.: 62.50%] [G loss: 0.928090]\n",
            "1079 [D loss: 0.566349, acc.: 62.50%] [G loss: 0.954865]\n",
            "1080 [D loss: 0.545208, acc.: 64.06%] [G loss: 1.025076]\n",
            "1081 [D loss: 0.523896, acc.: 67.19%] [G loss: 1.009015]\n",
            "1082 [D loss: 0.593171, acc.: 59.38%] [G loss: 0.859781]\n",
            "1083 [D loss: 0.525695, acc.: 73.44%] [G loss: 1.016089]\n",
            "1084 [D loss: 0.544083, acc.: 68.75%] [G loss: 0.931563]\n",
            "1085 [D loss: 0.533697, acc.: 68.75%] [G loss: 0.918957]\n",
            "1086 [D loss: 0.589584, acc.: 62.50%] [G loss: 0.937258]\n",
            "1087 [D loss: 0.571359, acc.: 65.62%] [G loss: 0.948275]\n",
            "1088 [D loss: 0.548457, acc.: 68.75%] [G loss: 0.962578]\n",
            "1089 [D loss: 0.555848, acc.: 70.31%] [G loss: 0.924897]\n",
            "1090 [D loss: 0.587950, acc.: 67.19%] [G loss: 1.013396]\n",
            "1091 [D loss: 0.562293, acc.: 68.75%] [G loss: 0.950534]\n",
            "1092 [D loss: 0.559800, acc.: 62.50%] [G loss: 0.912059]\n",
            "1093 [D loss: 0.591424, acc.: 57.81%] [G loss: 0.921504]\n",
            "1094 [D loss: 0.560487, acc.: 68.75%] [G loss: 0.901256]\n",
            "1095 [D loss: 0.535499, acc.: 67.19%] [G loss: 1.011953]\n",
            "1096 [D loss: 0.538861, acc.: 68.75%] [G loss: 1.052918]\n",
            "1097 [D loss: 0.544740, acc.: 73.44%] [G loss: 0.972032]\n",
            "1098 [D loss: 0.552148, acc.: 71.88%] [G loss: 0.929665]\n",
            "1099 [D loss: 0.553038, acc.: 70.31%] [G loss: 0.981097]\n",
            "1100 [D loss: 0.558041, acc.: 60.94%] [G loss: 1.020221]\n",
            "generated_data\n",
            "1101 [D loss: 0.528675, acc.: 76.56%] [G loss: 0.984522]\n",
            "1102 [D loss: 0.576724, acc.: 68.75%] [G loss: 1.052098]\n",
            "1103 [D loss: 0.532944, acc.: 70.31%] [G loss: 0.955692]\n",
            "1104 [D loss: 0.561284, acc.: 62.50%] [G loss: 1.040554]\n",
            "1105 [D loss: 0.518360, acc.: 71.88%] [G loss: 1.017948]\n",
            "1106 [D loss: 0.563516, acc.: 64.06%] [G loss: 1.000507]\n",
            "1107 [D loss: 0.575255, acc.: 65.62%] [G loss: 1.011581]\n",
            "1108 [D loss: 0.567550, acc.: 68.75%] [G loss: 0.996157]\n",
            "1109 [D loss: 0.529676, acc.: 68.75%] [G loss: 0.922953]\n",
            "1110 [D loss: 0.559117, acc.: 62.50%] [G loss: 0.923324]\n",
            "1111 [D loss: 0.537222, acc.: 65.62%] [G loss: 0.970568]\n",
            "1112 [D loss: 0.525132, acc.: 73.44%] [G loss: 0.974151]\n",
            "1113 [D loss: 0.541585, acc.: 65.62%] [G loss: 0.921705]\n",
            "1114 [D loss: 0.536641, acc.: 70.31%] [G loss: 0.910904]\n",
            "1115 [D loss: 0.502901, acc.: 78.12%] [G loss: 1.093240]\n",
            "1116 [D loss: 0.576386, acc.: 64.06%] [G loss: 0.970722]\n",
            "1117 [D loss: 0.534137, acc.: 68.75%] [G loss: 1.130016]\n",
            "1118 [D loss: 0.553889, acc.: 67.19%] [G loss: 0.879408]\n",
            "1119 [D loss: 0.578643, acc.: 60.94%] [G loss: 0.869428]\n",
            "1120 [D loss: 0.540936, acc.: 67.19%] [G loss: 1.011564]\n",
            "1121 [D loss: 0.563461, acc.: 67.19%] [G loss: 0.981801]\n",
            "1122 [D loss: 0.545493, acc.: 68.75%] [G loss: 0.941486]\n",
            "1123 [D loss: 0.548837, acc.: 65.62%] [G loss: 1.007020]\n",
            "1124 [D loss: 0.565991, acc.: 65.62%] [G loss: 0.990755]\n",
            "1125 [D loss: 0.600973, acc.: 64.06%] [G loss: 0.980858]\n",
            "1126 [D loss: 0.542689, acc.: 65.62%] [G loss: 1.038588]\n",
            "1127 [D loss: 0.557932, acc.: 67.19%] [G loss: 0.996817]\n",
            "1128 [D loss: 0.541631, acc.: 65.62%] [G loss: 1.005877]\n",
            "1129 [D loss: 0.554448, acc.: 68.75%] [G loss: 0.985594]\n",
            "1130 [D loss: 0.536575, acc.: 67.19%] [G loss: 0.988496]\n",
            "1131 [D loss: 0.557021, acc.: 65.62%] [G loss: 1.054738]\n",
            "1132 [D loss: 0.539964, acc.: 65.62%] [G loss: 1.018567]\n",
            "1133 [D loss: 0.518824, acc.: 71.88%] [G loss: 0.992809]\n",
            "1134 [D loss: 0.522067, acc.: 65.62%] [G loss: 1.025671]\n",
            "1135 [D loss: 0.558321, acc.: 70.31%] [G loss: 0.964332]\n",
            "1136 [D loss: 0.541138, acc.: 67.19%] [G loss: 0.921247]\n",
            "1137 [D loss: 0.540418, acc.: 70.31%] [G loss: 1.029380]\n",
            "1138 [D loss: 0.571576, acc.: 73.44%] [G loss: 1.051677]\n",
            "1139 [D loss: 0.513478, acc.: 70.31%] [G loss: 1.005805]\n",
            "1140 [D loss: 0.625774, acc.: 60.94%] [G loss: 1.006716]\n",
            "1141 [D loss: 0.567870, acc.: 67.19%] [G loss: 1.130143]\n",
            "1142 [D loss: 0.608405, acc.: 64.06%] [G loss: 0.959198]\n",
            "1143 [D loss: 0.564118, acc.: 65.62%] [G loss: 1.010933]\n",
            "1144 [D loss: 0.560838, acc.: 65.62%] [G loss: 0.981846]\n",
            "1145 [D loss: 0.606154, acc.: 56.25%] [G loss: 1.017328]\n",
            "1146 [D loss: 0.557102, acc.: 67.19%] [G loss: 1.022217]\n",
            "1147 [D loss: 0.574724, acc.: 65.62%] [G loss: 0.984944]\n",
            "1148 [D loss: 0.519713, acc.: 70.31%] [G loss: 1.001580]\n",
            "1149 [D loss: 0.538364, acc.: 68.75%] [G loss: 1.154786]\n",
            "1150 [D loss: 0.608172, acc.: 60.94%] [G loss: 1.010736]\n",
            "1151 [D loss: 0.550928, acc.: 64.06%] [G loss: 1.050354]\n",
            "1152 [D loss: 0.541095, acc.: 67.19%] [G loss: 1.168223]\n",
            "1153 [D loss: 0.558132, acc.: 70.31%] [G loss: 0.980509]\n",
            "1154 [D loss: 0.568170, acc.: 67.19%] [G loss: 0.973899]\n",
            "1155 [D loss: 0.550040, acc.: 65.62%] [G loss: 1.027062]\n",
            "1156 [D loss: 0.572181, acc.: 65.62%] [G loss: 1.049109]\n",
            "1157 [D loss: 0.567064, acc.: 67.19%] [G loss: 0.978883]\n",
            "1158 [D loss: 0.549543, acc.: 67.19%] [G loss: 1.015025]\n",
            "1159 [D loss: 0.567391, acc.: 71.88%] [G loss: 1.038689]\n",
            "1160 [D loss: 0.529142, acc.: 70.31%] [G loss: 0.981484]\n",
            "1161 [D loss: 0.537672, acc.: 67.19%] [G loss: 1.022377]\n",
            "1162 [D loss: 0.558047, acc.: 65.62%] [G loss: 0.939358]\n",
            "1163 [D loss: 0.577802, acc.: 67.19%] [G loss: 0.923831]\n",
            "1164 [D loss: 0.544730, acc.: 65.62%] [G loss: 1.090464]\n",
            "1165 [D loss: 0.552756, acc.: 62.50%] [G loss: 0.999320]\n",
            "1166 [D loss: 0.573164, acc.: 65.62%] [G loss: 0.987128]\n",
            "1167 [D loss: 0.537348, acc.: 68.75%] [G loss: 1.081714]\n",
            "1168 [D loss: 0.527985, acc.: 68.75%] [G loss: 1.084706]\n",
            "1169 [D loss: 0.550561, acc.: 64.06%] [G loss: 0.965086]\n",
            "1170 [D loss: 0.553019, acc.: 71.88%] [G loss: 1.024400]\n",
            "1171 [D loss: 0.536894, acc.: 71.88%] [G loss: 1.181839]\n",
            "1172 [D loss: 0.581962, acc.: 64.06%] [G loss: 1.056547]\n",
            "1173 [D loss: 0.594023, acc.: 67.19%] [G loss: 0.946874]\n",
            "1174 [D loss: 0.534484, acc.: 67.19%] [G loss: 1.027646]\n",
            "1175 [D loss: 0.516724, acc.: 68.75%] [G loss: 1.095665]\n",
            "1176 [D loss: 0.534065, acc.: 73.44%] [G loss: 1.036928]\n",
            "1177 [D loss: 0.526857, acc.: 79.69%] [G loss: 0.996728]\n",
            "1178 [D loss: 0.573742, acc.: 62.50%] [G loss: 0.958614]\n",
            "1179 [D loss: 0.562387, acc.: 65.62%] [G loss: 0.970267]\n",
            "1180 [D loss: 0.517002, acc.: 78.12%] [G loss: 0.918728]\n",
            "1181 [D loss: 0.532501, acc.: 67.19%] [G loss: 1.015072]\n",
            "1182 [D loss: 0.545154, acc.: 67.19%] [G loss: 1.122214]\n",
            "1183 [D loss: 0.530091, acc.: 67.19%] [G loss: 1.099891]\n",
            "1184 [D loss: 0.554159, acc.: 71.88%] [G loss: 1.028261]\n",
            "1185 [D loss: 0.591179, acc.: 54.69%] [G loss: 0.978998]\n",
            "1186 [D loss: 0.540862, acc.: 70.31%] [G loss: 1.138009]\n",
            "1187 [D loss: 0.551440, acc.: 68.75%] [G loss: 1.063753]\n",
            "1188 [D loss: 0.528187, acc.: 70.31%] [G loss: 1.031858]\n",
            "1189 [D loss: 0.511248, acc.: 70.31%] [G loss: 1.096353]\n",
            "1190 [D loss: 0.487386, acc.: 73.44%] [G loss: 1.119758]\n",
            "1191 [D loss: 0.530095, acc.: 75.00%] [G loss: 1.184560]\n",
            "1192 [D loss: 0.510379, acc.: 75.00%] [G loss: 1.066605]\n",
            "1193 [D loss: 0.559674, acc.: 70.31%] [G loss: 0.957688]\n",
            "1194 [D loss: 0.538241, acc.: 68.75%] [G loss: 1.089949]\n",
            "1195 [D loss: 0.520579, acc.: 78.12%] [G loss: 1.062498]\n",
            "1196 [D loss: 0.543544, acc.: 68.75%] [G loss: 0.970508]\n",
            "1197 [D loss: 0.545345, acc.: 65.62%] [G loss: 0.952556]\n",
            "1198 [D loss: 0.535884, acc.: 68.75%] [G loss: 0.936273]\n",
            "1199 [D loss: 0.503032, acc.: 76.56%] [G loss: 0.975857]\n",
            "1200 [D loss: 0.546126, acc.: 68.75%] [G loss: 1.062471]\n",
            "generated_data\n",
            "1201 [D loss: 0.537484, acc.: 64.06%] [G loss: 0.978560]\n",
            "1202 [D loss: 0.549520, acc.: 67.19%] [G loss: 0.940640]\n",
            "1203 [D loss: 0.511445, acc.: 70.31%] [G loss: 1.159628]\n",
            "1204 [D loss: 0.525497, acc.: 71.88%] [G loss: 1.097414]\n",
            "1205 [D loss: 0.546603, acc.: 67.19%] [G loss: 1.027068]\n",
            "1206 [D loss: 0.547368, acc.: 73.44%] [G loss: 1.071288]\n",
            "1207 [D loss: 0.594496, acc.: 56.25%] [G loss: 0.994667]\n",
            "1208 [D loss: 0.585998, acc.: 67.19%] [G loss: 1.011422]\n",
            "1209 [D loss: 0.512044, acc.: 71.88%] [G loss: 0.899983]\n",
            "1210 [D loss: 0.549275, acc.: 67.19%] [G loss: 1.019060]\n",
            "1211 [D loss: 0.528953, acc.: 68.75%] [G loss: 1.124500]\n",
            "1212 [D loss: 0.564721, acc.: 67.19%] [G loss: 0.998207]\n",
            "1213 [D loss: 0.552497, acc.: 73.44%] [G loss: 0.948904]\n",
            "1214 [D loss: 0.523266, acc.: 71.88%] [G loss: 1.033866]\n",
            "1215 [D loss: 0.549391, acc.: 64.06%] [G loss: 1.039475]\n",
            "1216 [D loss: 0.516254, acc.: 70.31%] [G loss: 0.935273]\n",
            "1217 [D loss: 0.527823, acc.: 67.19%] [G loss: 1.024986]\n",
            "1218 [D loss: 0.556812, acc.: 64.06%] [G loss: 1.141252]\n",
            "1219 [D loss: 0.583142, acc.: 62.50%] [G loss: 0.933969]\n",
            "1220 [D loss: 0.543934, acc.: 64.06%] [G loss: 0.978781]\n",
            "1221 [D loss: 0.553622, acc.: 70.31%] [G loss: 1.066098]\n",
            "1222 [D loss: 0.551794, acc.: 65.62%] [G loss: 0.861682]\n",
            "1223 [D loss: 0.521954, acc.: 73.44%] [G loss: 1.045452]\n",
            "1224 [D loss: 0.600620, acc.: 62.50%] [G loss: 0.918260]\n",
            "1225 [D loss: 0.529414, acc.: 70.31%] [G loss: 1.045357]\n",
            "1226 [D loss: 0.560364, acc.: 70.31%] [G loss: 0.987709]\n",
            "1227 [D loss: 0.567312, acc.: 71.88%] [G loss: 0.962551]\n",
            "1228 [D loss: 0.521888, acc.: 76.56%] [G loss: 1.031185]\n",
            "1229 [D loss: 0.527432, acc.: 71.88%] [G loss: 1.042710]\n",
            "1230 [D loss: 0.537969, acc.: 71.88%] [G loss: 0.997678]\n",
            "1231 [D loss: 0.541043, acc.: 70.31%] [G loss: 1.076602]\n",
            "1232 [D loss: 0.515032, acc.: 68.75%] [G loss: 1.049336]\n",
            "1233 [D loss: 0.568511, acc.: 67.19%] [G loss: 0.973279]\n",
            "1234 [D loss: 0.538883, acc.: 70.31%] [G loss: 0.976017]\n",
            "1235 [D loss: 0.563805, acc.: 70.31%] [G loss: 0.989485]\n",
            "1236 [D loss: 0.562668, acc.: 65.62%] [G loss: 0.934700]\n",
            "1237 [D loss: 0.593814, acc.: 67.19%] [G loss: 1.007336]\n",
            "1238 [D loss: 0.575584, acc.: 68.75%] [G loss: 0.979569]\n",
            "1239 [D loss: 0.578202, acc.: 67.19%] [G loss: 1.007400]\n",
            "1240 [D loss: 0.561022, acc.: 68.75%] [G loss: 1.026078]\n",
            "1241 [D loss: 0.554564, acc.: 65.62%] [G loss: 1.018307]\n",
            "1242 [D loss: 0.557151, acc.: 73.44%] [G loss: 0.978081]\n",
            "1243 [D loss: 0.536576, acc.: 70.31%] [G loss: 0.964675]\n",
            "1244 [D loss: 0.530277, acc.: 67.19%] [G loss: 0.945601]\n",
            "1245 [D loss: 0.547708, acc.: 70.31%] [G loss: 0.958664]\n",
            "1246 [D loss: 0.544708, acc.: 70.31%] [G loss: 0.931187]\n",
            "1247 [D loss: 0.535410, acc.: 67.19%] [G loss: 1.001179]\n",
            "1248 [D loss: 0.523927, acc.: 71.88%] [G loss: 0.967824]\n",
            "1249 [D loss: 0.532759, acc.: 73.44%] [G loss: 0.968537]\n",
            "1250 [D loss: 0.558277, acc.: 70.31%] [G loss: 1.011792]\n",
            "1251 [D loss: 0.526891, acc.: 68.75%] [G loss: 1.067344]\n",
            "1252 [D loss: 0.555016, acc.: 68.75%] [G loss: 1.023569]\n",
            "1253 [D loss: 0.544033, acc.: 70.31%] [G loss: 0.948829]\n",
            "1254 [D loss: 0.513857, acc.: 71.88%] [G loss: 0.997649]\n",
            "1255 [D loss: 0.607332, acc.: 65.62%] [G loss: 0.993371]\n",
            "1256 [D loss: 0.548592, acc.: 65.62%] [G loss: 1.081046]\n",
            "1257 [D loss: 0.527539, acc.: 73.44%] [G loss: 1.016739]\n",
            "1258 [D loss: 0.538361, acc.: 67.19%] [G loss: 1.006671]\n",
            "1259 [D loss: 0.562341, acc.: 62.50%] [G loss: 1.140015]\n",
            "1260 [D loss: 0.546930, acc.: 67.19%] [G loss: 1.057424]\n",
            "1261 [D loss: 0.549616, acc.: 65.62%] [G loss: 1.067899]\n",
            "1262 [D loss: 0.538963, acc.: 68.75%] [G loss: 1.033189]\n",
            "1263 [D loss: 0.551804, acc.: 68.75%] [G loss: 1.039137]\n",
            "1264 [D loss: 0.565505, acc.: 68.75%] [G loss: 1.040430]\n",
            "1265 [D loss: 0.554326, acc.: 67.19%] [G loss: 1.059261]\n",
            "1266 [D loss: 0.522062, acc.: 70.31%] [G loss: 1.080328]\n",
            "1267 [D loss: 0.489746, acc.: 70.31%] [G loss: 1.083878]\n",
            "1268 [D loss: 0.517823, acc.: 71.88%] [G loss: 0.981396]\n",
            "1269 [D loss: 0.517777, acc.: 70.31%] [G loss: 1.026559]\n",
            "1270 [D loss: 0.538609, acc.: 70.31%] [G loss: 0.950010]\n",
            "1271 [D loss: 0.519591, acc.: 70.31%] [G loss: 1.138475]\n",
            "1272 [D loss: 0.578486, acc.: 68.75%] [G loss: 1.024475]\n",
            "1273 [D loss: 0.569540, acc.: 68.75%] [G loss: 0.953609]\n",
            "1274 [D loss: 0.553255, acc.: 68.75%] [G loss: 1.105234]\n",
            "1275 [D loss: 0.577222, acc.: 68.75%] [G loss: 0.983016]\n",
            "1276 [D loss: 0.553911, acc.: 70.31%] [G loss: 1.061766]\n",
            "1277 [D loss: 0.593731, acc.: 67.19%] [G loss: 1.071997]\n",
            "1278 [D loss: 0.592344, acc.: 60.94%] [G loss: 1.049037]\n",
            "1279 [D loss: 0.522688, acc.: 67.19%] [G loss: 1.109633]\n",
            "1280 [D loss: 0.585352, acc.: 64.06%] [G loss: 1.134375]\n",
            "1281 [D loss: 0.567116, acc.: 65.62%] [G loss: 1.011435]\n",
            "1282 [D loss: 0.527052, acc.: 67.19%] [G loss: 1.273342]\n",
            "1283 [D loss: 0.522073, acc.: 70.31%] [G loss: 1.224892]\n",
            "1284 [D loss: 0.571132, acc.: 64.06%] [G loss: 0.965586]\n",
            "1285 [D loss: 0.526141, acc.: 71.88%] [G loss: 1.021783]\n",
            "1286 [D loss: 0.486403, acc.: 76.56%] [G loss: 1.175568]\n",
            "1287 [D loss: 0.551172, acc.: 67.19%] [G loss: 1.052327]\n",
            "1288 [D loss: 0.588261, acc.: 65.62%] [G loss: 1.111049]\n",
            "1289 [D loss: 0.593331, acc.: 64.06%] [G loss: 1.241242]\n",
            "1290 [D loss: 0.534378, acc.: 71.88%] [G loss: 1.119807]\n",
            "1291 [D loss: 0.584452, acc.: 68.75%] [G loss: 0.916787]\n",
            "1292 [D loss: 0.541854, acc.: 73.44%] [G loss: 1.046071]\n",
            "1293 [D loss: 0.550670, acc.: 65.62%] [G loss: 1.083400]\n",
            "1294 [D loss: 0.590402, acc.: 65.62%] [G loss: 1.007558]\n",
            "1295 [D loss: 0.564238, acc.: 64.06%] [G loss: 0.995537]\n",
            "1296 [D loss: 0.511372, acc.: 67.19%] [G loss: 1.041293]\n",
            "1297 [D loss: 0.541447, acc.: 67.19%] [G loss: 1.041853]\n",
            "1298 [D loss: 0.524504, acc.: 68.75%] [G loss: 0.980946]\n",
            "1299 [D loss: 0.532528, acc.: 67.19%] [G loss: 0.951407]\n",
            "1300 [D loss: 0.529430, acc.: 67.19%] [G loss: 0.977380]\n",
            "generated_data\n",
            "1301 [D loss: 0.527509, acc.: 68.75%] [G loss: 1.084248]\n",
            "1302 [D loss: 0.507917, acc.: 73.44%] [G loss: 1.044573]\n",
            "1303 [D loss: 0.552213, acc.: 75.00%] [G loss: 0.950628]\n",
            "1304 [D loss: 0.521352, acc.: 78.12%] [G loss: 0.954907]\n",
            "1305 [D loss: 0.518485, acc.: 71.88%] [G loss: 1.037138]\n",
            "1306 [D loss: 0.547871, acc.: 64.06%] [G loss: 1.040966]\n",
            "1307 [D loss: 0.534108, acc.: 67.19%] [G loss: 0.941733]\n",
            "1308 [D loss: 0.552875, acc.: 67.19%] [G loss: 1.062230]\n",
            "1309 [D loss: 0.523440, acc.: 71.88%] [G loss: 1.028998]\n",
            "1310 [D loss: 0.555591, acc.: 60.94%] [G loss: 1.022892]\n",
            "1311 [D loss: 0.539215, acc.: 71.88%] [G loss: 0.978129]\n",
            "1312 [D loss: 0.636026, acc.: 64.06%] [G loss: 1.088802]\n",
            "1313 [D loss: 0.536866, acc.: 70.31%] [G loss: 1.055576]\n",
            "1314 [D loss: 0.554513, acc.: 68.75%] [G loss: 0.987422]\n",
            "1315 [D loss: 0.550009, acc.: 67.19%] [G loss: 1.035581]\n",
            "1316 [D loss: 0.531321, acc.: 68.75%] [G loss: 0.912257]\n",
            "1317 [D loss: 0.544469, acc.: 68.75%] [G loss: 1.047841]\n",
            "1318 [D loss: 0.548634, acc.: 64.06%] [G loss: 0.865492]\n",
            "1319 [D loss: 0.513124, acc.: 71.88%] [G loss: 1.075983]\n",
            "1320 [D loss: 0.590506, acc.: 59.38%] [G loss: 0.927065]\n",
            "1321 [D loss: 0.547503, acc.: 60.94%] [G loss: 0.942535]\n",
            "1322 [D loss: 0.551698, acc.: 64.06%] [G loss: 1.095947]\n",
            "1323 [D loss: 0.595996, acc.: 64.06%] [G loss: 1.062239]\n",
            "1324 [D loss: 0.561830, acc.: 64.06%] [G loss: 1.036778]\n",
            "1325 [D loss: 0.617633, acc.: 62.50%] [G loss: 1.054671]\n",
            "1326 [D loss: 0.619332, acc.: 53.12%] [G loss: 1.061096]\n",
            "1327 [D loss: 0.564289, acc.: 65.62%] [G loss: 0.914466]\n",
            "1328 [D loss: 0.520814, acc.: 73.44%] [G loss: 1.106750]\n",
            "1329 [D loss: 0.542241, acc.: 75.00%] [G loss: 1.090045]\n",
            "1330 [D loss: 0.541202, acc.: 71.88%] [G loss: 1.109990]\n",
            "1331 [D loss: 0.538695, acc.: 71.88%] [G loss: 1.054654]\n",
            "1332 [D loss: 0.603589, acc.: 62.50%] [G loss: 1.244722]\n",
            "1333 [D loss: 0.588478, acc.: 68.75%] [G loss: 1.100439]\n",
            "1334 [D loss: 0.533616, acc.: 67.19%] [G loss: 1.160359]\n",
            "1335 [D loss: 0.516657, acc.: 70.31%] [G loss: 1.312210]\n",
            "1336 [D loss: 0.557128, acc.: 70.31%] [G loss: 0.975708]\n",
            "1337 [D loss: 0.511576, acc.: 71.88%] [G loss: 1.064743]\n",
            "1338 [D loss: 0.571703, acc.: 68.75%] [G loss: 1.210172]\n",
            "1339 [D loss: 0.557235, acc.: 67.19%] [G loss: 0.971388]\n",
            "1340 [D loss: 0.590924, acc.: 65.62%] [G loss: 1.149919]\n",
            "1341 [D loss: 0.553304, acc.: 70.31%] [G loss: 1.087414]\n",
            "1342 [D loss: 0.566220, acc.: 68.75%] [G loss: 1.052795]\n",
            "1343 [D loss: 0.525237, acc.: 68.75%] [G loss: 1.197082]\n",
            "1344 [D loss: 0.535631, acc.: 70.31%] [G loss: 1.067703]\n",
            "1345 [D loss: 0.510923, acc.: 65.62%] [G loss: 0.909309]\n",
            "1346 [D loss: 0.590657, acc.: 70.31%] [G loss: 0.986714]\n",
            "1347 [D loss: 0.581006, acc.: 67.19%] [G loss: 1.055268]\n",
            "1348 [D loss: 0.561591, acc.: 65.62%] [G loss: 0.877759]\n",
            "1349 [D loss: 0.571076, acc.: 70.31%] [G loss: 0.934813]\n",
            "1350 [D loss: 0.542337, acc.: 71.88%] [G loss: 0.977304]\n",
            "1351 [D loss: 0.598661, acc.: 60.94%] [G loss: 1.061590]\n",
            "1352 [D loss: 0.546528, acc.: 68.75%] [G loss: 1.128036]\n",
            "1353 [D loss: 0.511388, acc.: 75.00%] [G loss: 1.094400]\n",
            "1354 [D loss: 0.549537, acc.: 71.88%] [G loss: 1.175862]\n",
            "1355 [D loss: 0.504757, acc.: 78.12%] [G loss: 1.116290]\n",
            "1356 [D loss: 0.554861, acc.: 65.62%] [G loss: 1.075052]\n",
            "1357 [D loss: 0.487109, acc.: 71.88%] [G loss: 1.286431]\n",
            "1358 [D loss: 0.513844, acc.: 70.31%] [G loss: 0.866581]\n",
            "1359 [D loss: 0.515516, acc.: 78.12%] [G loss: 1.101015]\n",
            "1360 [D loss: 0.548569, acc.: 70.31%] [G loss: 1.073448]\n",
            "1361 [D loss: 0.551089, acc.: 67.19%] [G loss: 0.995332]\n",
            "1362 [D loss: 0.570307, acc.: 70.31%] [G loss: 0.976551]\n",
            "1363 [D loss: 0.559313, acc.: 68.75%] [G loss: 0.996049]\n",
            "1364 [D loss: 0.587839, acc.: 68.75%] [G loss: 1.021467]\n",
            "1365 [D loss: 0.540308, acc.: 70.31%] [G loss: 0.957051]\n",
            "1366 [D loss: 0.551749, acc.: 65.62%] [G loss: 1.032470]\n",
            "1367 [D loss: 0.560131, acc.: 64.06%] [G loss: 1.166470]\n",
            "1368 [D loss: 0.542712, acc.: 67.19%] [G loss: 1.093749]\n",
            "1369 [D loss: 0.557234, acc.: 70.31%] [G loss: 1.171428]\n",
            "1370 [D loss: 0.552764, acc.: 68.75%] [G loss: 0.979676]\n",
            "1371 [D loss: 0.588365, acc.: 62.50%] [G loss: 1.086653]\n",
            "1372 [D loss: 0.566676, acc.: 64.06%] [G loss: 0.954754]\n",
            "1373 [D loss: 0.542434, acc.: 71.88%] [G loss: 1.189332]\n",
            "1374 [D loss: 0.589196, acc.: 67.19%] [G loss: 1.015524]\n",
            "1375 [D loss: 0.540111, acc.: 67.19%] [G loss: 1.115301]\n",
            "1376 [D loss: 0.550677, acc.: 70.31%] [G loss: 1.079251]\n",
            "1377 [D loss: 0.563371, acc.: 70.31%] [G loss: 1.092645]\n",
            "1378 [D loss: 0.567874, acc.: 67.19%] [G loss: 1.028776]\n",
            "1379 [D loss: 0.526252, acc.: 73.44%] [G loss: 0.913137]\n",
            "1380 [D loss: 0.593972, acc.: 67.19%] [G loss: 1.092202]\n",
            "1381 [D loss: 0.527909, acc.: 73.44%] [G loss: 1.052235]\n",
            "1382 [D loss: 0.565111, acc.: 64.06%] [G loss: 0.958533]\n",
            "1383 [D loss: 0.566722, acc.: 67.19%] [G loss: 1.099804]\n",
            "1384 [D loss: 0.543338, acc.: 67.19%] [G loss: 1.102630]\n",
            "1385 [D loss: 0.530854, acc.: 67.19%] [G loss: 1.013147]\n",
            "1386 [D loss: 0.552604, acc.: 71.88%] [G loss: 0.909532]\n",
            "1387 [D loss: 0.560130, acc.: 71.88%] [G loss: 1.025681]\n",
            "1388 [D loss: 0.583635, acc.: 65.62%] [G loss: 0.976895]\n",
            "1389 [D loss: 0.517421, acc.: 70.31%] [G loss: 1.157906]\n",
            "1390 [D loss: 0.559347, acc.: 70.31%] [G loss: 1.051600]\n",
            "1391 [D loss: 0.553251, acc.: 59.38%] [G loss: 0.989990]\n",
            "1392 [D loss: 0.530159, acc.: 64.06%] [G loss: 1.135985]\n",
            "1393 [D loss: 0.548538, acc.: 67.19%] [G loss: 0.992284]\n",
            "1394 [D loss: 0.541407, acc.: 68.75%] [G loss: 1.081490]\n",
            "1395 [D loss: 0.579855, acc.: 64.06%] [G loss: 1.128171]\n",
            "1396 [D loss: 0.552138, acc.: 67.19%] [G loss: 1.323285]\n",
            "1397 [D loss: 0.582578, acc.: 65.62%] [G loss: 1.044011]\n",
            "1398 [D loss: 0.585415, acc.: 59.38%] [G loss: 1.180573]\n",
            "1399 [D loss: 0.567036, acc.: 67.19%] [G loss: 1.161348]\n",
            "1400 [D loss: 0.558289, acc.: 67.19%] [G loss: 1.215016]\n",
            "generated_data\n",
            "1401 [D loss: 0.530585, acc.: 73.44%] [G loss: 1.085064]\n",
            "1402 [D loss: 0.507226, acc.: 71.88%] [G loss: 1.174911]\n",
            "1403 [D loss: 0.568808, acc.: 70.31%] [G loss: 1.158751]\n",
            "1404 [D loss: 0.560387, acc.: 70.31%] [G loss: 1.186797]\n",
            "1405 [D loss: 0.538128, acc.: 73.44%] [G loss: 1.030960]\n",
            "1406 [D loss: 0.563865, acc.: 62.50%] [G loss: 1.073417]\n",
            "1407 [D loss: 0.562208, acc.: 68.75%] [G loss: 1.058514]\n",
            "1408 [D loss: 0.558483, acc.: 65.62%] [G loss: 1.140003]\n",
            "1409 [D loss: 0.566987, acc.: 67.19%] [G loss: 1.097093]\n",
            "1410 [D loss: 0.536563, acc.: 70.31%] [G loss: 1.172726]\n",
            "1411 [D loss: 0.538514, acc.: 70.31%] [G loss: 1.105337]\n",
            "1412 [D loss: 0.532271, acc.: 68.75%] [G loss: 1.143057]\n",
            "1413 [D loss: 0.528061, acc.: 71.88%] [G loss: 1.118794]\n",
            "1414 [D loss: 0.550263, acc.: 68.75%] [G loss: 1.235230]\n",
            "1415 [D loss: 0.636946, acc.: 59.38%] [G loss: 0.884817]\n",
            "1416 [D loss: 0.533715, acc.: 68.75%] [G loss: 1.014387]\n",
            "1417 [D loss: 0.549691, acc.: 65.62%] [G loss: 0.953942]\n",
            "1418 [D loss: 0.550293, acc.: 67.19%] [G loss: 1.051226]\n",
            "1419 [D loss: 0.559144, acc.: 65.62%] [G loss: 1.124304]\n",
            "1420 [D loss: 0.509280, acc.: 70.31%] [G loss: 1.068929]\n",
            "1421 [D loss: 0.589767, acc.: 65.62%] [G loss: 0.948238]\n",
            "1422 [D loss: 0.545981, acc.: 67.19%] [G loss: 0.952136]\n",
            "1423 [D loss: 0.569917, acc.: 65.62%] [G loss: 1.146577]\n",
            "1424 [D loss: 0.542333, acc.: 73.44%] [G loss: 1.028830]\n",
            "1425 [D loss: 0.592273, acc.: 64.06%] [G loss: 0.942310]\n",
            "1426 [D loss: 0.543485, acc.: 67.19%] [G loss: 1.021028]\n",
            "1427 [D loss: 0.528390, acc.: 70.31%] [G loss: 1.026564]\n",
            "1428 [D loss: 0.534957, acc.: 73.44%] [G loss: 1.071980]\n",
            "1429 [D loss: 0.561325, acc.: 70.31%] [G loss: 1.025621]\n",
            "1430 [D loss: 0.565043, acc.: 56.25%] [G loss: 1.067494]\n",
            "1431 [D loss: 0.555572, acc.: 62.50%] [G loss: 1.012688]\n",
            "1432 [D loss: 0.520922, acc.: 70.31%] [G loss: 1.143001]\n",
            "1433 [D loss: 0.573743, acc.: 67.19%] [G loss: 1.078326]\n",
            "1434 [D loss: 0.522960, acc.: 70.31%] [G loss: 0.978319]\n",
            "1435 [D loss: 0.524683, acc.: 73.44%] [G loss: 1.033065]\n",
            "1436 [D loss: 0.516712, acc.: 71.88%] [G loss: 1.041796]\n",
            "1437 [D loss: 0.563928, acc.: 67.19%] [G loss: 1.057150]\n",
            "1438 [D loss: 0.498949, acc.: 75.00%] [G loss: 1.097320]\n",
            "1439 [D loss: 0.520888, acc.: 71.88%] [G loss: 1.029060]\n",
            "1440 [D loss: 0.544795, acc.: 65.62%] [G loss: 1.305641]\n",
            "1441 [D loss: 0.508719, acc.: 73.44%] [G loss: 1.181287]\n",
            "1442 [D loss: 0.519935, acc.: 68.75%] [G loss: 1.045936]\n",
            "1443 [D loss: 0.534860, acc.: 70.31%] [G loss: 0.874418]\n",
            "1444 [D loss: 0.508956, acc.: 73.44%] [G loss: 0.960871]\n",
            "1445 [D loss: 0.477137, acc.: 79.69%] [G loss: 1.150691]\n",
            "1446 [D loss: 0.575081, acc.: 70.31%] [G loss: 0.872766]\n",
            "1447 [D loss: 0.499465, acc.: 76.56%] [G loss: 1.038331]\n",
            "1448 [D loss: 0.534032, acc.: 68.75%] [G loss: 0.964223]\n",
            "1449 [D loss: 0.513412, acc.: 70.31%] [G loss: 1.046254]\n",
            "1450 [D loss: 0.495984, acc.: 73.44%] [G loss: 0.969183]\n",
            "1451 [D loss: 0.556194, acc.: 68.75%] [G loss: 1.053657]\n",
            "1452 [D loss: 0.543867, acc.: 67.19%] [G loss: 1.116188]\n",
            "1453 [D loss: 0.546920, acc.: 65.62%] [G loss: 1.078018]\n",
            "1454 [D loss: 0.530014, acc.: 75.00%] [G loss: 1.105441]\n",
            "1455 [D loss: 0.557660, acc.: 67.19%] [G loss: 0.953751]\n",
            "1456 [D loss: 0.533819, acc.: 71.88%] [G loss: 1.151507]\n",
            "1457 [D loss: 0.495346, acc.: 76.56%] [G loss: 1.065941]\n",
            "1458 [D loss: 0.595161, acc.: 65.62%] [G loss: 1.066890]\n",
            "1459 [D loss: 0.562683, acc.: 71.88%] [G loss: 1.165077]\n",
            "1460 [D loss: 0.552488, acc.: 68.75%] [G loss: 1.126136]\n",
            "1461 [D loss: 0.572447, acc.: 67.19%] [G loss: 0.977516]\n",
            "1462 [D loss: 0.528442, acc.: 67.19%] [G loss: 1.263844]\n",
            "1463 [D loss: 0.493388, acc.: 73.44%] [G loss: 1.115116]\n",
            "1464 [D loss: 0.584286, acc.: 65.62%] [G loss: 1.064695]\n",
            "1465 [D loss: 0.537479, acc.: 68.75%] [G loss: 1.084499]\n",
            "1466 [D loss: 0.529967, acc.: 71.88%] [G loss: 1.025825]\n",
            "1467 [D loss: 0.555142, acc.: 70.31%] [G loss: 1.144724]\n",
            "1468 [D loss: 0.549074, acc.: 70.31%] [G loss: 1.122655]\n",
            "1469 [D loss: 0.555191, acc.: 64.06%] [G loss: 0.944728]\n",
            "1470 [D loss: 0.544479, acc.: 64.06%] [G loss: 1.085612]\n",
            "1471 [D loss: 0.596745, acc.: 65.62%] [G loss: 1.036101]\n",
            "1472 [D loss: 0.551854, acc.: 64.06%] [G loss: 1.021253]\n",
            "1473 [D loss: 0.574649, acc.: 65.62%] [G loss: 0.974460]\n",
            "1474 [D loss: 0.539168, acc.: 70.31%] [G loss: 0.977568]\n",
            "1475 [D loss: 0.537903, acc.: 67.19%] [G loss: 0.954138]\n",
            "1476 [D loss: 0.544423, acc.: 71.88%] [G loss: 1.119596]\n",
            "1477 [D loss: 0.573969, acc.: 71.88%] [G loss: 1.014378]\n",
            "1478 [D loss: 0.564099, acc.: 64.06%] [G loss: 1.150002]\n",
            "1479 [D loss: 0.579344, acc.: 68.75%] [G loss: 0.984331]\n",
            "1480 [D loss: 0.566064, acc.: 65.62%] [G loss: 0.999048]\n",
            "1481 [D loss: 0.535052, acc.: 68.75%] [G loss: 1.068279]\n",
            "1482 [D loss: 0.535560, acc.: 71.88%] [G loss: 0.943092]\n",
            "1483 [D loss: 0.543849, acc.: 70.31%] [G loss: 0.971901]\n",
            "1484 [D loss: 0.544419, acc.: 70.31%] [G loss: 1.012837]\n",
            "1485 [D loss: 0.577582, acc.: 64.06%] [G loss: 1.002439]\n",
            "1486 [D loss: 0.554702, acc.: 64.06%] [G loss: 1.211292]\n",
            "1487 [D loss: 0.582386, acc.: 70.31%] [G loss: 1.088681]\n",
            "1488 [D loss: 0.542001, acc.: 68.75%] [G loss: 1.054268]\n",
            "1489 [D loss: 0.645747, acc.: 57.81%] [G loss: 1.131066]\n",
            "1490 [D loss: 0.695885, acc.: 57.81%] [G loss: 1.049139]\n",
            "1491 [D loss: 0.510920, acc.: 73.44%] [G loss: 1.257594]\n",
            "1492 [D loss: 0.574604, acc.: 65.62%] [G loss: 1.000526]\n",
            "1493 [D loss: 0.567897, acc.: 70.31%] [G loss: 1.026034]\n",
            "1494 [D loss: 0.551628, acc.: 70.31%] [G loss: 1.260045]\n",
            "1495 [D loss: 0.593307, acc.: 64.06%] [G loss: 1.019170]\n",
            "1496 [D loss: 0.554331, acc.: 68.75%] [G loss: 1.169708]\n",
            "1497 [D loss: 0.564560, acc.: 67.19%] [G loss: 1.072057]\n",
            "1498 [D loss: 0.559108, acc.: 68.75%] [G loss: 1.341339]\n",
            "1499 [D loss: 0.527910, acc.: 68.75%] [G loss: 1.140773]\n",
            "1500 [D loss: 0.587568, acc.: 64.06%] [G loss: 0.985940]\n",
            "generated_data\n",
            "1501 [D loss: 0.524566, acc.: 73.44%] [G loss: 1.183053]\n",
            "1502 [D loss: 0.514600, acc.: 76.56%] [G loss: 1.171704]\n",
            "1503 [D loss: 0.577800, acc.: 64.06%] [G loss: 1.003095]\n",
            "1504 [D loss: 0.488184, acc.: 70.31%] [G loss: 1.141234]\n",
            "1505 [D loss: 0.616903, acc.: 62.50%] [G loss: 1.093802]\n",
            "1506 [D loss: 0.591054, acc.: 62.50%] [G loss: 1.189201]\n",
            "1507 [D loss: 0.495694, acc.: 73.44%] [G loss: 1.166458]\n",
            "1508 [D loss: 0.596972, acc.: 68.75%] [G loss: 0.923800]\n",
            "1509 [D loss: 0.511951, acc.: 73.44%] [G loss: 1.190286]\n",
            "1510 [D loss: 0.568352, acc.: 68.75%] [G loss: 1.128486]\n",
            "1511 [D loss: 0.538808, acc.: 70.31%] [G loss: 1.084674]\n",
            "1512 [D loss: 0.592293, acc.: 70.31%] [G loss: 1.092801]\n",
            "1513 [D loss: 0.546586, acc.: 65.62%] [G loss: 1.217611]\n",
            "1514 [D loss: 0.593524, acc.: 68.75%] [G loss: 1.124406]\n",
            "1515 [D loss: 0.541524, acc.: 65.62%] [G loss: 1.173050]\n",
            "1516 [D loss: 0.515873, acc.: 73.44%] [G loss: 1.042162]\n",
            "1517 [D loss: 0.528803, acc.: 75.00%] [G loss: 1.074430]\n",
            "1518 [D loss: 0.522982, acc.: 65.62%] [G loss: 1.139454]\n",
            "1519 [D loss: 0.560216, acc.: 67.19%] [G loss: 0.978572]\n",
            "1520 [D loss: 0.540321, acc.: 68.75%] [G loss: 1.091175]\n",
            "1521 [D loss: 0.524303, acc.: 70.31%] [G loss: 1.115124]\n",
            "1522 [D loss: 0.617213, acc.: 62.50%] [G loss: 1.141314]\n",
            "1523 [D loss: 0.579290, acc.: 64.06%] [G loss: 1.135500]\n",
            "1524 [D loss: 0.543834, acc.: 67.19%] [G loss: 1.028283]\n",
            "1525 [D loss: 0.543627, acc.: 62.50%] [G loss: 1.090448]\n",
            "1526 [D loss: 0.537168, acc.: 73.44%] [G loss: 1.157900]\n",
            "1527 [D loss: 0.531102, acc.: 68.75%] [G loss: 1.067365]\n",
            "1528 [D loss: 0.500889, acc.: 71.88%] [G loss: 1.121730]\n",
            "1529 [D loss: 0.543115, acc.: 67.19%] [G loss: 1.049678]\n",
            "1530 [D loss: 0.563726, acc.: 67.19%] [G loss: 0.946679]\n",
            "1531 [D loss: 0.558182, acc.: 73.44%] [G loss: 1.242720]\n",
            "1532 [D loss: 0.528982, acc.: 70.31%] [G loss: 1.009915]\n",
            "1533 [D loss: 0.541885, acc.: 71.88%] [G loss: 1.006946]\n",
            "1534 [D loss: 0.496354, acc.: 78.12%] [G loss: 1.089848]\n",
            "1535 [D loss: 0.576779, acc.: 68.75%] [G loss: 1.004271]\n",
            "1536 [D loss: 0.535532, acc.: 75.00%] [G loss: 1.036853]\n",
            "1537 [D loss: 0.589050, acc.: 65.62%] [G loss: 1.014172]\n",
            "1538 [D loss: 0.529559, acc.: 76.56%] [G loss: 1.082868]\n",
            "1539 [D loss: 0.604314, acc.: 64.06%] [G loss: 0.997135]\n",
            "1540 [D loss: 0.553513, acc.: 67.19%] [G loss: 1.032097]\n",
            "1541 [D loss: 0.530918, acc.: 71.88%] [G loss: 1.017133]\n",
            "1542 [D loss: 0.577326, acc.: 68.75%] [G loss: 1.111541]\n",
            "1543 [D loss: 0.559947, acc.: 68.75%] [G loss: 1.054415]\n",
            "1544 [D loss: 0.557350, acc.: 73.44%] [G loss: 1.032909]\n",
            "1545 [D loss: 0.531148, acc.: 71.88%] [G loss: 1.096048]\n",
            "1546 [D loss: 0.549601, acc.: 68.75%] [G loss: 1.016370]\n",
            "1547 [D loss: 0.572686, acc.: 70.31%] [G loss: 0.986944]\n",
            "1548 [D loss: 0.538526, acc.: 68.75%] [G loss: 1.134748]\n",
            "1549 [D loss: 0.523596, acc.: 68.75%] [G loss: 0.973887]\n",
            "1550 [D loss: 0.586401, acc.: 64.06%] [G loss: 0.990952]\n",
            "1551 [D loss: 0.518246, acc.: 70.31%] [G loss: 1.045898]\n",
            "1552 [D loss: 0.566246, acc.: 62.50%] [G loss: 1.101983]\n",
            "1553 [D loss: 0.547067, acc.: 70.31%] [G loss: 1.089665]\n",
            "1554 [D loss: 0.533895, acc.: 73.44%] [G loss: 1.116616]\n",
            "1555 [D loss: 0.535033, acc.: 71.88%] [G loss: 1.126662]\n",
            "1556 [D loss: 0.521198, acc.: 73.44%] [G loss: 1.017517]\n",
            "1557 [D loss: 0.529016, acc.: 68.75%] [G loss: 1.123579]\n",
            "1558 [D loss: 0.560886, acc.: 65.62%] [G loss: 1.001676]\n",
            "1559 [D loss: 0.527984, acc.: 73.44%] [G loss: 1.024193]\n",
            "1560 [D loss: 0.502579, acc.: 71.88%] [G loss: 1.385094]\n",
            "1561 [D loss: 0.541913, acc.: 71.88%] [G loss: 0.978193]\n",
            "1562 [D loss: 0.473627, acc.: 76.56%] [G loss: 1.226154]\n",
            "1563 [D loss: 0.508847, acc.: 68.75%] [G loss: 1.113290]\n",
            "1564 [D loss: 0.498060, acc.: 73.44%] [G loss: 1.100420]\n",
            "1565 [D loss: 0.478876, acc.: 76.56%] [G loss: 1.188313]\n",
            "1566 [D loss: 0.529588, acc.: 70.31%] [G loss: 1.009550]\n",
            "1567 [D loss: 0.512998, acc.: 73.44%] [G loss: 1.019771]\n",
            "1568 [D loss: 0.461767, acc.: 76.56%] [G loss: 1.248059]\n",
            "1569 [D loss: 0.565977, acc.: 73.44%] [G loss: 1.090691]\n",
            "1570 [D loss: 0.540985, acc.: 65.62%] [G loss: 1.269590]\n",
            "1571 [D loss: 0.516049, acc.: 71.88%] [G loss: 1.069882]\n",
            "1572 [D loss: 0.497147, acc.: 71.88%] [G loss: 1.069111]\n",
            "1573 [D loss: 0.507841, acc.: 73.44%] [G loss: 1.114881]\n",
            "1574 [D loss: 0.595217, acc.: 65.62%] [G loss: 1.185206]\n",
            "1575 [D loss: 0.515343, acc.: 75.00%] [G loss: 1.058052]\n",
            "1576 [D loss: 0.485276, acc.: 76.56%] [G loss: 1.066364]\n",
            "1577 [D loss: 0.515142, acc.: 70.31%] [G loss: 1.019758]\n",
            "1578 [D loss: 0.516861, acc.: 73.44%] [G loss: 1.034330]\n",
            "1579 [D loss: 0.536519, acc.: 68.75%] [G loss: 1.163832]\n",
            "1580 [D loss: 0.540055, acc.: 67.19%] [G loss: 0.976053]\n",
            "1581 [D loss: 0.527256, acc.: 67.19%] [G loss: 1.095320]\n",
            "1582 [D loss: 0.503775, acc.: 75.00%] [G loss: 1.139093]\n",
            "1583 [D loss: 0.594247, acc.: 62.50%] [G loss: 1.129253]\n",
            "1584 [D loss: 0.492664, acc.: 71.88%] [G loss: 1.254706]\n",
            "1585 [D loss: 0.562729, acc.: 65.62%] [G loss: 1.072229]\n",
            "1586 [D loss: 0.543388, acc.: 68.75%] [G loss: 1.170314]\n",
            "1587 [D loss: 0.568097, acc.: 68.75%] [G loss: 1.141068]\n",
            "1588 [D loss: 0.515693, acc.: 68.75%] [G loss: 1.133687]\n",
            "1589 [D loss: 0.540892, acc.: 73.44%] [G loss: 1.047031]\n",
            "1590 [D loss: 0.537595, acc.: 70.31%] [G loss: 1.132442]\n",
            "1591 [D loss: 0.606517, acc.: 59.38%] [G loss: 1.198133]\n",
            "1592 [D loss: 0.544168, acc.: 70.31%] [G loss: 1.072375]\n",
            "1593 [D loss: 0.540452, acc.: 65.62%] [G loss: 1.076494]\n",
            "1594 [D loss: 0.530581, acc.: 71.88%] [G loss: 1.110304]\n",
            "1595 [D loss: 0.520702, acc.: 75.00%] [G loss: 1.122650]\n",
            "1596 [D loss: 0.515083, acc.: 75.00%] [G loss: 1.047782]\n",
            "1597 [D loss: 0.527606, acc.: 70.31%] [G loss: 1.081282]\n",
            "1598 [D loss: 0.529342, acc.: 71.88%] [G loss: 1.274014]\n",
            "1599 [D loss: 0.566177, acc.: 71.88%] [G loss: 1.092003]\n",
            "1600 [D loss: 0.487950, acc.: 75.00%] [G loss: 1.136212]\n",
            "generated_data\n",
            "1601 [D loss: 0.529382, acc.: 70.31%] [G loss: 1.014603]\n",
            "1602 [D loss: 0.514935, acc.: 70.31%] [G loss: 1.002986]\n",
            "1603 [D loss: 0.470970, acc.: 78.12%] [G loss: 1.226308]\n",
            "1604 [D loss: 0.535012, acc.: 68.75%] [G loss: 1.046872]\n",
            "1605 [D loss: 0.569261, acc.: 65.62%] [G loss: 1.173507]\n",
            "1606 [D loss: 0.478481, acc.: 67.19%] [G loss: 1.220105]\n",
            "1607 [D loss: 0.533189, acc.: 65.62%] [G loss: 1.063620]\n",
            "1608 [D loss: 0.531409, acc.: 71.88%] [G loss: 1.245043]\n",
            "1609 [D loss: 0.567421, acc.: 62.50%] [G loss: 1.162561]\n",
            "1610 [D loss: 0.512378, acc.: 73.44%] [G loss: 1.149442]\n",
            "1611 [D loss: 0.529624, acc.: 73.44%] [G loss: 1.181068]\n",
            "1612 [D loss: 0.539913, acc.: 70.31%] [G loss: 1.145741]\n",
            "1613 [D loss: 0.514929, acc.: 73.44%] [G loss: 1.266305]\n",
            "1614 [D loss: 0.513293, acc.: 71.88%] [G loss: 1.087081]\n",
            "1615 [D loss: 0.496850, acc.: 70.31%] [G loss: 1.138556]\n",
            "1616 [D loss: 0.493840, acc.: 76.56%] [G loss: 1.111987]\n",
            "1617 [D loss: 0.538037, acc.: 68.75%] [G loss: 1.049730]\n",
            "1618 [D loss: 0.482942, acc.: 71.88%] [G loss: 1.101676]\n",
            "1619 [D loss: 0.502725, acc.: 67.19%] [G loss: 0.949882]\n",
            "1620 [D loss: 0.519978, acc.: 70.31%] [G loss: 0.943124]\n",
            "1621 [D loss: 0.495100, acc.: 71.88%] [G loss: 1.154695]\n",
            "1622 [D loss: 0.515538, acc.: 71.88%] [G loss: 1.093470]\n",
            "1623 [D loss: 0.553684, acc.: 71.88%] [G loss: 1.067260]\n",
            "1624 [D loss: 0.519220, acc.: 70.31%] [G loss: 1.185566]\n",
            "1625 [D loss: 0.580424, acc.: 59.38%] [G loss: 1.168263]\n",
            "1626 [D loss: 0.492604, acc.: 70.31%] [G loss: 1.117693]\n",
            "1627 [D loss: 0.569168, acc.: 75.00%] [G loss: 1.177382]\n",
            "1628 [D loss: 0.514890, acc.: 70.31%] [G loss: 1.094514]\n",
            "1629 [D loss: 0.542632, acc.: 70.31%] [G loss: 1.071745]\n",
            "1630 [D loss: 0.523424, acc.: 65.62%] [G loss: 1.215213]\n",
            "1631 [D loss: 0.532731, acc.: 67.19%] [G loss: 0.904631]\n",
            "1632 [D loss: 0.554221, acc.: 68.75%] [G loss: 1.236450]\n",
            "1633 [D loss: 0.539403, acc.: 67.19%] [G loss: 1.111562]\n",
            "1634 [D loss: 0.571655, acc.: 65.62%] [G loss: 0.919629]\n",
            "1635 [D loss: 0.493222, acc.: 71.88%] [G loss: 0.986422]\n",
            "1636 [D loss: 0.548307, acc.: 71.88%] [G loss: 0.994228]\n",
            "1637 [D loss: 0.561360, acc.: 67.19%] [G loss: 1.186201]\n",
            "1638 [D loss: 0.549337, acc.: 73.44%] [G loss: 1.012643]\n",
            "1639 [D loss: 0.526930, acc.: 70.31%] [G loss: 1.131534]\n",
            "1640 [D loss: 0.510871, acc.: 70.31%] [G loss: 1.124038]\n",
            "1641 [D loss: 0.520142, acc.: 70.31%] [G loss: 1.163489]\n",
            "1642 [D loss: 0.524529, acc.: 73.44%] [G loss: 1.151646]\n",
            "1643 [D loss: 0.485665, acc.: 79.69%] [G loss: 0.999697]\n",
            "1644 [D loss: 0.518050, acc.: 73.44%] [G loss: 1.123825]\n",
            "1645 [D loss: 0.486822, acc.: 73.44%] [G loss: 1.280322]\n",
            "1646 [D loss: 0.554010, acc.: 71.88%] [G loss: 1.164045]\n",
            "1647 [D loss: 0.540952, acc.: 67.19%] [G loss: 1.288589]\n",
            "1648 [D loss: 0.533363, acc.: 68.75%] [G loss: 1.056533]\n",
            "1649 [D loss: 0.485044, acc.: 78.12%] [G loss: 1.253332]\n",
            "1650 [D loss: 0.534483, acc.: 67.19%] [G loss: 1.308553]\n",
            "1651 [D loss: 0.512336, acc.: 73.44%] [G loss: 1.142831]\n",
            "1652 [D loss: 0.519489, acc.: 73.44%] [G loss: 1.222123]\n",
            "1653 [D loss: 0.538981, acc.: 71.88%] [G loss: 1.284141]\n",
            "1654 [D loss: 0.528052, acc.: 71.88%] [G loss: 1.408606]\n",
            "1655 [D loss: 0.461981, acc.: 75.00%] [G loss: 1.387316]\n",
            "1656 [D loss: 0.518009, acc.: 75.00%] [G loss: 1.049577]\n",
            "1657 [D loss: 0.527424, acc.: 70.31%] [G loss: 1.123692]\n",
            "1658 [D loss: 0.492797, acc.: 75.00%] [G loss: 1.075699]\n",
            "1659 [D loss: 0.517028, acc.: 68.75%] [G loss: 1.042702]\n",
            "1660 [D loss: 0.541413, acc.: 75.00%] [G loss: 1.080070]\n",
            "1661 [D loss: 0.521198, acc.: 73.44%] [G loss: 1.174426]\n",
            "1662 [D loss: 0.509025, acc.: 71.88%] [G loss: 1.069340]\n",
            "1663 [D loss: 0.544150, acc.: 71.88%] [G loss: 1.065709]\n",
            "1664 [D loss: 0.540495, acc.: 73.44%] [G loss: 1.135663]\n",
            "1665 [D loss: 0.512808, acc.: 75.00%] [G loss: 1.141548]\n",
            "1666 [D loss: 0.522260, acc.: 64.06%] [G loss: 1.033151]\n",
            "1667 [D loss: 0.558358, acc.: 65.62%] [G loss: 1.107553]\n",
            "1668 [D loss: 0.515126, acc.: 71.88%] [G loss: 1.407915]\n",
            "1669 [D loss: 0.527593, acc.: 70.31%] [G loss: 1.043081]\n",
            "1670 [D loss: 0.531651, acc.: 73.44%] [G loss: 1.193955]\n",
            "1671 [D loss: 0.499822, acc.: 75.00%] [G loss: 1.261900]\n",
            "1672 [D loss: 0.586573, acc.: 65.62%] [G loss: 0.988387]\n",
            "1673 [D loss: 0.468092, acc.: 76.56%] [G loss: 1.134828]\n",
            "1674 [D loss: 0.477524, acc.: 82.81%] [G loss: 1.126896]\n",
            "1675 [D loss: 0.549861, acc.: 73.44%] [G loss: 1.007795]\n",
            "1676 [D loss: 0.488071, acc.: 71.88%] [G loss: 1.175053]\n",
            "1677 [D loss: 0.524143, acc.: 68.75%] [G loss: 1.006699]\n",
            "1678 [D loss: 0.560875, acc.: 65.62%] [G loss: 1.144229]\n",
            "1679 [D loss: 0.562192, acc.: 65.62%] [G loss: 1.269729]\n",
            "1680 [D loss: 0.601629, acc.: 65.62%] [G loss: 1.197967]\n",
            "1681 [D loss: 0.541570, acc.: 70.31%] [G loss: 1.261841]\n",
            "1682 [D loss: 0.527716, acc.: 67.19%] [G loss: 1.245872]\n",
            "1683 [D loss: 0.496120, acc.: 71.88%] [G loss: 1.225115]\n",
            "1684 [D loss: 0.492312, acc.: 76.56%] [G loss: 1.250575]\n",
            "1685 [D loss: 0.561575, acc.: 65.62%] [G loss: 1.020404]\n",
            "1686 [D loss: 0.552799, acc.: 68.75%] [G loss: 1.137995]\n",
            "1687 [D loss: 0.521993, acc.: 62.50%] [G loss: 1.049646]\n",
            "1688 [D loss: 0.530353, acc.: 75.00%] [G loss: 1.135748]\n",
            "1689 [D loss: 0.509977, acc.: 76.56%] [G loss: 1.073569]\n",
            "1690 [D loss: 0.531572, acc.: 68.75%] [G loss: 0.991500]\n",
            "1691 [D loss: 0.566888, acc.: 71.88%] [G loss: 1.242230]\n",
            "1692 [D loss: 0.522704, acc.: 73.44%] [G loss: 1.216244]\n",
            "1693 [D loss: 0.515167, acc.: 73.44%] [G loss: 1.308623]\n",
            "1694 [D loss: 0.531620, acc.: 75.00%] [G loss: 1.172038]\n",
            "1695 [D loss: 0.548895, acc.: 67.19%] [G loss: 1.279198]\n",
            "1696 [D loss: 0.497305, acc.: 70.31%] [G loss: 1.487880]\n",
            "1697 [D loss: 0.552364, acc.: 64.06%] [G loss: 1.248874]\n",
            "1698 [D loss: 0.506838, acc.: 75.00%] [G loss: 1.245275]\n",
            "1699 [D loss: 0.467045, acc.: 78.12%] [G loss: 1.191341]\n",
            "1700 [D loss: 0.502850, acc.: 70.31%] [G loss: 1.056958]\n",
            "generated_data\n",
            "1701 [D loss: 0.457945, acc.: 76.56%] [G loss: 1.203692]\n",
            "1702 [D loss: 0.463667, acc.: 75.00%] [G loss: 1.382284]\n",
            "1703 [D loss: 0.498729, acc.: 73.44%] [G loss: 1.240771]\n",
            "1704 [D loss: 0.525178, acc.: 68.75%] [G loss: 1.080612]\n",
            "1705 [D loss: 0.479839, acc.: 79.69%] [G loss: 1.086723]\n",
            "1706 [D loss: 0.540913, acc.: 76.56%] [G loss: 1.276129]\n",
            "1707 [D loss: 0.570406, acc.: 67.19%] [G loss: 0.976879]\n",
            "1708 [D loss: 0.448263, acc.: 81.25%] [G loss: 1.072195]\n",
            "1709 [D loss: 0.513221, acc.: 68.75%] [G loss: 1.062359]\n",
            "1710 [D loss: 0.485230, acc.: 73.44%] [G loss: 1.107241]\n",
            "1711 [D loss: 0.533067, acc.: 75.00%] [G loss: 1.162378]\n",
            "1712 [D loss: 0.546609, acc.: 62.50%] [G loss: 1.082638]\n",
            "1713 [D loss: 0.517349, acc.: 67.19%] [G loss: 1.213156]\n",
            "1714 [D loss: 0.546810, acc.: 73.44%] [G loss: 1.146898]\n",
            "1715 [D loss: 0.620286, acc.: 54.69%] [G loss: 1.052794]\n",
            "1716 [D loss: 0.528820, acc.: 70.31%] [G loss: 1.102028]\n",
            "1717 [D loss: 0.524032, acc.: 71.88%] [G loss: 1.184166]\n",
            "1718 [D loss: 0.562592, acc.: 65.62%] [G loss: 1.214625]\n",
            "1719 [D loss: 0.544876, acc.: 65.62%] [G loss: 1.138352]\n",
            "1720 [D loss: 0.570120, acc.: 64.06%] [G loss: 1.146703]\n",
            "1721 [D loss: 0.469366, acc.: 76.56%] [G loss: 1.230847]\n",
            "1722 [D loss: 0.583142, acc.: 65.62%] [G loss: 1.126030]\n",
            "1723 [D loss: 0.537512, acc.: 64.06%] [G loss: 1.065249]\n",
            "1724 [D loss: 0.508679, acc.: 70.31%] [G loss: 1.084005]\n",
            "1725 [D loss: 0.523183, acc.: 68.75%] [G loss: 1.004254]\n",
            "1726 [D loss: 0.552012, acc.: 68.75%] [G loss: 0.998924]\n",
            "1727 [D loss: 0.524583, acc.: 70.31%] [G loss: 1.184106]\n",
            "1728 [D loss: 0.551872, acc.: 68.75%] [G loss: 1.104713]\n",
            "1729 [D loss: 0.532302, acc.: 70.31%] [G loss: 1.189850]\n",
            "1730 [D loss: 0.557149, acc.: 70.31%] [G loss: 1.024131]\n",
            "1731 [D loss: 0.508358, acc.: 76.56%] [G loss: 1.170027]\n",
            "1732 [D loss: 0.533700, acc.: 73.44%] [G loss: 1.195788]\n",
            "1733 [D loss: 0.577420, acc.: 71.88%] [G loss: 1.148214]\n",
            "1734 [D loss: 0.521401, acc.: 73.44%] [G loss: 1.244866]\n",
            "1735 [D loss: 0.557165, acc.: 71.88%] [G loss: 1.060239]\n",
            "1736 [D loss: 0.541051, acc.: 68.75%] [G loss: 1.127074]\n",
            "1737 [D loss: 0.547862, acc.: 64.06%] [G loss: 1.316609]\n",
            "1738 [D loss: 0.524162, acc.: 71.88%] [G loss: 1.176466]\n",
            "1739 [D loss: 0.575869, acc.: 67.19%] [G loss: 1.147015]\n",
            "1740 [D loss: 0.496779, acc.: 71.88%] [G loss: 1.138921]\n",
            "1741 [D loss: 0.541632, acc.: 68.75%] [G loss: 0.996243]\n",
            "1742 [D loss: 0.508265, acc.: 67.19%] [G loss: 1.158960]\n",
            "1743 [D loss: 0.526459, acc.: 65.62%] [G loss: 1.141711]\n",
            "1744 [D loss: 0.545387, acc.: 65.62%] [G loss: 1.256240]\n",
            "1745 [D loss: 0.517361, acc.: 67.19%] [G loss: 1.129410]\n",
            "1746 [D loss: 0.547510, acc.: 65.62%] [G loss: 1.262596]\n",
            "1747 [D loss: 0.496887, acc.: 70.31%] [G loss: 1.389520]\n",
            "1748 [D loss: 0.526903, acc.: 70.31%] [G loss: 1.169280]\n",
            "1749 [D loss: 0.496677, acc.: 75.00%] [G loss: 1.102826]\n",
            "1750 [D loss: 0.490674, acc.: 75.00%] [G loss: 1.069395]\n",
            "1751 [D loss: 0.513238, acc.: 70.31%] [G loss: 1.091630]\n",
            "1752 [D loss: 0.536032, acc.: 70.31%] [G loss: 1.020582]\n",
            "1753 [D loss: 0.531731, acc.: 75.00%] [G loss: 1.144485]\n",
            "1754 [D loss: 0.546433, acc.: 67.19%] [G loss: 1.133448]\n",
            "1755 [D loss: 0.496829, acc.: 70.31%] [G loss: 1.196703]\n",
            "1756 [D loss: 0.520663, acc.: 71.88%] [G loss: 1.144277]\n",
            "1757 [D loss: 0.503205, acc.: 71.88%] [G loss: 0.982588]\n",
            "1758 [D loss: 0.489966, acc.: 73.44%] [G loss: 1.088143]\n",
            "1759 [D loss: 0.549223, acc.: 68.75%] [G loss: 1.269013]\n",
            "1760 [D loss: 0.512856, acc.: 67.19%] [G loss: 1.269060]\n",
            "1761 [D loss: 0.568003, acc.: 64.06%] [G loss: 1.186621]\n",
            "1762 [D loss: 0.510416, acc.: 71.88%] [G loss: 1.159986]\n",
            "1763 [D loss: 0.514209, acc.: 70.31%] [G loss: 1.183876]\n",
            "1764 [D loss: 0.495331, acc.: 75.00%] [G loss: 1.227553]\n",
            "1765 [D loss: 0.526615, acc.: 68.75%] [G loss: 1.173207]\n",
            "1766 [D loss: 0.505350, acc.: 64.06%] [G loss: 1.358821]\n",
            "1767 [D loss: 0.502387, acc.: 78.12%] [G loss: 1.384063]\n",
            "1768 [D loss: 0.541902, acc.: 68.75%] [G loss: 1.201814]\n",
            "1769 [D loss: 0.499949, acc.: 70.31%] [G loss: 1.151719]\n",
            "1770 [D loss: 0.471633, acc.: 70.31%] [G loss: 1.338037]\n",
            "1771 [D loss: 0.548492, acc.: 68.75%] [G loss: 1.173429]\n",
            "1772 [D loss: 0.534455, acc.: 68.75%] [G loss: 1.223441]\n",
            "1773 [D loss: 0.485394, acc.: 76.56%] [G loss: 1.147775]\n",
            "1774 [D loss: 0.557117, acc.: 68.75%] [G loss: 1.267922]\n",
            "1775 [D loss: 0.473889, acc.: 73.44%] [G loss: 1.386623]\n",
            "1776 [D loss: 0.584325, acc.: 67.19%] [G loss: 1.276845]\n",
            "1777 [D loss: 0.501056, acc.: 76.56%] [G loss: 1.150999]\n",
            "1778 [D loss: 0.538040, acc.: 73.44%] [G loss: 1.193290]\n",
            "1779 [D loss: 0.510290, acc.: 65.62%] [G loss: 1.476443]\n",
            "1780 [D loss: 0.566001, acc.: 71.88%] [G loss: 1.190268]\n",
            "1781 [D loss: 0.520720, acc.: 75.00%] [G loss: 1.201759]\n",
            "1782 [D loss: 0.504255, acc.: 76.56%] [G loss: 1.124636]\n",
            "1783 [D loss: 0.514196, acc.: 68.75%] [G loss: 1.224634]\n",
            "1784 [D loss: 0.534756, acc.: 70.31%] [G loss: 1.116020]\n",
            "1785 [D loss: 0.516255, acc.: 78.12%] [G loss: 1.238146]\n",
            "1786 [D loss: 0.494129, acc.: 68.75%] [G loss: 1.205703]\n",
            "1787 [D loss: 0.511217, acc.: 71.88%] [G loss: 1.134337]\n",
            "1788 [D loss: 0.495798, acc.: 75.00%] [G loss: 1.460190]\n",
            "1789 [D loss: 0.515019, acc.: 71.88%] [G loss: 1.172672]\n",
            "1790 [D loss: 0.516200, acc.: 68.75%] [G loss: 1.086717]\n",
            "1791 [D loss: 0.521280, acc.: 76.56%] [G loss: 1.034851]\n",
            "1792 [D loss: 0.512032, acc.: 76.56%] [G loss: 1.094746]\n",
            "1793 [D loss: 0.496862, acc.: 76.56%] [G loss: 1.105730]\n",
            "1794 [D loss: 0.566259, acc.: 62.50%] [G loss: 1.179323]\n",
            "1795 [D loss: 0.509989, acc.: 70.31%] [G loss: 0.971790]\n",
            "1796 [D loss: 0.506710, acc.: 73.44%] [G loss: 1.042878]\n",
            "1797 [D loss: 0.503954, acc.: 73.44%] [G loss: 1.087855]\n",
            "1798 [D loss: 0.508743, acc.: 73.44%] [G loss: 1.127681]\n",
            "1799 [D loss: 0.490691, acc.: 73.44%] [G loss: 1.148180]\n",
            "1800 [D loss: 0.514700, acc.: 71.88%] [G loss: 1.182611]\n",
            "generated_data\n",
            "1801 [D loss: 0.516716, acc.: 71.88%] [G loss: 1.136024]\n",
            "1802 [D loss: 0.518560, acc.: 71.88%] [G loss: 1.234145]\n",
            "1803 [D loss: 0.472187, acc.: 68.75%] [G loss: 1.325294]\n",
            "1804 [D loss: 0.553897, acc.: 67.19%] [G loss: 1.129879]\n",
            "1805 [D loss: 0.498802, acc.: 75.00%] [G loss: 1.117519]\n",
            "1806 [D loss: 0.479019, acc.: 68.75%] [G loss: 1.064035]\n",
            "1807 [D loss: 0.537791, acc.: 64.06%] [G loss: 1.230046]\n",
            "1808 [D loss: 0.489111, acc.: 70.31%] [G loss: 1.317416]\n",
            "1809 [D loss: 0.529784, acc.: 65.62%] [G loss: 1.273964]\n",
            "1810 [D loss: 0.500741, acc.: 70.31%] [G loss: 1.240798]\n",
            "1811 [D loss: 0.443431, acc.: 73.44%] [G loss: 1.586920]\n",
            "1812 [D loss: 0.492499, acc.: 73.44%] [G loss: 1.112198]\n",
            "1813 [D loss: 0.494898, acc.: 75.00%] [G loss: 1.145921]\n",
            "1814 [D loss: 0.514537, acc.: 68.75%] [G loss: 1.283566]\n",
            "1815 [D loss: 0.537836, acc.: 71.88%] [G loss: 1.090697]\n",
            "1816 [D loss: 0.462812, acc.: 73.44%] [G loss: 1.255263]\n",
            "1817 [D loss: 0.553519, acc.: 65.62%] [G loss: 1.140054]\n",
            "1818 [D loss: 0.493047, acc.: 68.75%] [G loss: 1.273957]\n",
            "1819 [D loss: 0.510185, acc.: 68.75%] [G loss: 1.254978]\n",
            "1820 [D loss: 0.602122, acc.: 62.50%] [G loss: 0.922157]\n",
            "1821 [D loss: 0.483665, acc.: 75.00%] [G loss: 1.298654]\n",
            "1822 [D loss: 0.503017, acc.: 71.88%] [G loss: 0.994524]\n",
            "1823 [D loss: 0.518302, acc.: 73.44%] [G loss: 1.114193]\n",
            "1824 [D loss: 0.483716, acc.: 70.31%] [G loss: 1.295989]\n",
            "1825 [D loss: 0.542279, acc.: 64.06%] [G loss: 1.017153]\n",
            "1826 [D loss: 0.508709, acc.: 71.88%] [G loss: 1.190442]\n",
            "1827 [D loss: 0.534539, acc.: 70.31%] [G loss: 1.179580]\n",
            "1828 [D loss: 0.553726, acc.: 64.06%] [G loss: 1.248104]\n",
            "1829 [D loss: 0.481100, acc.: 71.88%] [G loss: 1.493451]\n",
            "1830 [D loss: 0.509497, acc.: 64.06%] [G loss: 1.221364]\n",
            "1831 [D loss: 0.487891, acc.: 75.00%] [G loss: 1.078370]\n",
            "1832 [D loss: 0.479314, acc.: 73.44%] [G loss: 1.378814]\n",
            "1833 [D loss: 0.548717, acc.: 64.06%] [G loss: 1.067529]\n",
            "1834 [D loss: 0.479150, acc.: 76.56%] [G loss: 1.302186]\n",
            "1835 [D loss: 0.531928, acc.: 65.62%] [G loss: 1.226008]\n",
            "1836 [D loss: 0.526318, acc.: 64.06%] [G loss: 1.145812]\n",
            "1837 [D loss: 0.533399, acc.: 70.31%] [G loss: 1.157971]\n",
            "1838 [D loss: 0.510975, acc.: 70.31%] [G loss: 1.116294]\n",
            "1839 [D loss: 0.510507, acc.: 75.00%] [G loss: 1.189626]\n",
            "1840 [D loss: 0.516026, acc.: 67.19%] [G loss: 1.224771]\n",
            "1841 [D loss: 0.508072, acc.: 68.75%] [G loss: 1.224098]\n",
            "1842 [D loss: 0.517465, acc.: 67.19%] [G loss: 1.248788]\n",
            "1843 [D loss: 0.476068, acc.: 73.44%] [G loss: 1.145625]\n",
            "1844 [D loss: 0.463877, acc.: 73.44%] [G loss: 1.334303]\n",
            "1845 [D loss: 0.519910, acc.: 68.75%] [G loss: 1.176975]\n",
            "1846 [D loss: 0.511885, acc.: 65.62%] [G loss: 1.168517]\n",
            "1847 [D loss: 0.491307, acc.: 75.00%] [G loss: 1.004338]\n",
            "1848 [D loss: 0.565431, acc.: 76.56%] [G loss: 1.131516]\n",
            "1849 [D loss: 0.514055, acc.: 70.31%] [G loss: 1.262879]\n",
            "1850 [D loss: 0.491949, acc.: 75.00%] [G loss: 1.323874]\n",
            "1851 [D loss: 0.486395, acc.: 76.56%] [G loss: 1.116592]\n",
            "1852 [D loss: 0.571465, acc.: 70.31%] [G loss: 1.226687]\n",
            "1853 [D loss: 0.495320, acc.: 70.31%] [G loss: 1.253277]\n",
            "1854 [D loss: 0.537630, acc.: 64.06%] [G loss: 1.197819]\n",
            "1855 [D loss: 0.504154, acc.: 71.88%] [G loss: 1.284042]\n",
            "1856 [D loss: 0.496014, acc.: 71.88%] [G loss: 1.121549]\n",
            "1857 [D loss: 0.511957, acc.: 68.75%] [G loss: 1.082275]\n",
            "1858 [D loss: 0.489403, acc.: 73.44%] [G loss: 1.325020]\n",
            "1859 [D loss: 0.493410, acc.: 75.00%] [G loss: 1.212204]\n",
            "1860 [D loss: 0.541860, acc.: 60.94%] [G loss: 1.353139]\n",
            "1861 [D loss: 0.432169, acc.: 76.56%] [G loss: 1.330815]\n",
            "1862 [D loss: 0.545406, acc.: 71.88%] [G loss: 1.030393]\n",
            "1863 [D loss: 0.488321, acc.: 76.56%] [G loss: 1.222784]\n",
            "1864 [D loss: 0.486384, acc.: 70.31%] [G loss: 1.016998]\n",
            "1865 [D loss: 0.475483, acc.: 75.00%] [G loss: 0.994113]\n",
            "1866 [D loss: 0.463151, acc.: 75.00%] [G loss: 1.028053]\n",
            "1867 [D loss: 0.518803, acc.: 68.75%] [G loss: 1.071935]\n",
            "1868 [D loss: 0.497588, acc.: 68.75%] [G loss: 1.085859]\n",
            "1869 [D loss: 0.481019, acc.: 78.12%] [G loss: 1.134461]\n",
            "1870 [D loss: 0.514300, acc.: 67.19%] [G loss: 1.140543]\n",
            "1871 [D loss: 0.516421, acc.: 64.06%] [G loss: 1.201451]\n",
            "1872 [D loss: 0.533110, acc.: 68.75%] [G loss: 0.994278]\n",
            "1873 [D loss: 0.524175, acc.: 71.88%] [G loss: 1.198312]\n",
            "1874 [D loss: 0.525462, acc.: 71.88%] [G loss: 1.200744]\n",
            "1875 [D loss: 0.588619, acc.: 67.19%] [G loss: 1.149013]\n",
            "1876 [D loss: 0.516492, acc.: 70.31%] [G loss: 1.204703]\n",
            "1877 [D loss: 0.544316, acc.: 70.31%] [G loss: 1.120707]\n",
            "1878 [D loss: 0.493181, acc.: 68.75%] [G loss: 1.148296]\n",
            "1879 [D loss: 0.473339, acc.: 75.00%] [G loss: 1.351778]\n",
            "1880 [D loss: 0.512953, acc.: 71.88%] [G loss: 1.226571]\n",
            "1881 [D loss: 0.547358, acc.: 68.75%] [G loss: 1.146870]\n",
            "1882 [D loss: 0.490798, acc.: 73.44%] [G loss: 1.117616]\n",
            "1883 [D loss: 0.454234, acc.: 79.69%] [G loss: 1.286577]\n",
            "1884 [D loss: 0.520598, acc.: 70.31%] [G loss: 1.321354]\n",
            "1885 [D loss: 0.501010, acc.: 73.44%] [G loss: 1.464364]\n",
            "1886 [D loss: 0.569529, acc.: 70.31%] [G loss: 1.255235]\n",
            "1887 [D loss: 0.525740, acc.: 70.31%] [G loss: 1.270739]\n",
            "1888 [D loss: 0.532467, acc.: 73.44%] [G loss: 1.313083]\n",
            "1889 [D loss: 0.493787, acc.: 68.75%] [G loss: 1.205385]\n",
            "1890 [D loss: 0.494294, acc.: 65.62%] [G loss: 1.262465]\n",
            "1891 [D loss: 0.530396, acc.: 67.19%] [G loss: 1.241730]\n",
            "1892 [D loss: 0.532716, acc.: 64.06%] [G loss: 1.271151]\n",
            "1893 [D loss: 0.486010, acc.: 73.44%] [G loss: 0.980834]\n",
            "1894 [D loss: 0.484851, acc.: 73.44%] [G loss: 1.183870]\n",
            "1895 [D loss: 0.547881, acc.: 71.88%] [G loss: 1.156276]\n",
            "1896 [D loss: 0.481582, acc.: 73.44%] [G loss: 1.250792]\n",
            "1897 [D loss: 0.568877, acc.: 60.94%] [G loss: 1.371406]\n",
            "1898 [D loss: 0.468885, acc.: 75.00%] [G loss: 1.444203]\n",
            "1899 [D loss: 0.484332, acc.: 73.44%] [G loss: 1.466418]\n",
            "1900 [D loss: 0.508566, acc.: 68.75%] [G loss: 1.240475]\n",
            "generated_data\n",
            "1901 [D loss: 0.490415, acc.: 71.88%] [G loss: 1.280440]\n",
            "1902 [D loss: 0.579897, acc.: 59.38%] [G loss: 1.066814]\n",
            "1903 [D loss: 0.459749, acc.: 73.44%] [G loss: 1.215792]\n",
            "1904 [D loss: 0.525099, acc.: 70.31%] [G loss: 1.255173]\n",
            "1905 [D loss: 0.502930, acc.: 70.31%] [G loss: 1.051561]\n",
            "1906 [D loss: 0.552002, acc.: 60.94%] [G loss: 1.260846]\n",
            "1907 [D loss: 0.590837, acc.: 62.50%] [G loss: 1.158948]\n",
            "1908 [D loss: 0.528870, acc.: 65.62%] [G loss: 1.256365]\n",
            "1909 [D loss: 0.470642, acc.: 71.88%] [G loss: 1.428553]\n",
            "1910 [D loss: 0.495366, acc.: 70.31%] [G loss: 1.141312]\n",
            "1911 [D loss: 0.485334, acc.: 68.75%] [G loss: 1.306208]\n",
            "1912 [D loss: 0.488756, acc.: 78.12%] [G loss: 1.091433]\n",
            "1913 [D loss: 0.482540, acc.: 71.88%] [G loss: 1.255447]\n",
            "1914 [D loss: 0.500539, acc.: 70.31%] [G loss: 1.148869]\n",
            "1915 [D loss: 0.485323, acc.: 75.00%] [G loss: 1.238019]\n",
            "1916 [D loss: 0.518307, acc.: 71.88%] [G loss: 1.177320]\n",
            "1917 [D loss: 0.488903, acc.: 67.19%] [G loss: 1.189006]\n",
            "1918 [D loss: 0.530245, acc.: 70.31%] [G loss: 1.247409]\n",
            "1919 [D loss: 0.557901, acc.: 71.88%] [G loss: 1.351575]\n",
            "1920 [D loss: 0.618538, acc.: 60.94%] [G loss: 1.238461]\n",
            "1921 [D loss: 0.567253, acc.: 67.19%] [G loss: 1.414188]\n",
            "1922 [D loss: 0.562835, acc.: 71.88%] [G loss: 1.377037]\n",
            "1923 [D loss: 0.561164, acc.: 73.44%] [G loss: 1.099144]\n",
            "1924 [D loss: 0.563856, acc.: 68.75%] [G loss: 1.417378]\n",
            "1925 [D loss: 0.504187, acc.: 76.56%] [G loss: 1.176595]\n",
            "1926 [D loss: 0.531076, acc.: 71.88%] [G loss: 1.175014]\n",
            "1927 [D loss: 0.471845, acc.: 78.12%] [G loss: 1.371139]\n",
            "1928 [D loss: 0.482177, acc.: 76.56%] [G loss: 1.457572]\n",
            "1929 [D loss: 0.491987, acc.: 73.44%] [G loss: 1.292876]\n",
            "1930 [D loss: 0.463653, acc.: 70.31%] [G loss: 1.316523]\n",
            "1931 [D loss: 0.489891, acc.: 75.00%] [G loss: 1.178153]\n",
            "1932 [D loss: 0.418647, acc.: 81.25%] [G loss: 1.345204]\n",
            "1933 [D loss: 0.562854, acc.: 64.06%] [G loss: 1.253682]\n",
            "1934 [D loss: 0.470114, acc.: 78.12%] [G loss: 1.221737]\n",
            "1935 [D loss: 0.468098, acc.: 78.12%] [G loss: 1.084774]\n",
            "1936 [D loss: 0.501152, acc.: 75.00%] [G loss: 1.321981]\n",
            "1937 [D loss: 0.489253, acc.: 73.44%] [G loss: 1.406576]\n",
            "1938 [D loss: 0.488503, acc.: 70.31%] [G loss: 1.381600]\n",
            "1939 [D loss: 0.505436, acc.: 71.88%] [G loss: 1.150381]\n",
            "1940 [D loss: 0.508294, acc.: 75.00%] [G loss: 1.175277]\n",
            "1941 [D loss: 0.500487, acc.: 70.31%] [G loss: 1.287069]\n",
            "1942 [D loss: 0.568098, acc.: 65.62%] [G loss: 1.086555]\n",
            "1943 [D loss: 0.493751, acc.: 73.44%] [G loss: 1.237715]\n",
            "1944 [D loss: 0.514976, acc.: 67.19%] [G loss: 1.142509]\n",
            "1945 [D loss: 0.464559, acc.: 75.00%] [G loss: 1.134953]\n",
            "1946 [D loss: 0.531590, acc.: 67.19%] [G loss: 1.183028]\n",
            "1947 [D loss: 0.494923, acc.: 76.56%] [G loss: 1.276841]\n",
            "1948 [D loss: 0.519857, acc.: 65.62%] [G loss: 1.245090]\n",
            "1949 [D loss: 0.513842, acc.: 71.88%] [G loss: 1.242394]\n",
            "1950 [D loss: 0.508432, acc.: 67.19%] [G loss: 1.272990]\n",
            "1951 [D loss: 0.528903, acc.: 68.75%] [G loss: 1.110514]\n",
            "1952 [D loss: 0.499067, acc.: 70.31%] [G loss: 1.322580]\n",
            "1953 [D loss: 0.545945, acc.: 71.88%] [G loss: 1.000362]\n",
            "1954 [D loss: 0.499334, acc.: 71.88%] [G loss: 1.140081]\n",
            "1955 [D loss: 0.508605, acc.: 68.75%] [G loss: 1.259767]\n",
            "1956 [D loss: 0.456175, acc.: 76.56%] [G loss: 1.210697]\n",
            "1957 [D loss: 0.508258, acc.: 68.75%] [G loss: 1.159380]\n",
            "1958 [D loss: 0.452184, acc.: 78.12%] [G loss: 1.216714]\n",
            "1959 [D loss: 0.534108, acc.: 68.75%] [G loss: 1.173828]\n",
            "1960 [D loss: 0.523059, acc.: 65.62%] [G loss: 1.171502]\n",
            "1961 [D loss: 0.498889, acc.: 71.88%] [G loss: 1.235426]\n",
            "1962 [D loss: 0.586338, acc.: 64.06%] [G loss: 1.039162]\n",
            "1963 [D loss: 0.479030, acc.: 71.88%] [G loss: 1.214734]\n",
            "1964 [D loss: 0.571153, acc.: 62.50%] [G loss: 1.008512]\n",
            "1965 [D loss: 0.468635, acc.: 73.44%] [G loss: 1.064238]\n",
            "1966 [D loss: 0.492478, acc.: 70.31%] [G loss: 1.273192]\n",
            "1967 [D loss: 0.471924, acc.: 73.44%] [G loss: 1.188344]\n",
            "1968 [D loss: 0.523619, acc.: 70.31%] [G loss: 1.315604]\n",
            "1969 [D loss: 0.467966, acc.: 70.31%] [G loss: 1.177117]\n",
            "1970 [D loss: 0.473580, acc.: 75.00%] [G loss: 1.182905]\n",
            "1971 [D loss: 0.509524, acc.: 71.88%] [G loss: 1.477599]\n",
            "1972 [D loss: 0.527443, acc.: 68.75%] [G loss: 1.143882]\n",
            "1973 [D loss: 0.503670, acc.: 65.62%] [G loss: 1.165421]\n",
            "1974 [D loss: 0.520125, acc.: 70.31%] [G loss: 0.948883]\n",
            "1975 [D loss: 0.451797, acc.: 73.44%] [G loss: 1.017270]\n",
            "1976 [D loss: 0.471520, acc.: 76.56%] [G loss: 1.173752]\n",
            "1977 [D loss: 0.502525, acc.: 71.88%] [G loss: 1.278460]\n",
            "1978 [D loss: 0.487347, acc.: 73.44%] [G loss: 1.087900]\n",
            "1979 [D loss: 0.498194, acc.: 70.31%] [G loss: 1.203332]\n",
            "1980 [D loss: 0.481154, acc.: 75.00%] [G loss: 1.033977]\n",
            "1981 [D loss: 0.464401, acc.: 75.00%] [G loss: 1.171063]\n",
            "1982 [D loss: 0.511698, acc.: 73.44%] [G loss: 1.094331]\n",
            "1983 [D loss: 0.456807, acc.: 81.25%] [G loss: 1.359683]\n",
            "1984 [D loss: 0.567644, acc.: 62.50%] [G loss: 1.285008]\n",
            "1985 [D loss: 0.514174, acc.: 68.75%] [G loss: 1.343670]\n",
            "1986 [D loss: 0.469508, acc.: 73.44%] [G loss: 1.508800]\n",
            "1987 [D loss: 0.519498, acc.: 70.31%] [G loss: 1.368907]\n",
            "1988 [D loss: 0.561230, acc.: 68.75%] [G loss: 1.407857]\n",
            "1989 [D loss: 0.480530, acc.: 71.88%] [G loss: 1.663671]\n",
            "1990 [D loss: 0.517975, acc.: 71.88%] [G loss: 1.429945]\n",
            "1991 [D loss: 0.469311, acc.: 71.88%] [G loss: 1.217361]\n",
            "1992 [D loss: 0.481112, acc.: 75.00%] [G loss: 1.245415]\n",
            "1993 [D loss: 0.545577, acc.: 71.88%] [G loss: 1.197950]\n",
            "1994 [D loss: 0.482419, acc.: 70.31%] [G loss: 1.511730]\n",
            "1995 [D loss: 0.506663, acc.: 71.88%] [G loss: 1.328737]\n",
            "1996 [D loss: 0.542074, acc.: 73.44%] [G loss: 1.227668]\n",
            "1997 [D loss: 0.461945, acc.: 70.31%] [G loss: 1.250390]\n",
            "1998 [D loss: 0.496721, acc.: 70.31%] [G loss: 1.165397]\n",
            "1999 [D loss: 0.496149, acc.: 71.88%] [G loss: 1.289623]\n",
            "2000 [D loss: 0.488762, acc.: 68.75%] [G loss: 1.220458]\n",
            "generated_data\n",
            "2001 [D loss: 0.520301, acc.: 68.75%] [G loss: 1.156762]\n",
            "2002 [D loss: 0.466033, acc.: 76.56%] [G loss: 1.192268]\n",
            "2003 [D loss: 0.520312, acc.: 67.19%] [G loss: 1.104519]\n",
            "2004 [D loss: 0.486111, acc.: 73.44%] [G loss: 1.243452]\n",
            "2005 [D loss: 0.523520, acc.: 68.75%] [G loss: 1.172248]\n",
            "2006 [D loss: 0.448109, acc.: 76.56%] [G loss: 1.035030]\n",
            "2007 [D loss: 0.506722, acc.: 70.31%] [G loss: 0.992384]\n",
            "2008 [D loss: 0.530989, acc.: 67.19%] [G loss: 1.131014]\n",
            "2009 [D loss: 0.483743, acc.: 75.00%] [G loss: 1.354439]\n",
            "2010 [D loss: 0.521789, acc.: 68.75%] [G loss: 1.180952]\n",
            "2011 [D loss: 0.498448, acc.: 67.19%] [G loss: 1.330045]\n",
            "2012 [D loss: 0.493006, acc.: 67.19%] [G loss: 1.256330]\n",
            "2013 [D loss: 0.536047, acc.: 71.88%] [G loss: 1.126223]\n",
            "2014 [D loss: 0.446168, acc.: 79.69%] [G loss: 1.391113]\n",
            "2015 [D loss: 0.498177, acc.: 75.00%] [G loss: 1.669568]\n",
            "2016 [D loss: 0.558412, acc.: 60.94%] [G loss: 1.175121]\n",
            "2017 [D loss: 0.510938, acc.: 70.31%] [G loss: 1.263488]\n",
            "2018 [D loss: 0.465038, acc.: 71.88%] [G loss: 1.473221]\n",
            "2019 [D loss: 0.531417, acc.: 71.88%] [G loss: 1.101623]\n",
            "2020 [D loss: 0.471619, acc.: 68.75%] [G loss: 1.367727]\n",
            "2021 [D loss: 0.508217, acc.: 70.31%] [G loss: 1.373181]\n",
            "2022 [D loss: 0.534389, acc.: 67.19%] [G loss: 1.150811]\n",
            "2023 [D loss: 0.483712, acc.: 73.44%] [G loss: 1.186658]\n",
            "2024 [D loss: 0.482712, acc.: 68.75%] [G loss: 1.249499]\n",
            "2025 [D loss: 0.466928, acc.: 71.88%] [G loss: 1.304725]\n",
            "2026 [D loss: 0.480165, acc.: 75.00%] [G loss: 1.330850]\n",
            "2027 [D loss: 0.506663, acc.: 71.88%] [G loss: 1.332631]\n",
            "2028 [D loss: 0.562259, acc.: 65.62%] [G loss: 1.164038]\n",
            "2029 [D loss: 0.473743, acc.: 71.88%] [G loss: 1.463087]\n",
            "2030 [D loss: 0.494194, acc.: 73.44%] [G loss: 1.323315]\n",
            "2031 [D loss: 0.473892, acc.: 73.44%] [G loss: 1.400678]\n",
            "2032 [D loss: 0.514073, acc.: 71.88%] [G loss: 1.480177]\n",
            "2033 [D loss: 0.526838, acc.: 67.19%] [G loss: 1.321199]\n",
            "2034 [D loss: 0.461558, acc.: 78.12%] [G loss: 1.495928]\n",
            "2035 [D loss: 0.534186, acc.: 71.88%] [G loss: 1.102963]\n",
            "2036 [D loss: 0.423268, acc.: 75.00%] [G loss: 1.302754]\n",
            "2037 [D loss: 0.471643, acc.: 68.75%] [G loss: 1.559327]\n",
            "2038 [D loss: 0.562017, acc.: 56.25%] [G loss: 1.220117]\n",
            "2039 [D loss: 0.394134, acc.: 73.44%] [G loss: 1.714633]\n",
            "2040 [D loss: 0.489810, acc.: 71.88%] [G loss: 1.442688]\n",
            "2041 [D loss: 0.517972, acc.: 70.31%] [G loss: 1.512511]\n",
            "2042 [D loss: 0.441777, acc.: 75.00%] [G loss: 1.546591]\n",
            "2043 [D loss: 0.486367, acc.: 73.44%] [G loss: 1.461702]\n",
            "2044 [D loss: 0.585003, acc.: 60.94%] [G loss: 1.115459]\n",
            "2045 [D loss: 0.496660, acc.: 68.75%] [G loss: 1.402781]\n",
            "2046 [D loss: 0.473294, acc.: 75.00%] [G loss: 1.428613]\n",
            "2047 [D loss: 0.497249, acc.: 70.31%] [G loss: 1.300563]\n",
            "2048 [D loss: 0.557909, acc.: 70.31%] [G loss: 1.408667]\n",
            "2049 [D loss: 0.593110, acc.: 67.19%] [G loss: 1.140849]\n",
            "2050 [D loss: 0.529893, acc.: 70.31%] [G loss: 1.324186]\n",
            "2051 [D loss: 0.442396, acc.: 75.00%] [G loss: 1.344953]\n",
            "2052 [D loss: 0.515300, acc.: 73.44%] [G loss: 1.144109]\n",
            "2053 [D loss: 0.488330, acc.: 71.88%] [G loss: 1.259399]\n",
            "2054 [D loss: 0.458843, acc.: 73.44%] [G loss: 1.202348]\n",
            "2055 [D loss: 0.548728, acc.: 67.19%] [G loss: 0.967407]\n",
            "2056 [D loss: 0.451519, acc.: 75.00%] [G loss: 1.273002]\n",
            "2057 [D loss: 0.520263, acc.: 81.25%] [G loss: 1.486452]\n",
            "2058 [D loss: 0.578341, acc.: 60.94%] [G loss: 1.312696]\n",
            "2059 [D loss: 0.509463, acc.: 71.88%] [G loss: 1.307402]\n",
            "2060 [D loss: 0.476840, acc.: 75.00%] [G loss: 1.268131]\n",
            "2061 [D loss: 0.471059, acc.: 78.12%] [G loss: 1.063516]\n",
            "2062 [D loss: 0.558475, acc.: 64.06%] [G loss: 1.271778]\n",
            "2063 [D loss: 0.461517, acc.: 73.44%] [G loss: 1.264917]\n",
            "2064 [D loss: 0.484045, acc.: 68.75%] [G loss: 1.324184]\n",
            "2065 [D loss: 0.496295, acc.: 67.19%] [G loss: 1.337005]\n",
            "2066 [D loss: 0.516734, acc.: 78.12%] [G loss: 1.306695]\n",
            "2067 [D loss: 0.551354, acc.: 73.44%] [G loss: 1.413667]\n",
            "2068 [D loss: 0.481281, acc.: 75.00%] [G loss: 1.291098]\n",
            "2069 [D loss: 0.556055, acc.: 71.88%] [G loss: 1.402310]\n",
            "2070 [D loss: 0.517193, acc.: 73.44%] [G loss: 1.529562]\n",
            "2071 [D loss: 0.560410, acc.: 70.31%] [G loss: 1.109420]\n",
            "2072 [D loss: 0.461646, acc.: 75.00%] [G loss: 1.445485]\n",
            "2073 [D loss: 0.455013, acc.: 79.69%] [G loss: 1.302547]\n",
            "2074 [D loss: 0.482503, acc.: 71.88%] [G loss: 1.290963]\n",
            "2075 [D loss: 0.538670, acc.: 67.19%] [G loss: 1.202601]\n",
            "2076 [D loss: 0.462651, acc.: 84.38%] [G loss: 1.306619]\n",
            "2077 [D loss: 0.452698, acc.: 75.00%] [G loss: 1.202110]\n",
            "2078 [D loss: 0.468327, acc.: 79.69%] [G loss: 1.421376]\n",
            "2079 [D loss: 0.493567, acc.: 67.19%] [G loss: 1.371664]\n",
            "2080 [D loss: 0.540021, acc.: 64.06%] [G loss: 1.263217]\n",
            "2081 [D loss: 0.575291, acc.: 62.50%] [G loss: 1.352953]\n",
            "2082 [D loss: 0.513592, acc.: 68.75%] [G loss: 1.150864]\n",
            "2083 [D loss: 0.491181, acc.: 73.44%] [G loss: 1.194935]\n",
            "2084 [D loss: 0.487800, acc.: 71.88%] [G loss: 1.250028]\n",
            "2085 [D loss: 0.474900, acc.: 71.88%] [G loss: 1.622548]\n",
            "2086 [D loss: 0.543441, acc.: 65.62%] [G loss: 1.262384]\n",
            "2087 [D loss: 0.478927, acc.: 73.44%] [G loss: 1.389544]\n",
            "2088 [D loss: 0.476915, acc.: 75.00%] [G loss: 1.314187]\n",
            "2089 [D loss: 0.600178, acc.: 67.19%] [G loss: 1.434561]\n",
            "2090 [D loss: 0.557416, acc.: 65.62%] [G loss: 1.317217]\n",
            "2091 [D loss: 0.531484, acc.: 65.62%] [G loss: 1.263714]\n",
            "2092 [D loss: 0.473050, acc.: 71.88%] [G loss: 1.527577]\n",
            "2093 [D loss: 0.424748, acc.: 75.00%] [G loss: 1.443317]\n",
            "2094 [D loss: 0.520356, acc.: 68.75%] [G loss: 1.118638]\n",
            "2095 [D loss: 0.430260, acc.: 78.12%] [G loss: 1.373710]\n",
            "2096 [D loss: 0.526290, acc.: 70.31%] [G loss: 1.206407]\n",
            "2097 [D loss: 0.462152, acc.: 73.44%] [G loss: 1.205155]\n",
            "2098 [D loss: 0.509064, acc.: 75.00%] [G loss: 1.499040]\n",
            "2099 [D loss: 0.487098, acc.: 75.00%] [G loss: 1.372046]\n",
            "2100 [D loss: 0.441539, acc.: 76.56%] [G loss: 1.375326]\n",
            "generated_data\n",
            "2101 [D loss: 0.444855, acc.: 78.12%] [G loss: 1.478990]\n",
            "2102 [D loss: 0.524673, acc.: 67.19%] [G loss: 1.164049]\n",
            "2103 [D loss: 0.465331, acc.: 73.44%] [G loss: 1.464587]\n",
            "2104 [D loss: 0.477618, acc.: 78.12%] [G loss: 1.449598]\n",
            "2105 [D loss: 0.520437, acc.: 68.75%] [G loss: 1.288344]\n",
            "2106 [D loss: 0.438470, acc.: 79.69%] [G loss: 1.616189]\n",
            "2107 [D loss: 0.507573, acc.: 70.31%] [G loss: 1.319218]\n",
            "2108 [D loss: 0.507535, acc.: 68.75%] [G loss: 1.140432]\n",
            "2109 [D loss: 0.514352, acc.: 70.31%] [G loss: 1.416100]\n",
            "2110 [D loss: 0.473124, acc.: 75.00%] [G loss: 1.324826]\n",
            "2111 [D loss: 0.437979, acc.: 78.12%] [G loss: 1.294943]\n",
            "2112 [D loss: 0.473062, acc.: 75.00%] [G loss: 1.245934]\n",
            "2113 [D loss: 0.490325, acc.: 73.44%] [G loss: 1.242593]\n",
            "2114 [D loss: 0.426589, acc.: 78.12%] [G loss: 1.221345]\n",
            "2115 [D loss: 0.499953, acc.: 71.88%] [G loss: 1.187382]\n",
            "2116 [D loss: 0.493624, acc.: 73.44%] [G loss: 1.347862]\n",
            "2117 [D loss: 0.542271, acc.: 65.62%] [G loss: 1.127350]\n",
            "2118 [D loss: 0.455514, acc.: 79.69%] [G loss: 1.300947]\n",
            "2119 [D loss: 0.480249, acc.: 73.44%] [G loss: 1.366716]\n",
            "2120 [D loss: 0.475402, acc.: 79.69%] [G loss: 1.150592]\n",
            "2121 [D loss: 0.487519, acc.: 70.31%] [G loss: 1.129556]\n",
            "2122 [D loss: 0.481823, acc.: 78.12%] [G loss: 1.295167]\n",
            "2123 [D loss: 0.523377, acc.: 76.56%] [G loss: 1.288481]\n",
            "2124 [D loss: 0.478418, acc.: 76.56%] [G loss: 1.339384]\n",
            "2125 [D loss: 0.528959, acc.: 70.31%] [G loss: 1.311951]\n",
            "2126 [D loss: 0.501690, acc.: 78.12%] [G loss: 1.321151]\n",
            "2127 [D loss: 0.460801, acc.: 75.00%] [G loss: 1.317433]\n",
            "2128 [D loss: 0.468572, acc.: 73.44%] [G loss: 1.254683]\n",
            "2129 [D loss: 0.483945, acc.: 71.88%] [G loss: 1.252733]\n",
            "2130 [D loss: 0.457577, acc.: 75.00%] [G loss: 1.403488]\n",
            "2131 [D loss: 0.465195, acc.: 78.12%] [G loss: 1.440921]\n",
            "2132 [D loss: 0.432342, acc.: 78.12%] [G loss: 1.589723]\n",
            "2133 [D loss: 0.494228, acc.: 73.44%] [G loss: 1.421450]\n",
            "2134 [D loss: 0.489993, acc.: 76.56%] [G loss: 1.171653]\n",
            "2135 [D loss: 0.463577, acc.: 73.44%] [G loss: 1.246623]\n",
            "2136 [D loss: 0.461001, acc.: 81.25%] [G loss: 1.317489]\n",
            "2137 [D loss: 0.472523, acc.: 76.56%] [G loss: 1.318908]\n",
            "2138 [D loss: 0.505143, acc.: 73.44%] [G loss: 1.429656]\n",
            "2139 [D loss: 0.491244, acc.: 75.00%] [G loss: 1.164062]\n",
            "2140 [D loss: 0.491073, acc.: 73.44%] [G loss: 1.385454]\n",
            "2141 [D loss: 0.500507, acc.: 71.88%] [G loss: 1.332138]\n",
            "2142 [D loss: 0.497858, acc.: 75.00%] [G loss: 1.328995]\n",
            "2143 [D loss: 0.411624, acc.: 81.25%] [G loss: 1.426813]\n",
            "2144 [D loss: 0.463189, acc.: 75.00%] [G loss: 1.438056]\n",
            "2145 [D loss: 0.511057, acc.: 71.88%] [G loss: 1.433638]\n",
            "2146 [D loss: 0.469014, acc.: 76.56%] [G loss: 1.436753]\n",
            "2147 [D loss: 0.522530, acc.: 71.88%] [G loss: 1.212143]\n",
            "2148 [D loss: 0.493549, acc.: 71.88%] [G loss: 1.435557]\n",
            "2149 [D loss: 0.462412, acc.: 70.31%] [G loss: 1.330985]\n",
            "2150 [D loss: 0.621091, acc.: 64.06%] [G loss: 1.318232]\n",
            "2151 [D loss: 0.514888, acc.: 73.44%] [G loss: 1.485148]\n",
            "2152 [D loss: 0.528680, acc.: 65.62%] [G loss: 1.315786]\n",
            "2153 [D loss: 0.501876, acc.: 68.75%] [G loss: 1.088747]\n",
            "2154 [D loss: 0.511468, acc.: 70.31%] [G loss: 1.285900]\n",
            "2155 [D loss: 0.465381, acc.: 75.00%] [G loss: 1.476599]\n",
            "2156 [D loss: 0.490067, acc.: 73.44%] [G loss: 1.294487]\n",
            "2157 [D loss: 0.538882, acc.: 68.75%] [G loss: 1.376561]\n",
            "2158 [D loss: 0.527831, acc.: 75.00%] [G loss: 1.490469]\n",
            "2159 [D loss: 0.493168, acc.: 70.31%] [G loss: 1.412106]\n",
            "2160 [D loss: 0.506625, acc.: 75.00%] [G loss: 1.490005]\n",
            "2161 [D loss: 0.514871, acc.: 70.31%] [G loss: 1.432886]\n",
            "2162 [D loss: 0.443858, acc.: 76.56%] [G loss: 1.752529]\n",
            "2163 [D loss: 0.493121, acc.: 75.00%] [G loss: 1.299287]\n",
            "2164 [D loss: 0.407489, acc.: 78.12%] [G loss: 1.552516]\n",
            "2165 [D loss: 0.532600, acc.: 65.62%] [G loss: 1.355002]\n",
            "2166 [D loss: 0.534943, acc.: 70.31%] [G loss: 1.530798]\n",
            "2167 [D loss: 0.425881, acc.: 73.44%] [G loss: 1.687232]\n",
            "2168 [D loss: 0.508308, acc.: 73.44%] [G loss: 1.344603]\n",
            "2169 [D loss: 0.447581, acc.: 78.12%] [G loss: 1.362311]\n",
            "2170 [D loss: 0.509091, acc.: 70.31%] [G loss: 1.500952]\n",
            "2171 [D loss: 0.493686, acc.: 71.88%] [G loss: 1.199646]\n",
            "2172 [D loss: 0.487582, acc.: 71.88%] [G loss: 1.233735]\n",
            "2173 [D loss: 0.464007, acc.: 75.00%] [G loss: 1.302260]\n",
            "2174 [D loss: 0.443696, acc.: 79.69%] [G loss: 1.335917]\n",
            "2175 [D loss: 0.485800, acc.: 68.75%] [G loss: 1.125237]\n",
            "2176 [D loss: 0.427982, acc.: 81.25%] [G loss: 1.277563]\n",
            "2177 [D loss: 0.422655, acc.: 76.56%] [G loss: 1.446179]\n",
            "2178 [D loss: 0.470385, acc.: 76.56%] [G loss: 1.666135]\n",
            "2179 [D loss: 0.505336, acc.: 73.44%] [G loss: 1.365034]\n",
            "2180 [D loss: 0.504152, acc.: 75.00%] [G loss: 1.249944]\n",
            "2181 [D loss: 0.445319, acc.: 78.12%] [G loss: 1.237424]\n",
            "2182 [D loss: 0.482478, acc.: 68.75%] [G loss: 1.126126]\n",
            "2183 [D loss: 0.426433, acc.: 81.25%] [G loss: 1.282277]\n",
            "2184 [D loss: 0.432873, acc.: 81.25%] [G loss: 1.368464]\n",
            "2185 [D loss: 0.524257, acc.: 68.75%] [G loss: 1.341857]\n",
            "2186 [D loss: 0.468660, acc.: 78.12%] [G loss: 1.429905]\n",
            "2187 [D loss: 0.473400, acc.: 73.44%] [G loss: 1.342121]\n",
            "2188 [D loss: 0.463288, acc.: 78.12%] [G loss: 1.276297]\n",
            "2189 [D loss: 0.496657, acc.: 75.00%] [G loss: 1.302340]\n",
            "2190 [D loss: 0.445052, acc.: 75.00%] [G loss: 1.244807]\n",
            "2191 [D loss: 0.494900, acc.: 71.88%] [G loss: 1.140771]\n",
            "2192 [D loss: 0.475574, acc.: 75.00%] [G loss: 1.369119]\n",
            "2193 [D loss: 0.481687, acc.: 71.88%] [G loss: 1.379804]\n",
            "2194 [D loss: 0.508692, acc.: 67.19%] [G loss: 1.277930]\n",
            "2195 [D loss: 0.521711, acc.: 68.75%] [G loss: 1.358355]\n",
            "2196 [D loss: 0.468935, acc.: 79.69%] [G loss: 1.241328]\n",
            "2197 [D loss: 0.482248, acc.: 68.75%] [G loss: 1.293273]\n",
            "2198 [D loss: 0.523974, acc.: 70.31%] [G loss: 1.338571]\n",
            "2199 [D loss: 0.496381, acc.: 71.88%] [G loss: 1.369039]\n",
            "2200 [D loss: 0.460818, acc.: 75.00%] [G loss: 1.356320]\n",
            "generated_data\n",
            "2201 [D loss: 0.452486, acc.: 75.00%] [G loss: 1.521887]\n",
            "2202 [D loss: 0.487517, acc.: 73.44%] [G loss: 1.351043]\n",
            "2203 [D loss: 0.472731, acc.: 75.00%] [G loss: 1.501861]\n",
            "2204 [D loss: 0.504041, acc.: 71.88%] [G loss: 1.275011]\n",
            "2205 [D loss: 0.463513, acc.: 68.75%] [G loss: 1.155344]\n",
            "2206 [D loss: 0.413679, acc.: 75.00%] [G loss: 1.294795]\n",
            "2207 [D loss: 0.433473, acc.: 76.56%] [G loss: 1.390472]\n",
            "2208 [D loss: 0.457461, acc.: 78.12%] [G loss: 1.266009]\n",
            "2209 [D loss: 0.531316, acc.: 73.44%] [G loss: 1.440759]\n",
            "2210 [D loss: 0.457300, acc.: 76.56%] [G loss: 1.245469]\n",
            "2211 [D loss: 0.466065, acc.: 70.31%] [G loss: 1.500827]\n",
            "2212 [D loss: 0.491080, acc.: 73.44%] [G loss: 1.440110]\n",
            "2213 [D loss: 0.448900, acc.: 75.00%] [G loss: 1.583815]\n",
            "2214 [D loss: 0.475783, acc.: 71.88%] [G loss: 1.938404]\n",
            "2215 [D loss: 0.530805, acc.: 64.06%] [G loss: 1.334217]\n",
            "2216 [D loss: 0.448798, acc.: 73.44%] [G loss: 1.371248]\n",
            "2217 [D loss: 0.486805, acc.: 67.19%] [G loss: 1.256723]\n",
            "2218 [D loss: 0.419975, acc.: 76.56%] [G loss: 1.453895]\n",
            "2219 [D loss: 0.437977, acc.: 73.44%] [G loss: 1.485597]\n",
            "2220 [D loss: 0.547702, acc.: 62.50%] [G loss: 1.510443]\n",
            "2221 [D loss: 0.456768, acc.: 76.56%] [G loss: 1.126734]\n",
            "2222 [D loss: 0.459495, acc.: 78.12%] [G loss: 1.233572]\n",
            "2223 [D loss: 0.501497, acc.: 71.88%] [G loss: 1.334527]\n",
            "2224 [D loss: 0.425788, acc.: 70.31%] [G loss: 1.495135]\n",
            "2225 [D loss: 0.415609, acc.: 75.00%] [G loss: 1.337631]\n",
            "2226 [D loss: 0.528779, acc.: 71.88%] [G loss: 1.181074]\n",
            "2227 [D loss: 0.479877, acc.: 78.12%] [G loss: 1.324196]\n",
            "2228 [D loss: 0.435886, acc.: 78.12%] [G loss: 1.115105]\n",
            "2229 [D loss: 0.453818, acc.: 70.31%] [G loss: 1.126659]\n",
            "2230 [D loss: 0.472887, acc.: 78.12%] [G loss: 1.241990]\n",
            "2231 [D loss: 0.496353, acc.: 71.88%] [G loss: 1.313435]\n",
            "2232 [D loss: 0.513198, acc.: 70.31%] [G loss: 1.238728]\n",
            "2233 [D loss: 0.480111, acc.: 70.31%] [G loss: 1.237617]\n",
            "2234 [D loss: 0.421842, acc.: 73.44%] [G loss: 1.460240]\n",
            "2235 [D loss: 0.501398, acc.: 71.88%] [G loss: 1.428696]\n",
            "2236 [D loss: 0.512467, acc.: 70.31%] [G loss: 1.409990]\n",
            "2237 [D loss: 0.455091, acc.: 76.56%] [G loss: 1.346204]\n",
            "2238 [D loss: 0.443721, acc.: 73.44%] [G loss: 1.727220]\n",
            "2239 [D loss: 0.588652, acc.: 68.75%] [G loss: 1.124130]\n",
            "2240 [D loss: 0.478877, acc.: 73.44%] [G loss: 1.364766]\n",
            "2241 [D loss: 0.452011, acc.: 75.00%] [G loss: 1.395709]\n",
            "2242 [D loss: 0.498652, acc.: 75.00%] [G loss: 1.279963]\n",
            "2243 [D loss: 0.541115, acc.: 71.88%] [G loss: 1.183939]\n",
            "2244 [D loss: 0.511918, acc.: 75.00%] [G loss: 1.488404]\n",
            "2245 [D loss: 0.537772, acc.: 65.62%] [G loss: 1.393029]\n",
            "2246 [D loss: 0.451702, acc.: 76.56%] [G loss: 1.469355]\n",
            "2247 [D loss: 0.474835, acc.: 73.44%] [G loss: 1.545342]\n",
            "2248 [D loss: 0.475361, acc.: 78.12%] [G loss: 1.212129]\n",
            "2249 [D loss: 0.452694, acc.: 75.00%] [G loss: 1.527607]\n",
            "2250 [D loss: 0.417261, acc.: 79.69%] [G loss: 1.480756]\n",
            "2251 [D loss: 0.470270, acc.: 76.56%] [G loss: 1.258084]\n",
            "2252 [D loss: 0.540820, acc.: 70.31%] [G loss: 1.098520]\n",
            "2253 [D loss: 0.439376, acc.: 78.12%] [G loss: 1.492061]\n",
            "2254 [D loss: 0.474455, acc.: 76.56%] [G loss: 1.406943]\n",
            "2255 [D loss: 0.568976, acc.: 67.19%] [G loss: 1.417721]\n",
            "2256 [D loss: 0.434170, acc.: 78.12%] [G loss: 1.275763]\n",
            "2257 [D loss: 0.539686, acc.: 68.75%] [G loss: 1.587661]\n",
            "2258 [D loss: 0.486762, acc.: 73.44%] [G loss: 1.296163]\n",
            "2259 [D loss: 0.449338, acc.: 73.44%] [G loss: 1.239449]\n",
            "2260 [D loss: 0.461689, acc.: 75.00%] [G loss: 1.474515]\n",
            "2261 [D loss: 0.505457, acc.: 75.00%] [G loss: 1.532698]\n",
            "2262 [D loss: 0.473531, acc.: 73.44%] [G loss: 1.293356]\n",
            "2263 [D loss: 0.537632, acc.: 71.88%] [G loss: 1.460392]\n",
            "2264 [D loss: 0.421076, acc.: 76.56%] [G loss: 1.586065]\n",
            "2265 [D loss: 0.467043, acc.: 78.12%] [G loss: 1.417466]\n",
            "2266 [D loss: 0.494246, acc.: 71.88%] [G loss: 1.269145]\n",
            "2267 [D loss: 0.479783, acc.: 73.44%] [G loss: 1.218286]\n",
            "2268 [D loss: 0.465862, acc.: 75.00%] [G loss: 1.321244]\n",
            "2269 [D loss: 0.471555, acc.: 78.12%] [G loss: 1.609921]\n",
            "2270 [D loss: 0.499054, acc.: 71.88%] [G loss: 1.318412]\n",
            "2271 [D loss: 0.487940, acc.: 70.31%] [G loss: 1.255928]\n",
            "2272 [D loss: 0.472363, acc.: 76.56%] [G loss: 1.358956]\n",
            "2273 [D loss: 0.498559, acc.: 73.44%] [G loss: 1.204380]\n",
            "2274 [D loss: 0.480386, acc.: 73.44%] [G loss: 1.587557]\n",
            "2275 [D loss: 0.493401, acc.: 73.44%] [G loss: 1.067716]\n",
            "2276 [D loss: 0.496409, acc.: 73.44%] [G loss: 1.396622]\n",
            "2277 [D loss: 0.454869, acc.: 68.75%] [G loss: 1.313655]\n",
            "2278 [D loss: 0.500085, acc.: 75.00%] [G loss: 1.289566]\n",
            "2279 [D loss: 0.496321, acc.: 78.12%] [G loss: 1.220350]\n",
            "2280 [D loss: 0.403122, acc.: 79.69%] [G loss: 1.444058]\n",
            "2281 [D loss: 0.458779, acc.: 78.12%] [G loss: 1.187143]\n",
            "2282 [D loss: 0.522801, acc.: 64.06%] [G loss: 1.689871]\n",
            "2283 [D loss: 0.544585, acc.: 64.06%] [G loss: 1.126865]\n",
            "2284 [D loss: 0.494324, acc.: 75.00%] [G loss: 1.255212]\n",
            "2285 [D loss: 0.475842, acc.: 71.88%] [G loss: 1.241202]\n",
            "2286 [D loss: 0.492018, acc.: 71.88%] [G loss: 1.206319]\n",
            "2287 [D loss: 0.509395, acc.: 73.44%] [G loss: 1.282342]\n",
            "2288 [D loss: 0.529569, acc.: 68.75%] [G loss: 1.153338]\n",
            "2289 [D loss: 0.476413, acc.: 70.31%] [G loss: 1.305305]\n",
            "2290 [D loss: 0.479450, acc.: 73.44%] [G loss: 1.414780]\n",
            "2291 [D loss: 0.430251, acc.: 78.12%] [G loss: 1.267067]\n",
            "2292 [D loss: 0.473645, acc.: 78.12%] [G loss: 1.494058]\n",
            "2293 [D loss: 0.472291, acc.: 71.88%] [G loss: 1.505647]\n",
            "2294 [D loss: 0.475722, acc.: 75.00%] [G loss: 1.408687]\n",
            "2295 [D loss: 0.486611, acc.: 71.88%] [G loss: 1.495836]\n",
            "2296 [D loss: 0.526672, acc.: 73.44%] [G loss: 1.573855]\n",
            "2297 [D loss: 0.507274, acc.: 71.88%] [G loss: 1.310974]\n",
            "2298 [D loss: 0.442881, acc.: 76.56%] [G loss: 1.253069]\n",
            "2299 [D loss: 0.454826, acc.: 73.44%] [G loss: 1.476522]\n",
            "2300 [D loss: 0.580882, acc.: 71.88%] [G loss: 1.346075]\n",
            "generated_data\n",
            "2301 [D loss: 0.497788, acc.: 75.00%] [G loss: 1.353240]\n",
            "2302 [D loss: 0.437187, acc.: 75.00%] [G loss: 1.479393]\n",
            "2303 [D loss: 0.521253, acc.: 70.31%] [G loss: 1.461843]\n",
            "2304 [D loss: 0.468261, acc.: 78.12%] [G loss: 1.260958]\n",
            "2305 [D loss: 0.496912, acc.: 68.75%] [G loss: 1.298761]\n",
            "2306 [D loss: 0.497371, acc.: 71.88%] [G loss: 1.377840]\n",
            "2307 [D loss: 0.501254, acc.: 68.75%] [G loss: 1.259939]\n",
            "2308 [D loss: 0.510288, acc.: 75.00%] [G loss: 1.451643]\n",
            "2309 [D loss: 0.505877, acc.: 75.00%] [G loss: 1.430345]\n",
            "2310 [D loss: 0.455681, acc.: 73.44%] [G loss: 1.581658]\n",
            "2311 [D loss: 0.460300, acc.: 78.12%] [G loss: 1.284968]\n",
            "2312 [D loss: 0.405850, acc.: 82.81%] [G loss: 1.414758]\n",
            "2313 [D loss: 0.454732, acc.: 73.44%] [G loss: 1.216454]\n",
            "2314 [D loss: 0.489182, acc.: 70.31%] [G loss: 1.338814]\n",
            "2315 [D loss: 0.512574, acc.: 75.00%] [G loss: 1.263807]\n",
            "2316 [D loss: 0.429068, acc.: 78.12%] [G loss: 1.320432]\n",
            "2317 [D loss: 0.479716, acc.: 71.88%] [G loss: 1.194648]\n",
            "2318 [D loss: 0.463071, acc.: 73.44%] [G loss: 1.156904]\n",
            "2319 [D loss: 0.500133, acc.: 71.88%] [G loss: 1.540441]\n",
            "2320 [D loss: 0.540820, acc.: 75.00%] [G loss: 1.491059]\n",
            "2321 [D loss: 0.516942, acc.: 75.00%] [G loss: 1.621922]\n",
            "2322 [D loss: 0.524003, acc.: 71.88%] [G loss: 1.382011]\n",
            "2323 [D loss: 0.449926, acc.: 81.25%] [G loss: 1.340033]\n",
            "2324 [D loss: 0.454431, acc.: 79.69%] [G loss: 1.404779]\n",
            "2325 [D loss: 0.482316, acc.: 76.56%] [G loss: 1.361874]\n",
            "2326 [D loss: 0.438542, acc.: 71.88%] [G loss: 1.491513]\n",
            "2327 [D loss: 0.493447, acc.: 75.00%] [G loss: 1.407250]\n",
            "2328 [D loss: 0.477497, acc.: 73.44%] [G loss: 1.563522]\n",
            "2329 [D loss: 0.464095, acc.: 78.12%] [G loss: 1.686442]\n",
            "2330 [D loss: 0.483360, acc.: 81.25%] [G loss: 1.372474]\n",
            "2331 [D loss: 0.411692, acc.: 78.12%] [G loss: 1.813622]\n",
            "2332 [D loss: 0.453720, acc.: 78.12%] [G loss: 1.542647]\n",
            "2333 [D loss: 0.428726, acc.: 76.56%] [G loss: 1.652709]\n",
            "2334 [D loss: 0.452060, acc.: 73.44%] [G loss: 1.496969]\n",
            "2335 [D loss: 0.548755, acc.: 65.62%] [G loss: 1.440429]\n",
            "2336 [D loss: 0.483780, acc.: 78.12%] [G loss: 1.734894]\n",
            "2337 [D loss: 0.512274, acc.: 73.44%] [G loss: 1.639370]\n",
            "2338 [D loss: 0.425357, acc.: 76.56%] [G loss: 1.873605]\n",
            "2339 [D loss: 0.526114, acc.: 70.31%] [G loss: 1.168892]\n",
            "2340 [D loss: 0.414634, acc.: 79.69%] [G loss: 1.400995]\n",
            "2341 [D loss: 0.454775, acc.: 71.88%] [G loss: 1.416985]\n",
            "2342 [D loss: 0.482604, acc.: 76.56%] [G loss: 1.130182]\n",
            "2343 [D loss: 0.426480, acc.: 75.00%] [G loss: 1.225551]\n",
            "2344 [D loss: 0.584416, acc.: 75.00%] [G loss: 1.414088]\n",
            "2345 [D loss: 0.441356, acc.: 78.12%] [G loss: 1.443855]\n",
            "2346 [D loss: 0.482016, acc.: 76.56%] [G loss: 1.279793]\n",
            "2347 [D loss: 0.436247, acc.: 76.56%] [G loss: 1.379272]\n",
            "2348 [D loss: 0.510071, acc.: 73.44%] [G loss: 1.374703]\n",
            "2349 [D loss: 0.520033, acc.: 68.75%] [G loss: 1.427234]\n",
            "2350 [D loss: 0.515320, acc.: 76.56%] [G loss: 1.485578]\n",
            "2351 [D loss: 0.451253, acc.: 73.44%] [G loss: 1.303931]\n",
            "2352 [D loss: 0.444020, acc.: 78.12%] [G loss: 1.496861]\n",
            "2353 [D loss: 0.480581, acc.: 73.44%] [G loss: 1.353575]\n",
            "2354 [D loss: 0.499096, acc.: 73.44%] [G loss: 1.260901]\n",
            "2355 [D loss: 0.481674, acc.: 71.88%] [G loss: 1.227771]\n",
            "2356 [D loss: 0.506076, acc.: 70.31%] [G loss: 1.370030]\n",
            "2357 [D loss: 0.458250, acc.: 76.56%] [G loss: 1.529815]\n",
            "2358 [D loss: 0.438198, acc.: 78.12%] [G loss: 1.338608]\n",
            "2359 [D loss: 0.491874, acc.: 71.88%] [G loss: 1.382042]\n",
            "2360 [D loss: 0.456422, acc.: 73.44%] [G loss: 1.287732]\n",
            "2361 [D loss: 0.454854, acc.: 78.12%] [G loss: 1.517025]\n",
            "2362 [D loss: 0.415496, acc.: 78.12%] [G loss: 1.303618]\n",
            "2363 [D loss: 0.421335, acc.: 79.69%] [G loss: 1.419855]\n",
            "2364 [D loss: 0.418098, acc.: 76.56%] [G loss: 1.285844]\n",
            "2365 [D loss: 0.488104, acc.: 73.44%] [G loss: 1.707689]\n",
            "2366 [D loss: 0.475521, acc.: 78.12%] [G loss: 1.342832]\n",
            "2367 [D loss: 0.477766, acc.: 71.88%] [G loss: 1.373215]\n",
            "2368 [D loss: 0.447776, acc.: 78.12%] [G loss: 1.405311]\n",
            "2369 [D loss: 0.493423, acc.: 73.44%] [G loss: 1.414888]\n",
            "2370 [D loss: 0.430742, acc.: 78.12%] [G loss: 1.569563]\n",
            "2371 [D loss: 0.442671, acc.: 79.69%] [G loss: 1.480245]\n",
            "2372 [D loss: 0.547288, acc.: 70.31%] [G loss: 1.474940]\n",
            "2373 [D loss: 0.551784, acc.: 73.44%] [G loss: 1.697397]\n",
            "2374 [D loss: 0.472526, acc.: 78.12%] [G loss: 1.288421]\n",
            "2375 [D loss: 0.529543, acc.: 71.88%] [G loss: 1.229494]\n",
            "2376 [D loss: 0.460735, acc.: 76.56%] [G loss: 1.415966]\n",
            "2377 [D loss: 0.407119, acc.: 79.69%] [G loss: 1.530690]\n",
            "2378 [D loss: 0.435676, acc.: 78.12%] [G loss: 1.546914]\n",
            "2379 [D loss: 0.491721, acc.: 75.00%] [G loss: 1.249404]\n",
            "2380 [D loss: 0.423347, acc.: 78.12%] [G loss: 1.547099]\n",
            "2381 [D loss: 0.469815, acc.: 75.00%] [G loss: 1.437085]\n",
            "2382 [D loss: 0.500524, acc.: 71.88%] [G loss: 1.331153]\n",
            "2383 [D loss: 0.451971, acc.: 76.56%] [G loss: 1.511220]\n",
            "2384 [D loss: 0.534903, acc.: 73.44%] [G loss: 1.626179]\n",
            "2385 [D loss: 0.434826, acc.: 81.25%] [G loss: 1.630916]\n",
            "2386 [D loss: 0.499384, acc.: 76.56%] [G loss: 1.575240]\n",
            "2387 [D loss: 0.419855, acc.: 81.25%] [G loss: 1.525873]\n",
            "2388 [D loss: 0.461188, acc.: 76.56%] [G loss: 1.438025]\n",
            "2389 [D loss: 0.484392, acc.: 73.44%] [G loss: 1.344431]\n",
            "2390 [D loss: 0.436708, acc.: 78.12%] [G loss: 1.345102]\n",
            "2391 [D loss: 0.501491, acc.: 75.00%] [G loss: 1.303416]\n",
            "2392 [D loss: 0.514752, acc.: 71.88%] [G loss: 1.539858]\n",
            "2393 [D loss: 0.455213, acc.: 75.00%] [G loss: 1.469731]\n",
            "2394 [D loss: 0.427299, acc.: 78.12%] [G loss: 1.459600]\n",
            "2395 [D loss: 0.470549, acc.: 73.44%] [G loss: 1.423471]\n",
            "2396 [D loss: 0.444447, acc.: 76.56%] [G loss: 1.616334]\n",
            "2397 [D loss: 0.437908, acc.: 76.56%] [G loss: 1.528557]\n",
            "2398 [D loss: 0.467627, acc.: 76.56%] [G loss: 1.196114]\n",
            "2399 [D loss: 0.524367, acc.: 73.44%] [G loss: 1.157929]\n",
            "2400 [D loss: 0.490575, acc.: 76.56%] [G loss: 1.340561]\n",
            "generated_data\n",
            "2401 [D loss: 0.425055, acc.: 78.12%] [G loss: 1.380303]\n",
            "2402 [D loss: 0.507124, acc.: 65.62%] [G loss: 1.519405]\n",
            "2403 [D loss: 0.428346, acc.: 78.12%] [G loss: 1.610212]\n",
            "2404 [D loss: 0.457754, acc.: 78.12%] [G loss: 1.487269]\n",
            "2405 [D loss: 0.448864, acc.: 76.56%] [G loss: 1.517442]\n",
            "2406 [D loss: 0.475738, acc.: 76.56%] [G loss: 1.333740]\n",
            "2407 [D loss: 0.469138, acc.: 78.12%] [G loss: 1.647937]\n",
            "2408 [D loss: 0.521939, acc.: 68.75%] [G loss: 1.351763]\n",
            "2409 [D loss: 0.439798, acc.: 76.56%] [G loss: 1.303268]\n",
            "2410 [D loss: 0.422848, acc.: 78.12%] [G loss: 1.474273]\n",
            "2411 [D loss: 0.456531, acc.: 78.12%] [G loss: 1.281071]\n",
            "2412 [D loss: 0.508641, acc.: 70.31%] [G loss: 1.288890]\n",
            "2413 [D loss: 0.470485, acc.: 76.56%] [G loss: 1.254699]\n",
            "2414 [D loss: 0.432230, acc.: 76.56%] [G loss: 1.475507]\n",
            "2415 [D loss: 0.442628, acc.: 79.69%] [G loss: 1.480031]\n",
            "2416 [D loss: 0.484137, acc.: 75.00%] [G loss: 1.653215]\n",
            "2417 [D loss: 0.564963, acc.: 75.00%] [G loss: 1.475816]\n",
            "2418 [D loss: 0.452761, acc.: 78.12%] [G loss: 1.406730]\n",
            "2419 [D loss: 0.543509, acc.: 73.44%] [G loss: 1.324408]\n",
            "2420 [D loss: 0.444110, acc.: 75.00%] [G loss: 1.323949]\n",
            "2421 [D loss: 0.453428, acc.: 73.44%] [G loss: 1.341097]\n",
            "2422 [D loss: 0.471554, acc.: 71.88%] [G loss: 1.488051]\n",
            "2423 [D loss: 0.508486, acc.: 67.19%] [G loss: 1.570250]\n",
            "2424 [D loss: 0.486352, acc.: 73.44%] [G loss: 1.385349]\n",
            "2425 [D loss: 0.464914, acc.: 71.88%] [G loss: 1.556554]\n",
            "2426 [D loss: 0.494320, acc.: 75.00%] [G loss: 1.392600]\n",
            "2427 [D loss: 0.435523, acc.: 76.56%] [G loss: 1.440847]\n",
            "2428 [D loss: 0.488867, acc.: 75.00%] [G loss: 1.609949]\n",
            "2429 [D loss: 0.475916, acc.: 73.44%] [G loss: 1.437483]\n",
            "2430 [D loss: 0.407996, acc.: 75.00%] [G loss: 1.467061]\n",
            "2431 [D loss: 0.454298, acc.: 76.56%] [G loss: 1.636606]\n",
            "2432 [D loss: 0.489535, acc.: 70.31%] [G loss: 1.306998]\n",
            "2433 [D loss: 0.509385, acc.: 76.56%] [G loss: 1.369582]\n",
            "2434 [D loss: 0.463632, acc.: 76.56%] [G loss: 1.470176]\n",
            "2435 [D loss: 0.494563, acc.: 76.56%] [G loss: 1.282724]\n",
            "2436 [D loss: 0.449762, acc.: 76.56%] [G loss: 1.729054]\n",
            "2437 [D loss: 0.420137, acc.: 73.44%] [G loss: 1.688469]\n",
            "2438 [D loss: 0.457840, acc.: 76.56%] [G loss: 1.436754]\n",
            "2439 [D loss: 0.481942, acc.: 71.88%] [G loss: 1.359954]\n",
            "2440 [D loss: 0.387181, acc.: 78.12%] [G loss: 1.428841]\n",
            "2441 [D loss: 0.444933, acc.: 75.00%] [G loss: 1.627084]\n",
            "2442 [D loss: 0.462860, acc.: 73.44%] [G loss: 1.942230]\n",
            "2443 [D loss: 0.506936, acc.: 75.00%] [G loss: 1.459951]\n",
            "2444 [D loss: 0.420737, acc.: 76.56%] [G loss: 1.520698]\n",
            "2445 [D loss: 0.382671, acc.: 76.56%] [G loss: 1.457762]\n",
            "2446 [D loss: 0.395921, acc.: 76.56%] [G loss: 1.409808]\n",
            "2447 [D loss: 0.461626, acc.: 73.44%] [G loss: 1.559157]\n",
            "2448 [D loss: 0.501091, acc.: 71.88%] [G loss: 1.143319]\n",
            "2449 [D loss: 0.442333, acc.: 75.00%] [G loss: 1.450307]\n",
            "2450 [D loss: 0.471797, acc.: 73.44%] [G loss: 1.364955]\n",
            "2451 [D loss: 0.490899, acc.: 78.12%] [G loss: 1.400036]\n",
            "2452 [D loss: 0.471549, acc.: 73.44%] [G loss: 1.556476]\n",
            "2453 [D loss: 0.479884, acc.: 71.88%] [G loss: 1.440803]\n",
            "2454 [D loss: 0.435838, acc.: 78.12%] [G loss: 1.428317]\n",
            "2455 [D loss: 0.485249, acc.: 73.44%] [G loss: 1.409668]\n",
            "2456 [D loss: 0.436204, acc.: 76.56%] [G loss: 1.420429]\n",
            "2457 [D loss: 0.564841, acc.: 71.88%] [G loss: 1.248498]\n",
            "2458 [D loss: 0.472724, acc.: 70.31%] [G loss: 1.588488]\n",
            "2459 [D loss: 0.559362, acc.: 70.31%] [G loss: 1.552317]\n",
            "2460 [D loss: 0.502913, acc.: 75.00%] [G loss: 1.648282]\n",
            "2461 [D loss: 0.474644, acc.: 71.88%] [G loss: 1.637638]\n",
            "2462 [D loss: 0.499588, acc.: 68.75%] [G loss: 1.432606]\n",
            "2463 [D loss: 0.419261, acc.: 73.44%] [G loss: 1.456884]\n",
            "2464 [D loss: 0.423916, acc.: 73.44%] [G loss: 1.776940]\n",
            "2465 [D loss: 0.459278, acc.: 76.56%] [G loss: 1.687194]\n",
            "2466 [D loss: 0.512971, acc.: 70.31%] [G loss: 1.259331]\n",
            "2467 [D loss: 0.469079, acc.: 75.00%] [G loss: 1.348413]\n",
            "2468 [D loss: 0.488472, acc.: 70.31%] [G loss: 1.276422]\n",
            "2469 [D loss: 0.513194, acc.: 76.56%] [G loss: 1.342607]\n",
            "2470 [D loss: 0.511444, acc.: 75.00%] [G loss: 1.475912]\n",
            "2471 [D loss: 0.439516, acc.: 78.12%] [G loss: 1.401056]\n",
            "2472 [D loss: 0.493973, acc.: 75.00%] [G loss: 1.649603]\n",
            "2473 [D loss: 0.435983, acc.: 76.56%] [G loss: 1.574953]\n",
            "2474 [D loss: 0.435125, acc.: 79.69%] [G loss: 1.282043]\n",
            "2475 [D loss: 0.507333, acc.: 71.88%] [G loss: 1.212983]\n",
            "2476 [D loss: 0.456239, acc.: 73.44%] [G loss: 1.315341]\n",
            "2477 [D loss: 0.468776, acc.: 73.44%] [G loss: 1.388831]\n",
            "2478 [D loss: 0.469990, acc.: 76.56%] [G loss: 1.154454]\n",
            "2479 [D loss: 0.444192, acc.: 75.00%] [G loss: 1.314154]\n",
            "2480 [D loss: 0.447222, acc.: 76.56%] [G loss: 1.462717]\n",
            "2481 [D loss: 0.488099, acc.: 75.00%] [G loss: 1.506141]\n",
            "2482 [D loss: 0.480865, acc.: 75.00%] [G loss: 1.396411]\n",
            "2483 [D loss: 0.452524, acc.: 75.00%] [G loss: 1.658115]\n",
            "2484 [D loss: 0.520929, acc.: 73.44%] [G loss: 1.480685]\n",
            "2485 [D loss: 0.444947, acc.: 73.44%] [G loss: 1.347933]\n",
            "2486 [D loss: 0.491786, acc.: 75.00%] [G loss: 1.193172]\n",
            "2487 [D loss: 0.426635, acc.: 78.12%] [G loss: 1.360026]\n",
            "2488 [D loss: 0.467455, acc.: 78.12%] [G loss: 1.319705]\n",
            "2489 [D loss: 0.434220, acc.: 76.56%] [G loss: 1.492559]\n",
            "2490 [D loss: 0.451260, acc.: 78.12%] [G loss: 1.494612]\n",
            "2491 [D loss: 0.486807, acc.: 73.44%] [G loss: 1.310196]\n",
            "2492 [D loss: 0.444473, acc.: 75.00%] [G loss: 1.516845]\n",
            "2493 [D loss: 0.494585, acc.: 71.88%] [G loss: 1.263143]\n",
            "2494 [D loss: 0.500730, acc.: 75.00%] [G loss: 1.465796]\n",
            "2495 [D loss: 0.450731, acc.: 75.00%] [G loss: 1.272052]\n",
            "2496 [D loss: 0.445153, acc.: 75.00%] [G loss: 1.359691]\n",
            "2497 [D loss: 0.462525, acc.: 73.44%] [G loss: 1.754627]\n",
            "2498 [D loss: 0.529553, acc.: 70.31%] [G loss: 1.393051]\n",
            "2499 [D loss: 0.493468, acc.: 71.88%] [G loss: 1.336627]\n",
            "2500 [D loss: 0.438122, acc.: 75.00%] [G loss: 1.668223]\n",
            "generated_data\n",
            "2501 [D loss: 0.451612, acc.: 76.56%] [G loss: 1.579092]\n",
            "2502 [D loss: 0.438625, acc.: 75.00%] [G loss: 1.370440]\n",
            "2503 [D loss: 0.513178, acc.: 73.44%] [G loss: 1.544021]\n",
            "2504 [D loss: 0.436559, acc.: 76.56%] [G loss: 1.457498]\n",
            "2505 [D loss: 0.414159, acc.: 75.00%] [G loss: 1.257030]\n",
            "2506 [D loss: 0.406991, acc.: 78.12%] [G loss: 1.666979]\n",
            "2507 [D loss: 0.418247, acc.: 76.56%] [G loss: 1.767727]\n",
            "2508 [D loss: 0.425222, acc.: 76.56%] [G loss: 1.488720]\n",
            "2509 [D loss: 0.446331, acc.: 71.88%] [G loss: 1.326231]\n",
            "2510 [D loss: 0.471530, acc.: 73.44%] [G loss: 1.542141]\n",
            "2511 [D loss: 0.433102, acc.: 76.56%] [G loss: 1.845906]\n",
            "2512 [D loss: 0.491708, acc.: 70.31%] [G loss: 1.229124]\n",
            "2513 [D loss: 0.428557, acc.: 78.12%] [G loss: 1.589066]\n",
            "2514 [D loss: 0.450362, acc.: 78.12%] [G loss: 1.268159]\n",
            "2515 [D loss: 0.469654, acc.: 76.56%] [G loss: 1.309636]\n",
            "2516 [D loss: 0.437553, acc.: 75.00%] [G loss: 1.425268]\n",
            "2517 [D loss: 0.425427, acc.: 76.56%] [G loss: 1.509944]\n",
            "2518 [D loss: 0.449364, acc.: 73.44%] [G loss: 1.702469]\n",
            "2519 [D loss: 0.459901, acc.: 76.56%] [G loss: 1.515000]\n",
            "2520 [D loss: 0.477091, acc.: 76.56%] [G loss: 1.648744]\n",
            "2521 [D loss: 0.487198, acc.: 76.56%] [G loss: 1.538488]\n",
            "2522 [D loss: 0.483841, acc.: 75.00%] [G loss: 1.462590]\n",
            "2523 [D loss: 0.433864, acc.: 75.00%] [G loss: 1.753676]\n",
            "2524 [D loss: 0.493456, acc.: 75.00%] [G loss: 1.651028]\n",
            "2525 [D loss: 0.429900, acc.: 73.44%] [G loss: 1.508439]\n",
            "2526 [D loss: 0.489951, acc.: 75.00%] [G loss: 1.351342]\n",
            "2527 [D loss: 0.446736, acc.: 76.56%] [G loss: 1.518974]\n",
            "2528 [D loss: 0.428732, acc.: 76.56%] [G loss: 1.308440]\n",
            "2529 [D loss: 0.452757, acc.: 76.56%] [G loss: 1.814643]\n",
            "2530 [D loss: 0.433880, acc.: 75.00%] [G loss: 1.660114]\n",
            "2531 [D loss: 0.455258, acc.: 76.56%] [G loss: 1.627593]\n",
            "2532 [D loss: 0.410677, acc.: 75.00%] [G loss: 1.464140]\n",
            "2533 [D loss: 0.498810, acc.: 73.44%] [G loss: 1.298963]\n",
            "2534 [D loss: 0.438240, acc.: 75.00%] [G loss: 1.660843]\n",
            "2535 [D loss: 0.466355, acc.: 76.56%] [G loss: 1.739415]\n",
            "2536 [D loss: 0.418027, acc.: 78.12%] [G loss: 1.281008]\n",
            "2537 [D loss: 0.500329, acc.: 70.31%] [G loss: 1.282774]\n",
            "2538 [D loss: 0.485790, acc.: 70.31%] [G loss: 1.375264]\n",
            "2539 [D loss: 0.437217, acc.: 76.56%] [G loss: 1.313528]\n",
            "2540 [D loss: 0.446570, acc.: 73.44%] [G loss: 1.384013]\n",
            "2541 [D loss: 0.447027, acc.: 76.56%] [G loss: 1.268456]\n",
            "2542 [D loss: 0.441414, acc.: 78.12%] [G loss: 1.715308]\n",
            "2543 [D loss: 0.457109, acc.: 75.00%] [G loss: 1.338024]\n",
            "2544 [D loss: 0.434961, acc.: 76.56%] [G loss: 1.699898]\n",
            "2545 [D loss: 0.409751, acc.: 81.25%] [G loss: 1.385488]\n",
            "2546 [D loss: 0.418269, acc.: 81.25%] [G loss: 1.194123]\n",
            "2547 [D loss: 0.433644, acc.: 79.69%] [G loss: 1.491887]\n",
            "2548 [D loss: 0.444099, acc.: 78.12%] [G loss: 1.308209]\n",
            "2549 [D loss: 0.487192, acc.: 68.75%] [G loss: 1.475832]\n",
            "2550 [D loss: 0.448181, acc.: 76.56%] [G loss: 1.521788]\n",
            "2551 [D loss: 0.465877, acc.: 75.00%] [G loss: 1.350549]\n",
            "2552 [D loss: 0.412840, acc.: 79.69%] [G loss: 1.454045]\n",
            "2553 [D loss: 0.432616, acc.: 73.44%] [G loss: 1.540862]\n",
            "2554 [D loss: 0.513449, acc.: 75.00%] [G loss: 1.815442]\n",
            "2555 [D loss: 0.454126, acc.: 75.00%] [G loss: 1.290137]\n",
            "2556 [D loss: 0.429622, acc.: 78.12%] [G loss: 1.301195]\n",
            "2557 [D loss: 0.469519, acc.: 71.88%] [G loss: 1.473942]\n",
            "2558 [D loss: 0.452219, acc.: 76.56%] [G loss: 1.393230]\n",
            "2559 [D loss: 0.406854, acc.: 78.12%] [G loss: 1.369985]\n",
            "2560 [D loss: 0.409988, acc.: 76.56%] [G loss: 1.906256]\n",
            "2561 [D loss: 0.499981, acc.: 71.88%] [G loss: 1.361713]\n",
            "2562 [D loss: 0.451070, acc.: 76.56%] [G loss: 1.427101]\n",
            "2563 [D loss: 0.434110, acc.: 76.56%] [G loss: 1.413359]\n",
            "2564 [D loss: 0.451620, acc.: 76.56%] [G loss: 1.272900]\n",
            "2565 [D loss: 0.415653, acc.: 79.69%] [G loss: 1.214790]\n",
            "2566 [D loss: 0.499947, acc.: 68.75%] [G loss: 1.322172]\n",
            "2567 [D loss: 0.460017, acc.: 75.00%] [G loss: 1.295074]\n",
            "2568 [D loss: 0.479001, acc.: 75.00%] [G loss: 1.185607]\n",
            "2569 [D loss: 0.434448, acc.: 82.81%] [G loss: 1.574179]\n",
            "2570 [D loss: 0.484773, acc.: 68.75%] [G loss: 1.308568]\n",
            "2571 [D loss: 0.430633, acc.: 76.56%] [G loss: 1.262723]\n",
            "2572 [D loss: 0.558519, acc.: 73.44%] [G loss: 1.537500]\n",
            "2573 [D loss: 0.513251, acc.: 71.88%] [G loss: 1.455485]\n",
            "2574 [D loss: 0.452206, acc.: 76.56%] [G loss: 1.371064]\n",
            "2575 [D loss: 0.504036, acc.: 71.88%] [G loss: 1.495923]\n",
            "2576 [D loss: 0.476896, acc.: 75.00%] [G loss: 1.432465]\n",
            "2577 [D loss: 0.453293, acc.: 75.00%] [G loss: 1.647620]\n",
            "2578 [D loss: 0.513785, acc.: 71.88%] [G loss: 1.216652]\n",
            "2579 [D loss: 0.464516, acc.: 76.56%] [G loss: 1.695813]\n",
            "2580 [D loss: 0.481968, acc.: 75.00%] [G loss: 1.354571]\n",
            "2581 [D loss: 0.479110, acc.: 76.56%] [G loss: 1.237929]\n",
            "2582 [D loss: 0.508349, acc.: 71.88%] [G loss: 1.281355]\n",
            "2583 [D loss: 0.433669, acc.: 78.12%] [G loss: 1.659385]\n",
            "2584 [D loss: 0.463544, acc.: 75.00%] [G loss: 1.478834]\n",
            "2585 [D loss: 0.466040, acc.: 76.56%] [G loss: 1.353043]\n",
            "2586 [D loss: 0.537448, acc.: 71.88%] [G loss: 1.328470]\n",
            "2587 [D loss: 0.480820, acc.: 73.44%] [G loss: 1.302207]\n",
            "2588 [D loss: 0.468254, acc.: 73.44%] [G loss: 1.615821]\n",
            "2589 [D loss: 0.453749, acc.: 73.44%] [G loss: 1.618994]\n",
            "2590 [D loss: 0.450384, acc.: 75.00%] [G loss: 1.524441]\n",
            "2591 [D loss: 0.420214, acc.: 79.69%] [G loss: 1.633044]\n",
            "2592 [D loss: 0.559740, acc.: 71.88%] [G loss: 1.226734]\n",
            "2593 [D loss: 0.429571, acc.: 79.69%] [G loss: 1.362401]\n",
            "2594 [D loss: 0.461160, acc.: 76.56%] [G loss: 1.481368]\n",
            "2595 [D loss: 0.430402, acc.: 78.12%] [G loss: 1.537547]\n",
            "2596 [D loss: 0.465810, acc.: 75.00%] [G loss: 1.362711]\n",
            "2597 [D loss: 0.441676, acc.: 75.00%] [G loss: 1.771374]\n",
            "2598 [D loss: 0.474045, acc.: 73.44%] [G loss: 1.691801]\n",
            "2599 [D loss: 0.509892, acc.: 70.31%] [G loss: 1.608445]\n",
            "2600 [D loss: 0.468357, acc.: 76.56%] [G loss: 1.617428]\n",
            "generated_data\n",
            "2601 [D loss: 0.547217, acc.: 70.31%] [G loss: 1.341614]\n",
            "2602 [D loss: 0.470427, acc.: 73.44%] [G loss: 1.707030]\n",
            "2603 [D loss: 0.437544, acc.: 75.00%] [G loss: 1.463770]\n",
            "2604 [D loss: 0.487586, acc.: 71.88%] [G loss: 1.617058]\n",
            "2605 [D loss: 0.430641, acc.: 76.56%] [G loss: 1.471575]\n",
            "2606 [D loss: 0.463190, acc.: 78.12%] [G loss: 1.182798]\n",
            "2607 [D loss: 0.424061, acc.: 75.00%] [G loss: 1.584474]\n",
            "2608 [D loss: 0.443643, acc.: 73.44%] [G loss: 1.487575]\n",
            "2609 [D loss: 0.484750, acc.: 75.00%] [G loss: 1.687940]\n",
            "2610 [D loss: 0.476802, acc.: 73.44%] [G loss: 1.420536]\n",
            "2611 [D loss: 0.402468, acc.: 79.69%] [G loss: 1.382098]\n",
            "2612 [D loss: 0.371490, acc.: 78.12%] [G loss: 1.381956]\n",
            "2613 [D loss: 0.456203, acc.: 75.00%] [G loss: 1.242014]\n",
            "2614 [D loss: 0.489837, acc.: 68.75%] [G loss: 1.356657]\n",
            "2615 [D loss: 0.450700, acc.: 78.12%] [G loss: 1.578228]\n",
            "2616 [D loss: 0.470143, acc.: 78.12%] [G loss: 1.366920]\n",
            "2617 [D loss: 0.447408, acc.: 73.44%] [G loss: 1.634383]\n",
            "2618 [D loss: 0.462748, acc.: 73.44%] [G loss: 1.537836]\n",
            "2619 [D loss: 0.450117, acc.: 71.88%] [G loss: 1.470908]\n",
            "2620 [D loss: 0.451182, acc.: 79.69%] [G loss: 1.575680]\n",
            "2621 [D loss: 0.456598, acc.: 73.44%] [G loss: 1.428087]\n",
            "2622 [D loss: 0.491769, acc.: 73.44%] [G loss: 1.426784]\n",
            "2623 [D loss: 0.589355, acc.: 68.75%] [G loss: 1.777391]\n",
            "2624 [D loss: 0.415147, acc.: 79.69%] [G loss: 1.480638]\n",
            "2625 [D loss: 0.458787, acc.: 75.00%] [G loss: 1.589597]\n",
            "2626 [D loss: 0.378602, acc.: 79.69%] [G loss: 1.654735]\n",
            "2627 [D loss: 0.461622, acc.: 73.44%] [G loss: 1.463041]\n",
            "2628 [D loss: 0.422773, acc.: 76.56%] [G loss: 1.610690]\n",
            "2629 [D loss: 0.445555, acc.: 79.69%] [G loss: 1.664749]\n",
            "2630 [D loss: 0.472834, acc.: 73.44%] [G loss: 1.413455]\n",
            "2631 [D loss: 0.440396, acc.: 78.12%] [G loss: 1.627737]\n",
            "2632 [D loss: 0.452244, acc.: 78.12%] [G loss: 1.265973]\n",
            "2633 [D loss: 0.441428, acc.: 79.69%] [G loss: 1.190579]\n",
            "2634 [D loss: 0.462933, acc.: 79.69%] [G loss: 1.364338]\n",
            "2635 [D loss: 0.515620, acc.: 75.00%] [G loss: 1.498743]\n",
            "2636 [D loss: 0.525107, acc.: 71.88%] [G loss: 1.265352]\n",
            "2637 [D loss: 0.455209, acc.: 75.00%] [G loss: 1.372704]\n",
            "2638 [D loss: 0.396737, acc.: 76.56%] [G loss: 1.493329]\n",
            "2639 [D loss: 0.411272, acc.: 76.56%] [G loss: 1.514050]\n",
            "2640 [D loss: 0.421165, acc.: 76.56%] [G loss: 1.524074]\n",
            "2641 [D loss: 0.455934, acc.: 75.00%] [G loss: 1.495039]\n",
            "2642 [D loss: 0.478181, acc.: 75.00%] [G loss: 1.350176]\n",
            "2643 [D loss: 0.437620, acc.: 73.44%] [G loss: 1.400762]\n",
            "2644 [D loss: 0.452872, acc.: 78.12%] [G loss: 1.387727]\n",
            "2645 [D loss: 0.474535, acc.: 73.44%] [G loss: 1.569847]\n",
            "2646 [D loss: 0.485882, acc.: 73.44%] [G loss: 1.476364]\n",
            "2647 [D loss: 0.466452, acc.: 71.88%] [G loss: 1.223299]\n",
            "2648 [D loss: 0.416155, acc.: 75.00%] [G loss: 1.448249]\n",
            "2649 [D loss: 0.414486, acc.: 78.12%] [G loss: 1.797847]\n",
            "2650 [D loss: 0.450147, acc.: 76.56%] [G loss: 1.384290]\n",
            "2651 [D loss: 0.450193, acc.: 71.88%] [G loss: 1.786361]\n",
            "2652 [D loss: 0.410998, acc.: 76.56%] [G loss: 1.593305]\n",
            "2653 [D loss: 0.407722, acc.: 76.56%] [G loss: 1.549404]\n",
            "2654 [D loss: 0.457947, acc.: 75.00%] [G loss: 1.497340]\n",
            "2655 [D loss: 0.390644, acc.: 79.69%] [G loss: 1.562461]\n",
            "2656 [D loss: 0.452389, acc.: 73.44%] [G loss: 1.942149]\n",
            "2657 [D loss: 0.438730, acc.: 76.56%] [G loss: 1.518047]\n",
            "2658 [D loss: 0.452443, acc.: 76.56%] [G loss: 1.638409]\n",
            "2659 [D loss: 0.453299, acc.: 76.56%] [G loss: 2.022833]\n",
            "2660 [D loss: 0.408125, acc.: 79.69%] [G loss: 1.531547]\n",
            "2661 [D loss: 0.458591, acc.: 78.12%] [G loss: 1.250066]\n",
            "2662 [D loss: 0.482489, acc.: 75.00%] [G loss: 1.531075]\n",
            "2663 [D loss: 0.416621, acc.: 79.69%] [G loss: 1.664446]\n",
            "2664 [D loss: 0.463556, acc.: 71.88%] [G loss: 1.354036]\n",
            "2665 [D loss: 0.417184, acc.: 79.69%] [G loss: 1.363698]\n",
            "2666 [D loss: 0.381958, acc.: 78.12%] [G loss: 1.469533]\n",
            "2667 [D loss: 0.465836, acc.: 73.44%] [G loss: 1.551874]\n",
            "2668 [D loss: 0.452675, acc.: 76.56%] [G loss: 1.360407]\n",
            "2669 [D loss: 0.410454, acc.: 78.12%] [G loss: 1.489230]\n",
            "2670 [D loss: 0.454287, acc.: 75.00%] [G loss: 1.487910]\n",
            "2671 [D loss: 0.487257, acc.: 71.88%] [G loss: 1.424685]\n",
            "2672 [D loss: 0.476366, acc.: 73.44%] [G loss: 1.806687]\n",
            "2673 [D loss: 0.418150, acc.: 78.12%] [G loss: 1.466840]\n",
            "2674 [D loss: 0.478793, acc.: 73.44%] [G loss: 1.602543]\n",
            "2675 [D loss: 0.497215, acc.: 70.31%] [G loss: 1.689424]\n",
            "2676 [D loss: 0.440306, acc.: 78.12%] [G loss: 1.532609]\n",
            "2677 [D loss: 0.426828, acc.: 76.56%] [G loss: 1.548739]\n",
            "2678 [D loss: 0.501256, acc.: 73.44%] [G loss: 1.459054]\n",
            "2679 [D loss: 0.495236, acc.: 76.56%] [G loss: 1.338921]\n",
            "2680 [D loss: 0.445051, acc.: 73.44%] [G loss: 1.330360]\n",
            "2681 [D loss: 0.429212, acc.: 73.44%] [G loss: 1.528660]\n",
            "2682 [D loss: 0.478681, acc.: 73.44%] [G loss: 1.440499]\n",
            "2683 [D loss: 0.451148, acc.: 76.56%] [G loss: 1.471803]\n",
            "2684 [D loss: 0.437591, acc.: 76.56%] [G loss: 1.658577]\n",
            "2685 [D loss: 0.511675, acc.: 75.00%] [G loss: 1.480145]\n",
            "2686 [D loss: 0.441960, acc.: 75.00%] [G loss: 1.242508]\n",
            "2687 [D loss: 0.423849, acc.: 79.69%] [G loss: 1.552316]\n",
            "2688 [D loss: 0.534486, acc.: 73.44%] [G loss: 1.552423]\n",
            "2689 [D loss: 0.421553, acc.: 78.12%] [G loss: 1.784571]\n",
            "2690 [D loss: 0.459738, acc.: 76.56%] [G loss: 1.586705]\n",
            "2691 [D loss: 0.504566, acc.: 73.44%] [G loss: 1.551798]\n",
            "2692 [D loss: 0.458559, acc.: 78.12%] [G loss: 1.553206]\n",
            "2693 [D loss: 0.477976, acc.: 78.12%] [G loss: 1.615105]\n",
            "2694 [D loss: 0.453649, acc.: 78.12%] [G loss: 1.464362]\n",
            "2695 [D loss: 0.488369, acc.: 73.44%] [G loss: 1.556243]\n",
            "2696 [D loss: 0.418399, acc.: 75.00%] [G loss: 1.484409]\n",
            "2697 [D loss: 0.474724, acc.: 76.56%] [G loss: 1.478114]\n",
            "2698 [D loss: 0.450340, acc.: 76.56%] [G loss: 1.231149]\n",
            "2699 [D loss: 0.456996, acc.: 73.44%] [G loss: 1.543732]\n",
            "2700 [D loss: 0.457520, acc.: 71.88%] [G loss: 1.417834]\n",
            "generated_data\n",
            "2701 [D loss: 0.462581, acc.: 75.00%] [G loss: 1.250030]\n",
            "2702 [D loss: 0.427605, acc.: 76.56%] [G loss: 1.285337]\n",
            "2703 [D loss: 0.413193, acc.: 76.56%] [G loss: 1.497807]\n",
            "2704 [D loss: 0.379452, acc.: 76.56%] [G loss: 1.557537]\n",
            "2705 [D loss: 0.460717, acc.: 76.56%] [G loss: 1.758082]\n",
            "2706 [D loss: 0.431776, acc.: 76.56%] [G loss: 1.602088]\n",
            "2707 [D loss: 0.427492, acc.: 76.56%] [G loss: 1.288427]\n",
            "2708 [D loss: 0.479426, acc.: 76.56%] [G loss: 1.392033]\n",
            "2709 [D loss: 0.480963, acc.: 71.88%] [G loss: 1.420184]\n",
            "2710 [D loss: 0.498601, acc.: 73.44%] [G loss: 1.459615]\n",
            "2711 [D loss: 0.442473, acc.: 73.44%] [G loss: 1.392516]\n",
            "2712 [D loss: 0.462784, acc.: 73.44%] [G loss: 1.426937]\n",
            "2713 [D loss: 0.438840, acc.: 73.44%] [G loss: 1.458132]\n",
            "2714 [D loss: 0.521349, acc.: 76.56%] [G loss: 1.624664]\n",
            "2715 [D loss: 0.435908, acc.: 73.44%] [G loss: 1.744158]\n",
            "2716 [D loss: 0.509939, acc.: 73.44%] [G loss: 1.611493]\n",
            "2717 [D loss: 0.506558, acc.: 73.44%] [G loss: 1.528934]\n",
            "2718 [D loss: 0.414796, acc.: 78.12%] [G loss: 1.640196]\n",
            "2719 [D loss: 0.491258, acc.: 71.88%] [G loss: 1.695165]\n",
            "2720 [D loss: 0.453523, acc.: 73.44%] [G loss: 1.478140]\n",
            "2721 [D loss: 0.531269, acc.: 70.31%] [G loss: 1.350290]\n",
            "2722 [D loss: 0.466934, acc.: 73.44%] [G loss: 1.352858]\n",
            "2723 [D loss: 0.445098, acc.: 75.00%] [G loss: 1.524320]\n",
            "2724 [D loss: 0.512292, acc.: 68.75%] [G loss: 1.868222]\n",
            "2725 [D loss: 0.491443, acc.: 71.88%] [G loss: 1.349559]\n",
            "2726 [D loss: 0.471877, acc.: 73.44%] [G loss: 1.510047]\n",
            "2727 [D loss: 0.459892, acc.: 78.12%] [G loss: 1.513223]\n",
            "2728 [D loss: 0.498938, acc.: 75.00%] [G loss: 1.216227]\n",
            "2729 [D loss: 0.426938, acc.: 78.12%] [G loss: 1.453594]\n",
            "2730 [D loss: 0.468221, acc.: 70.31%] [G loss: 1.456370]\n",
            "2731 [D loss: 0.463734, acc.: 68.75%] [G loss: 1.344390]\n",
            "2732 [D loss: 0.511938, acc.: 73.44%] [G loss: 1.643792]\n",
            "2733 [D loss: 0.456097, acc.: 76.56%] [G loss: 1.730184]\n",
            "2734 [D loss: 0.464487, acc.: 71.88%] [G loss: 1.551957]\n",
            "2735 [D loss: 0.443942, acc.: 75.00%] [G loss: 1.621998]\n",
            "2736 [D loss: 0.472004, acc.: 73.44%] [G loss: 1.318747]\n",
            "2737 [D loss: 0.437801, acc.: 75.00%] [G loss: 1.627624]\n",
            "2738 [D loss: 0.589701, acc.: 70.31%] [G loss: 1.282717]\n",
            "2739 [D loss: 0.473599, acc.: 73.44%] [G loss: 1.462036]\n",
            "2740 [D loss: 0.482534, acc.: 76.56%] [G loss: 1.327206]\n",
            "2741 [D loss: 0.446619, acc.: 75.00%] [G loss: 1.508466]\n",
            "2742 [D loss: 0.528572, acc.: 71.88%] [G loss: 1.880320]\n",
            "2743 [D loss: 0.495989, acc.: 75.00%] [G loss: 1.332236]\n",
            "2744 [D loss: 0.450725, acc.: 76.56%] [G loss: 1.309838]\n",
            "2745 [D loss: 0.481965, acc.: 76.56%] [G loss: 1.354656]\n",
            "2746 [D loss: 0.481946, acc.: 75.00%] [G loss: 1.498902]\n",
            "2747 [D loss: 0.459651, acc.: 75.00%] [G loss: 1.254333]\n",
            "2748 [D loss: 0.451329, acc.: 75.00%] [G loss: 1.334873]\n",
            "2749 [D loss: 0.456624, acc.: 76.56%] [G loss: 1.411246]\n",
            "2750 [D loss: 0.455865, acc.: 71.88%] [G loss: 1.308137]\n",
            "2751 [D loss: 0.449387, acc.: 78.12%] [G loss: 1.305756]\n",
            "2752 [D loss: 0.436381, acc.: 75.00%] [G loss: 1.215399]\n",
            "2753 [D loss: 0.423396, acc.: 78.12%] [G loss: 1.513530]\n",
            "2754 [D loss: 0.443593, acc.: 78.12%] [G loss: 1.424722]\n",
            "2755 [D loss: 0.459573, acc.: 76.56%] [G loss: 1.270775]\n",
            "2756 [D loss: 0.471863, acc.: 73.44%] [G loss: 1.429495]\n",
            "2757 [D loss: 0.426381, acc.: 78.12%] [G loss: 1.404337]\n",
            "2758 [D loss: 0.508321, acc.: 71.88%] [G loss: 1.480255]\n",
            "2759 [D loss: 0.390699, acc.: 78.12%] [G loss: 1.766623]\n",
            "2760 [D loss: 0.472522, acc.: 75.00%] [G loss: 1.317931]\n",
            "2761 [D loss: 0.434452, acc.: 76.56%] [G loss: 1.521690]\n",
            "2762 [D loss: 0.415062, acc.: 76.56%] [G loss: 1.467371]\n",
            "2763 [D loss: 0.491122, acc.: 75.00%] [G loss: 1.397520]\n",
            "2764 [D loss: 0.446441, acc.: 78.12%] [G loss: 1.305791]\n",
            "2765 [D loss: 0.413112, acc.: 76.56%] [G loss: 1.775357]\n",
            "2766 [D loss: 0.486435, acc.: 75.00%] [G loss: 1.640874]\n",
            "2767 [D loss: 0.435189, acc.: 78.12%] [G loss: 1.438926]\n",
            "2768 [D loss: 0.476995, acc.: 76.56%] [G loss: 1.474749]\n",
            "2769 [D loss: 0.541834, acc.: 71.88%] [G loss: 1.461040]\n",
            "2770 [D loss: 0.524921, acc.: 71.88%] [G loss: 1.534052]\n",
            "2771 [D loss: 0.458191, acc.: 76.56%] [G loss: 1.342325]\n",
            "2772 [D loss: 0.469189, acc.: 76.56%] [G loss: 1.384249]\n",
            "2773 [D loss: 0.436350, acc.: 75.00%] [G loss: 1.885908]\n",
            "2774 [D loss: 0.458220, acc.: 71.88%] [G loss: 1.452618]\n",
            "2775 [D loss: 0.456947, acc.: 76.56%] [G loss: 1.917382]\n",
            "2776 [D loss: 0.451938, acc.: 75.00%] [G loss: 1.654082]\n",
            "2777 [D loss: 0.430351, acc.: 76.56%] [G loss: 1.578841]\n",
            "2778 [D loss: 0.427861, acc.: 76.56%] [G loss: 1.498122]\n",
            "2779 [D loss: 0.435259, acc.: 78.12%] [G loss: 1.618727]\n",
            "2780 [D loss: 0.473657, acc.: 76.56%] [G loss: 1.807001]\n",
            "2781 [D loss: 0.441995, acc.: 75.00%] [G loss: 1.591148]\n",
            "2782 [D loss: 0.428946, acc.: 76.56%] [G loss: 1.610476]\n",
            "2783 [D loss: 0.422044, acc.: 76.56%] [G loss: 1.453815]\n",
            "2784 [D loss: 0.439749, acc.: 75.00%] [G loss: 1.643834]\n",
            "2785 [D loss: 0.489867, acc.: 70.31%] [G loss: 1.506968]\n",
            "2786 [D loss: 0.462633, acc.: 78.12%] [G loss: 1.337363]\n",
            "2787 [D loss: 0.417928, acc.: 79.69%] [G loss: 1.568416]\n",
            "2788 [D loss: 0.477971, acc.: 76.56%] [G loss: 1.478395]\n",
            "2789 [D loss: 0.395743, acc.: 76.56%] [G loss: 1.710830]\n",
            "2790 [D loss: 0.446018, acc.: 76.56%] [G loss: 1.613499]\n",
            "2791 [D loss: 0.430999, acc.: 78.12%] [G loss: 1.652237]\n",
            "2792 [D loss: 0.436976, acc.: 73.44%] [G loss: 1.642038]\n",
            "2793 [D loss: 0.483242, acc.: 73.44%] [G loss: 1.454752]\n",
            "2794 [D loss: 0.473178, acc.: 70.31%] [G loss: 1.705242]\n",
            "2795 [D loss: 0.431015, acc.: 76.56%] [G loss: 1.746202]\n",
            "2796 [D loss: 0.445736, acc.: 76.56%] [G loss: 1.583453]\n",
            "2797 [D loss: 0.402037, acc.: 79.69%] [G loss: 1.691033]\n",
            "2798 [D loss: 0.512748, acc.: 73.44%] [G loss: 1.470368]\n",
            "2799 [D loss: 0.463880, acc.: 76.56%] [G loss: 1.759621]\n",
            "2800 [D loss: 0.518621, acc.: 73.44%] [G loss: 1.633960]\n",
            "generated_data\n",
            "2801 [D loss: 0.477536, acc.: 75.00%] [G loss: 1.682248]\n",
            "2802 [D loss: 0.466270, acc.: 75.00%] [G loss: 1.583029]\n",
            "2803 [D loss: 0.495140, acc.: 71.88%] [G loss: 1.684744]\n",
            "2804 [D loss: 0.404220, acc.: 78.12%] [G loss: 1.532080]\n",
            "2805 [D loss: 0.465321, acc.: 75.00%] [G loss: 1.673959]\n",
            "2806 [D loss: 0.466322, acc.: 75.00%] [G loss: 1.883568]\n",
            "2807 [D loss: 0.443865, acc.: 75.00%] [G loss: 1.240754]\n",
            "2808 [D loss: 0.429366, acc.: 78.12%] [G loss: 1.856873]\n",
            "2809 [D loss: 0.404541, acc.: 78.12%] [G loss: 1.750966]\n",
            "2810 [D loss: 0.393085, acc.: 76.56%] [G loss: 1.661011]\n",
            "2811 [D loss: 0.414767, acc.: 76.56%] [G loss: 1.506744]\n",
            "2812 [D loss: 0.480575, acc.: 73.44%] [G loss: 1.475656]\n",
            "2813 [D loss: 0.509261, acc.: 75.00%] [G loss: 1.999032]\n",
            "2814 [D loss: 0.447536, acc.: 78.12%] [G loss: 1.520495]\n",
            "2815 [D loss: 0.458731, acc.: 71.88%] [G loss: 1.616962]\n",
            "2816 [D loss: 0.425243, acc.: 78.12%] [G loss: 1.612393]\n",
            "2817 [D loss: 0.471821, acc.: 75.00%] [G loss: 1.747751]\n",
            "2818 [D loss: 0.439107, acc.: 75.00%] [G loss: 1.559807]\n",
            "2819 [D loss: 0.482911, acc.: 73.44%] [G loss: 1.469671]\n",
            "2820 [D loss: 0.446758, acc.: 76.56%] [G loss: 1.633749]\n",
            "2821 [D loss: 0.426446, acc.: 78.12%] [G loss: 1.575068]\n",
            "2822 [D loss: 0.465943, acc.: 78.12%] [G loss: 1.614565]\n",
            "2823 [D loss: 0.446111, acc.: 71.88%] [G loss: 1.550754]\n",
            "2824 [D loss: 0.397610, acc.: 81.25%] [G loss: 1.494335]\n",
            "2825 [D loss: 0.437770, acc.: 76.56%] [G loss: 1.951722]\n",
            "2826 [D loss: 0.542675, acc.: 71.88%] [G loss: 1.509990]\n",
            "2827 [D loss: 0.561662, acc.: 73.44%] [G loss: 1.542390]\n",
            "2828 [D loss: 0.390967, acc.: 78.12%] [G loss: 1.703048]\n",
            "2829 [D loss: 0.500110, acc.: 75.00%] [G loss: 1.690671]\n",
            "2830 [D loss: 0.515128, acc.: 73.44%] [G loss: 1.357518]\n",
            "2831 [D loss: 0.449826, acc.: 76.56%] [G loss: 1.476699]\n",
            "2832 [D loss: 0.522756, acc.: 75.00%] [G loss: 1.494683]\n",
            "2833 [D loss: 0.438323, acc.: 78.12%] [G loss: 1.159635]\n",
            "2834 [D loss: 0.500564, acc.: 68.75%] [G loss: 1.426944]\n",
            "2835 [D loss: 0.484947, acc.: 75.00%] [G loss: 1.757456]\n",
            "2836 [D loss: 0.537511, acc.: 71.88%] [G loss: 1.417478]\n",
            "2837 [D loss: 0.449918, acc.: 76.56%] [G loss: 1.367658]\n",
            "2838 [D loss: 0.435938, acc.: 79.69%] [G loss: 1.475086]\n",
            "2839 [D loss: 0.408094, acc.: 76.56%] [G loss: 1.311778]\n",
            "2840 [D loss: 0.448615, acc.: 79.69%] [G loss: 1.772481]\n",
            "2841 [D loss: 0.416043, acc.: 79.69%] [G loss: 1.253231]\n",
            "2842 [D loss: 0.438952, acc.: 78.12%] [G loss: 1.249659]\n",
            "2843 [D loss: 0.434684, acc.: 76.56%] [G loss: 1.386847]\n",
            "2844 [D loss: 0.460681, acc.: 75.00%] [G loss: 1.580715]\n",
            "2845 [D loss: 0.473722, acc.: 76.56%] [G loss: 1.465603]\n",
            "2846 [D loss: 0.463659, acc.: 73.44%] [G loss: 1.325869]\n",
            "2847 [D loss: 0.487854, acc.: 79.69%] [G loss: 1.178575]\n",
            "2848 [D loss: 0.445995, acc.: 78.12%] [G loss: 1.411474]\n",
            "2849 [D loss: 0.439915, acc.: 75.00%] [G loss: 1.260720]\n",
            "2850 [D loss: 0.594856, acc.: 71.88%] [G loss: 1.346200]\n",
            "2851 [D loss: 0.495410, acc.: 71.88%] [G loss: 1.663956]\n",
            "2852 [D loss: 0.469675, acc.: 73.44%] [G loss: 1.424507]\n",
            "2853 [D loss: 0.404509, acc.: 79.69%] [G loss: 1.687608]\n",
            "2854 [D loss: 0.423443, acc.: 73.44%] [G loss: 1.486644]\n",
            "2855 [D loss: 0.428825, acc.: 76.56%] [G loss: 1.457031]\n",
            "2856 [D loss: 0.431185, acc.: 76.56%] [G loss: 1.543687]\n",
            "2857 [D loss: 0.401771, acc.: 78.12%] [G loss: 1.720208]\n",
            "2858 [D loss: 0.451972, acc.: 76.56%] [G loss: 1.722338]\n",
            "2859 [D loss: 0.465989, acc.: 75.00%] [G loss: 1.486886]\n",
            "2860 [D loss: 0.511976, acc.: 75.00%] [G loss: 1.228804]\n",
            "2861 [D loss: 0.416870, acc.: 78.12%] [G loss: 1.427367]\n",
            "2862 [D loss: 0.558106, acc.: 75.00%] [G loss: 1.182159]\n",
            "2863 [D loss: 0.524321, acc.: 75.00%] [G loss: 1.296121]\n",
            "2864 [D loss: 0.543559, acc.: 71.88%] [G loss: 1.329451]\n",
            "2865 [D loss: 0.444763, acc.: 75.00%] [G loss: 1.225507]\n",
            "2866 [D loss: 0.418093, acc.: 75.00%] [G loss: 1.412553]\n",
            "2867 [D loss: 0.484782, acc.: 76.56%] [G loss: 1.263920]\n",
            "2868 [D loss: 0.432943, acc.: 75.00%] [G loss: 1.499236]\n",
            "2869 [D loss: 0.393793, acc.: 79.69%] [G loss: 1.252732]\n",
            "2870 [D loss: 0.454972, acc.: 73.44%] [G loss: 1.343370]\n",
            "2871 [D loss: 0.433643, acc.: 76.56%] [G loss: 1.267853]\n",
            "2872 [D loss: 0.474345, acc.: 75.00%] [G loss: 1.469107]\n",
            "2873 [D loss: 0.457503, acc.: 71.88%] [G loss: 1.196902]\n",
            "2874 [D loss: 0.389059, acc.: 81.25%] [G loss: 1.225588]\n",
            "2875 [D loss: 0.519512, acc.: 75.00%] [G loss: 1.283259]\n",
            "2876 [D loss: 0.473678, acc.: 73.44%] [G loss: 1.293125]\n",
            "2877 [D loss: 0.436121, acc.: 76.56%] [G loss: 1.512785]\n",
            "2878 [D loss: 0.462167, acc.: 79.69%] [G loss: 1.407631]\n",
            "2879 [D loss: 0.451474, acc.: 78.12%] [G loss: 1.716050]\n",
            "2880 [D loss: 0.497965, acc.: 73.44%] [G loss: 1.614487]\n",
            "2881 [D loss: 0.495692, acc.: 71.88%] [G loss: 1.441760]\n",
            "2882 [D loss: 0.472879, acc.: 71.88%] [G loss: 1.489300]\n",
            "2883 [D loss: 0.497001, acc.: 78.12%] [G loss: 1.454742]\n",
            "2884 [D loss: 0.381008, acc.: 79.69%] [G loss: 1.478188]\n",
            "2885 [D loss: 0.427855, acc.: 79.69%] [G loss: 1.317767]\n",
            "2886 [D loss: 0.434774, acc.: 76.56%] [G loss: 1.189960]\n",
            "2887 [D loss: 0.442109, acc.: 76.56%] [G loss: 1.472415]\n",
            "2888 [D loss: 0.475895, acc.: 71.88%] [G loss: 1.403092]\n",
            "2889 [D loss: 0.458637, acc.: 75.00%] [G loss: 1.279235]\n",
            "2890 [D loss: 0.464369, acc.: 73.44%] [G loss: 1.417425]\n",
            "2891 [D loss: 0.448061, acc.: 76.56%] [G loss: 1.237149]\n",
            "2892 [D loss: 0.506987, acc.: 75.00%] [G loss: 1.189275]\n",
            "2893 [D loss: 0.484771, acc.: 73.44%] [G loss: 1.427208]\n",
            "2894 [D loss: 0.415343, acc.: 76.56%] [G loss: 1.731251]\n",
            "2895 [D loss: 0.449620, acc.: 73.44%] [G loss: 1.377979]\n",
            "2896 [D loss: 0.452536, acc.: 75.00%] [G loss: 1.273992]\n",
            "2897 [D loss: 0.434033, acc.: 76.56%] [G loss: 1.522287]\n",
            "2898 [D loss: 0.437971, acc.: 71.88%] [G loss: 1.649150]\n",
            "2899 [D loss: 0.423179, acc.: 79.69%] [G loss: 1.380431]\n",
            "2900 [D loss: 0.430400, acc.: 81.25%] [G loss: 1.445182]\n",
            "generated_data\n",
            "2901 [D loss: 0.439850, acc.: 81.25%] [G loss: 1.512105]\n",
            "2902 [D loss: 0.435655, acc.: 78.12%] [G loss: 1.493596]\n",
            "2903 [D loss: 0.449218, acc.: 76.56%] [G loss: 1.281645]\n",
            "2904 [D loss: 0.491500, acc.: 71.88%] [G loss: 1.653627]\n",
            "2905 [D loss: 0.427654, acc.: 76.56%] [G loss: 1.628475]\n",
            "2906 [D loss: 0.438435, acc.: 76.56%] [G loss: 1.424973]\n",
            "2907 [D loss: 0.497145, acc.: 76.56%] [G loss: 1.613058]\n",
            "2908 [D loss: 0.447139, acc.: 79.69%] [G loss: 1.603295]\n",
            "2909 [D loss: 0.448112, acc.: 78.12%] [G loss: 1.665823]\n",
            "2910 [D loss: 0.467028, acc.: 76.56%] [G loss: 1.329077]\n",
            "2911 [D loss: 0.413875, acc.: 76.56%] [G loss: 1.665146]\n",
            "2912 [D loss: 0.433149, acc.: 76.56%] [G loss: 1.549051]\n",
            "2913 [D loss: 0.400245, acc.: 79.69%] [G loss: 1.727756]\n",
            "2914 [D loss: 0.444614, acc.: 75.00%] [G loss: 1.418150]\n",
            "2915 [D loss: 0.472294, acc.: 70.31%] [G loss: 1.257745]\n",
            "2916 [D loss: 0.437475, acc.: 71.88%] [G loss: 1.736420]\n",
            "2917 [D loss: 0.399022, acc.: 81.25%] [G loss: 1.954911]\n",
            "2918 [D loss: 0.530885, acc.: 78.12%] [G loss: 1.605365]\n",
            "2919 [D loss: 0.419053, acc.: 78.12%] [G loss: 1.759302]\n",
            "2920 [D loss: 0.424095, acc.: 76.56%] [G loss: 1.655841]\n",
            "2921 [D loss: 0.479661, acc.: 78.12%] [G loss: 1.257107]\n",
            "2922 [D loss: 0.443073, acc.: 78.12%] [G loss: 1.359054]\n",
            "2923 [D loss: 0.416488, acc.: 81.25%] [G loss: 1.472681]\n",
            "2924 [D loss: 0.523775, acc.: 78.12%] [G loss: 1.265017]\n",
            "2925 [D loss: 0.441508, acc.: 79.69%] [G loss: 1.262035]\n",
            "2926 [D loss: 0.464200, acc.: 70.31%] [G loss: 1.406461]\n",
            "2927 [D loss: 0.487287, acc.: 73.44%] [G loss: 1.199308]\n",
            "2928 [D loss: 0.436493, acc.: 75.00%] [G loss: 1.279382]\n",
            "2929 [D loss: 0.408490, acc.: 78.12%] [G loss: 1.606806]\n",
            "2930 [D loss: 0.465122, acc.: 76.56%] [G loss: 1.661985]\n",
            "2931 [D loss: 0.442346, acc.: 76.56%] [G loss: 1.433779]\n",
            "2932 [D loss: 0.401567, acc.: 76.56%] [G loss: 1.920734]\n",
            "2933 [D loss: 0.440477, acc.: 76.56%] [G loss: 1.755750]\n",
            "2934 [D loss: 0.400671, acc.: 81.25%] [G loss: 1.744796]\n",
            "2935 [D loss: 0.483968, acc.: 76.56%] [G loss: 1.319065]\n",
            "2936 [D loss: 0.410230, acc.: 78.12%] [G loss: 1.747949]\n",
            "2937 [D loss: 0.439447, acc.: 73.44%] [G loss: 1.530045]\n",
            "2938 [D loss: 0.403244, acc.: 76.56%] [G loss: 1.570311]\n",
            "2939 [D loss: 0.386411, acc.: 79.69%] [G loss: 1.770432]\n",
            "2940 [D loss: 0.472032, acc.: 79.69%] [G loss: 1.481347]\n",
            "2941 [D loss: 0.365957, acc.: 81.25%] [G loss: 1.844767]\n",
            "2942 [D loss: 0.412589, acc.: 81.25%] [G loss: 1.793239]\n",
            "2943 [D loss: 0.402946, acc.: 81.25%] [G loss: 1.926166]\n",
            "2944 [D loss: 0.499346, acc.: 78.12%] [G loss: 1.730605]\n",
            "2945 [D loss: 0.395366, acc.: 79.69%] [G loss: 1.488000]\n",
            "2946 [D loss: 0.447809, acc.: 78.12%] [G loss: 1.639454]\n",
            "2947 [D loss: 0.426472, acc.: 76.56%] [G loss: 1.853033]\n",
            "2948 [D loss: 0.469687, acc.: 73.44%] [G loss: 1.480114]\n",
            "2949 [D loss: 0.462103, acc.: 75.00%] [G loss: 1.211524]\n",
            "2950 [D loss: 0.435365, acc.: 78.12%] [G loss: 1.322917]\n",
            "2951 [D loss: 0.433685, acc.: 71.88%] [G loss: 1.238473]\n",
            "2952 [D loss: 0.398373, acc.: 78.12%] [G loss: 1.230258]\n",
            "2953 [D loss: 0.490904, acc.: 76.56%] [G loss: 1.345299]\n",
            "2954 [D loss: 0.408998, acc.: 75.00%] [G loss: 1.582695]\n",
            "2955 [D loss: 0.418011, acc.: 76.56%] [G loss: 1.422667]\n",
            "2956 [D loss: 0.495225, acc.: 70.31%] [G loss: 1.456074]\n",
            "2957 [D loss: 0.477501, acc.: 70.31%] [G loss: 1.507875]\n",
            "2958 [D loss: 0.456464, acc.: 78.12%] [G loss: 1.431624]\n",
            "2959 [D loss: 0.448909, acc.: 75.00%] [G loss: 1.652519]\n",
            "2960 [D loss: 0.448569, acc.: 76.56%] [G loss: 1.367733]\n",
            "2961 [D loss: 0.445984, acc.: 75.00%] [G loss: 1.293889]\n",
            "2962 [D loss: 0.471113, acc.: 76.56%] [G loss: 1.601881]\n",
            "2963 [D loss: 0.450299, acc.: 78.12%] [G loss: 1.622329]\n",
            "2964 [D loss: 0.432864, acc.: 78.12%] [G loss: 1.722929]\n",
            "2965 [D loss: 0.447995, acc.: 75.00%] [G loss: 1.317684]\n",
            "2966 [D loss: 0.439943, acc.: 75.00%] [G loss: 2.481831]\n",
            "2967 [D loss: 0.444156, acc.: 75.00%] [G loss: 1.687022]\n",
            "2968 [D loss: 0.478037, acc.: 76.56%] [G loss: 1.561355]\n",
            "2969 [D loss: 0.499026, acc.: 75.00%] [G loss: 1.746500]\n",
            "2970 [D loss: 0.433050, acc.: 75.00%] [G loss: 1.653723]\n",
            "2971 [D loss: 0.408149, acc.: 78.12%] [G loss: 1.524948]\n",
            "2972 [D loss: 0.472006, acc.: 71.88%] [G loss: 1.472383]\n",
            "2973 [D loss: 0.485524, acc.: 79.69%] [G loss: 1.700407]\n",
            "2974 [D loss: 0.429613, acc.: 76.56%] [G loss: 1.688085]\n",
            "2975 [D loss: 0.492920, acc.: 73.44%] [G loss: 1.213123]\n",
            "2976 [D loss: 0.455216, acc.: 76.56%] [G loss: 1.562979]\n",
            "2977 [D loss: 0.407014, acc.: 78.12%] [G loss: 1.451967]\n",
            "2978 [D loss: 0.454443, acc.: 78.12%] [G loss: 1.362313]\n",
            "2979 [D loss: 0.506572, acc.: 75.00%] [G loss: 1.394776]\n",
            "2980 [D loss: 0.465359, acc.: 75.00%] [G loss: 1.518384]\n",
            "2981 [D loss: 0.402541, acc.: 79.69%] [G loss: 1.888740]\n",
            "2982 [D loss: 0.383348, acc.: 78.12%] [G loss: 2.027191]\n",
            "2983 [D loss: 0.520624, acc.: 75.00%] [G loss: 1.236278]\n",
            "2984 [D loss: 0.432637, acc.: 71.88%] [G loss: 1.839982]\n",
            "2985 [D loss: 0.395005, acc.: 76.56%] [G loss: 1.503598]\n",
            "2986 [D loss: 0.456267, acc.: 78.12%] [G loss: 1.705837]\n",
            "2987 [D loss: 0.501891, acc.: 75.00%] [G loss: 1.533805]\n",
            "2988 [D loss: 0.462561, acc.: 78.12%] [G loss: 1.138588]\n",
            "2989 [D loss: 0.430016, acc.: 75.00%] [G loss: 1.966934]\n",
            "2990 [D loss: 0.457002, acc.: 75.00%] [G loss: 1.302213]\n",
            "2991 [D loss: 0.413406, acc.: 78.12%] [G loss: 1.923350]\n",
            "2992 [D loss: 0.380939, acc.: 78.12%] [G loss: 2.073982]\n",
            "2993 [D loss: 0.510869, acc.: 73.44%] [G loss: 1.654295]\n",
            "2994 [D loss: 0.428830, acc.: 78.12%] [G loss: 1.446174]\n",
            "2995 [D loss: 0.432727, acc.: 78.12%] [G loss: 1.386472]\n",
            "2996 [D loss: 0.458644, acc.: 75.00%] [G loss: 1.626112]\n",
            "2997 [D loss: 0.467368, acc.: 76.56%] [G loss: 1.244357]\n",
            "2998 [D loss: 0.415646, acc.: 79.69%] [G loss: 1.260138]\n",
            "2999 [D loss: 0.452818, acc.: 78.12%] [G loss: 1.486873]\n",
            "3000 [D loss: 0.435387, acc.: 75.00%] [G loss: 1.283461]\n",
            "generated_data\n",
            "3001 [D loss: 0.425860, acc.: 75.00%] [G loss: 1.536394]\n",
            "3002 [D loss: 0.499272, acc.: 75.00%] [G loss: 1.527229]\n",
            "3003 [D loss: 0.424673, acc.: 78.12%] [G loss: 1.962512]\n",
            "3004 [D loss: 0.399142, acc.: 76.56%] [G loss: 1.765843]\n",
            "3005 [D loss: 0.410948, acc.: 79.69%] [G loss: 1.600889]\n",
            "3006 [D loss: 0.404342, acc.: 75.00%] [G loss: 1.344125]\n",
            "3007 [D loss: 0.413280, acc.: 78.12%] [G loss: 1.483168]\n",
            "3008 [D loss: 0.418207, acc.: 75.00%] [G loss: 1.712253]\n",
            "3009 [D loss: 0.458626, acc.: 75.00%] [G loss: 1.660569]\n",
            "3010 [D loss: 0.479875, acc.: 75.00%] [G loss: 1.421824]\n",
            "3011 [D loss: 0.436489, acc.: 75.00%] [G loss: 1.711214]\n",
            "3012 [D loss: 0.420541, acc.: 78.12%] [G loss: 1.517456]\n",
            "3013 [D loss: 0.449036, acc.: 79.69%] [G loss: 1.184280]\n",
            "3014 [D loss: 0.418317, acc.: 75.00%] [G loss: 1.247103]\n",
            "3015 [D loss: 0.463227, acc.: 73.44%] [G loss: 1.165591]\n",
            "3016 [D loss: 0.415182, acc.: 78.12%] [G loss: 1.400025]\n",
            "3017 [D loss: 0.457259, acc.: 75.00%] [G loss: 1.623679]\n",
            "3018 [D loss: 0.486377, acc.: 71.88%] [G loss: 1.402724]\n",
            "3019 [D loss: 0.462602, acc.: 75.00%] [G loss: 1.648275]\n",
            "3020 [D loss: 0.404876, acc.: 78.12%] [G loss: 1.415969]\n",
            "3021 [D loss: 0.441876, acc.: 76.56%] [G loss: 1.504232]\n",
            "3022 [D loss: 0.388997, acc.: 79.69%] [G loss: 1.819250]\n",
            "3023 [D loss: 0.441370, acc.: 75.00%] [G loss: 1.641276]\n",
            "3024 [D loss: 0.452835, acc.: 79.69%] [G loss: 1.452415]\n",
            "3025 [D loss: 0.477371, acc.: 75.00%] [G loss: 1.757929]\n",
            "3026 [D loss: 0.453577, acc.: 79.69%] [G loss: 1.430738]\n",
            "3027 [D loss: 0.413064, acc.: 81.25%] [G loss: 1.800769]\n",
            "3028 [D loss: 0.498679, acc.: 73.44%] [G loss: 1.708498]\n",
            "3029 [D loss: 0.413755, acc.: 78.12%] [G loss: 1.595318]\n",
            "3030 [D loss: 0.470535, acc.: 76.56%] [G loss: 1.492575]\n",
            "3031 [D loss: 0.542558, acc.: 70.31%] [G loss: 1.363720]\n",
            "3032 [D loss: 0.531005, acc.: 68.75%] [G loss: 1.694971]\n",
            "3033 [D loss: 0.445533, acc.: 79.69%] [G loss: 1.474325]\n",
            "3034 [D loss: 0.423692, acc.: 78.12%] [G loss: 1.448390]\n",
            "3035 [D loss: 0.469564, acc.: 76.56%] [G loss: 1.450492]\n",
            "3036 [D loss: 0.443951, acc.: 76.56%] [G loss: 1.455569]\n",
            "3037 [D loss: 0.430301, acc.: 75.00%] [G loss: 1.634057]\n",
            "3038 [D loss: 0.420304, acc.: 75.00%] [G loss: 1.503810]\n",
            "3039 [D loss: 0.501058, acc.: 73.44%] [G loss: 1.446765]\n",
            "3040 [D loss: 0.429664, acc.: 76.56%] [G loss: 1.521117]\n",
            "3041 [D loss: 0.438132, acc.: 76.56%] [G loss: 1.493327]\n",
            "3042 [D loss: 0.473257, acc.: 76.56%] [G loss: 1.638790]\n",
            "3043 [D loss: 0.514905, acc.: 75.00%] [G loss: 1.472090]\n",
            "3044 [D loss: 0.425954, acc.: 79.69%] [G loss: 1.635780]\n",
            "3045 [D loss: 0.522590, acc.: 71.88%] [G loss: 1.668128]\n",
            "3046 [D loss: 0.425178, acc.: 73.44%] [G loss: 1.730442]\n",
            "3047 [D loss: 0.418377, acc.: 78.12%] [G loss: 2.077324]\n",
            "3048 [D loss: 0.453016, acc.: 75.00%] [G loss: 1.635507]\n",
            "3049 [D loss: 0.419842, acc.: 76.56%] [G loss: 2.059439]\n",
            "3050 [D loss: 0.530507, acc.: 71.88%] [G loss: 1.196492]\n",
            "3051 [D loss: 0.422453, acc.: 76.56%] [G loss: 1.648209]\n",
            "3052 [D loss: 0.486145, acc.: 76.56%] [G loss: 1.620075]\n",
            "3053 [D loss: 0.428699, acc.: 78.12%] [G loss: 1.637068]\n",
            "3054 [D loss: 0.400381, acc.: 79.69%] [G loss: 1.889705]\n",
            "3055 [D loss: 0.484343, acc.: 73.44%] [G loss: 1.317478]\n",
            "3056 [D loss: 0.459163, acc.: 75.00%] [G loss: 1.903749]\n",
            "3057 [D loss: 0.417365, acc.: 75.00%] [G loss: 1.544890]\n",
            "3058 [D loss: 0.485739, acc.: 76.56%] [G loss: 1.477877]\n",
            "3059 [D loss: 0.435219, acc.: 76.56%] [G loss: 1.496947]\n",
            "3060 [D loss: 0.458459, acc.: 78.12%] [G loss: 1.354625]\n",
            "3061 [D loss: 0.502022, acc.: 71.88%] [G loss: 1.328829]\n",
            "3062 [D loss: 0.422466, acc.: 76.56%] [G loss: 1.605244]\n",
            "3063 [D loss: 0.421239, acc.: 75.00%] [G loss: 1.383328]\n",
            "3064 [D loss: 0.443928, acc.: 79.69%] [G loss: 1.752843]\n",
            "3065 [D loss: 0.406862, acc.: 78.12%] [G loss: 1.752071]\n",
            "3066 [D loss: 0.432792, acc.: 78.12%] [G loss: 1.429311]\n",
            "3067 [D loss: 0.402906, acc.: 78.12%] [G loss: 1.559477]\n",
            "3068 [D loss: 0.431062, acc.: 78.12%] [G loss: 1.167212]\n",
            "3069 [D loss: 0.457862, acc.: 70.31%] [G loss: 1.318784]\n",
            "3070 [D loss: 0.438201, acc.: 76.56%] [G loss: 1.404185]\n",
            "3071 [D loss: 0.448317, acc.: 78.12%] [G loss: 1.539465]\n",
            "3072 [D loss: 0.440894, acc.: 75.00%] [G loss: 1.561159]\n",
            "3073 [D loss: 0.413480, acc.: 76.56%] [G loss: 1.696797]\n",
            "3074 [D loss: 0.408072, acc.: 78.12%] [G loss: 1.251838]\n",
            "3075 [D loss: 0.422305, acc.: 79.69%] [G loss: 1.353264]\n",
            "3076 [D loss: 0.389044, acc.: 76.56%] [G loss: 1.635960]\n",
            "3077 [D loss: 0.418816, acc.: 76.56%] [G loss: 1.400495]\n",
            "3078 [D loss: 0.447538, acc.: 78.12%] [G loss: 1.381646]\n",
            "3079 [D loss: 0.399332, acc.: 73.44%] [G loss: 1.753327]\n",
            "3080 [D loss: 0.440221, acc.: 76.56%] [G loss: 1.406293]\n",
            "3081 [D loss: 0.450011, acc.: 75.00%] [G loss: 1.463999]\n",
            "3082 [D loss: 0.380246, acc.: 78.12%] [G loss: 1.510114]\n",
            "3083 [D loss: 0.478849, acc.: 71.88%] [G loss: 1.409282]\n",
            "3084 [D loss: 0.445827, acc.: 75.00%] [G loss: 1.838412]\n",
            "3085 [D loss: 0.455335, acc.: 78.12%] [G loss: 1.532711]\n",
            "3086 [D loss: 0.382860, acc.: 78.12%] [G loss: 1.439595]\n",
            "3087 [D loss: 0.478911, acc.: 73.44%] [G loss: 1.461045]\n",
            "3088 [D loss: 0.474944, acc.: 73.44%] [G loss: 1.766065]\n",
            "3089 [D loss: 0.430302, acc.: 76.56%] [G loss: 1.477910]\n",
            "3090 [D loss: 0.407910, acc.: 78.12%] [G loss: 1.663878]\n",
            "3091 [D loss: 0.459461, acc.: 75.00%] [G loss: 1.567800]\n",
            "3092 [D loss: 0.390632, acc.: 76.56%] [G loss: 1.397546]\n",
            "3093 [D loss: 0.444330, acc.: 75.00%] [G loss: 1.402796]\n",
            "3094 [D loss: 0.450498, acc.: 75.00%] [G loss: 1.289952]\n",
            "3095 [D loss: 0.450219, acc.: 73.44%] [G loss: 1.683901]\n",
            "3096 [D loss: 0.445301, acc.: 78.12%] [G loss: 1.335538]\n",
            "3097 [D loss: 0.410935, acc.: 76.56%] [G loss: 1.328248]\n",
            "3098 [D loss: 0.424922, acc.: 78.12%] [G loss: 1.673915]\n",
            "3099 [D loss: 0.504975, acc.: 73.44%] [G loss: 1.539625]\n",
            "3100 [D loss: 0.469037, acc.: 75.00%] [G loss: 1.639708]\n",
            "generated_data\n",
            "3101 [D loss: 0.417197, acc.: 76.56%] [G loss: 1.486174]\n",
            "3102 [D loss: 0.496223, acc.: 75.00%] [G loss: 1.330382]\n",
            "3103 [D loss: 0.402451, acc.: 76.56%] [G loss: 1.490827]\n",
            "3104 [D loss: 0.487991, acc.: 75.00%] [G loss: 1.326669]\n",
            "3105 [D loss: 0.506141, acc.: 67.19%] [G loss: 1.425116]\n",
            "3106 [D loss: 0.452846, acc.: 76.56%] [G loss: 1.578129]\n",
            "3107 [D loss: 0.481211, acc.: 75.00%] [G loss: 1.282831]\n",
            "3108 [D loss: 0.474104, acc.: 76.56%] [G loss: 1.435797]\n",
            "3109 [D loss: 0.443651, acc.: 78.12%] [G loss: 1.442009]\n",
            "3110 [D loss: 0.454099, acc.: 73.44%] [G loss: 1.510681]\n",
            "3111 [D loss: 0.457829, acc.: 71.88%] [G loss: 1.583260]\n",
            "3112 [D loss: 0.514535, acc.: 76.56%] [G loss: 1.503543]\n",
            "3113 [D loss: 0.448764, acc.: 71.88%] [G loss: 1.285581]\n",
            "3114 [D loss: 0.438828, acc.: 76.56%] [G loss: 1.298020]\n",
            "3115 [D loss: 0.432367, acc.: 76.56%] [G loss: 1.376177]\n",
            "3116 [D loss: 0.441862, acc.: 76.56%] [G loss: 1.367134]\n",
            "3117 [D loss: 0.470869, acc.: 75.00%] [G loss: 1.219259]\n",
            "3118 [D loss: 0.436489, acc.: 78.12%] [G loss: 1.649277]\n",
            "3119 [D loss: 0.554490, acc.: 75.00%] [G loss: 1.477446]\n",
            "3120 [D loss: 0.461295, acc.: 75.00%] [G loss: 1.180178]\n",
            "3121 [D loss: 0.483356, acc.: 71.88%] [G loss: 1.432735]\n",
            "3122 [D loss: 0.419920, acc.: 78.12%] [G loss: 1.730661]\n",
            "3123 [D loss: 0.511736, acc.: 71.88%] [G loss: 1.312010]\n",
            "3124 [D loss: 0.389172, acc.: 79.69%] [G loss: 1.484934]\n",
            "3125 [D loss: 0.425004, acc.: 76.56%] [G loss: 1.658273]\n",
            "3126 [D loss: 0.473384, acc.: 78.12%] [G loss: 1.541964]\n",
            "3127 [D loss: 0.428393, acc.: 75.00%] [G loss: 2.099501]\n",
            "3128 [D loss: 0.491176, acc.: 73.44%] [G loss: 1.564302]\n",
            "3129 [D loss: 0.482149, acc.: 76.56%] [G loss: 1.482071]\n",
            "3130 [D loss: 0.430433, acc.: 76.56%] [G loss: 1.484215]\n",
            "3131 [D loss: 0.396288, acc.: 78.12%] [G loss: 1.702967]\n",
            "3132 [D loss: 0.416025, acc.: 76.56%] [G loss: 1.394638]\n",
            "3133 [D loss: 0.393973, acc.: 79.69%] [G loss: 1.792454]\n",
            "3134 [D loss: 0.441860, acc.: 78.12%] [G loss: 1.211901]\n",
            "3135 [D loss: 0.478939, acc.: 73.44%] [G loss: 1.248786]\n",
            "3136 [D loss: 0.466758, acc.: 79.69%] [G loss: 1.381742]\n",
            "3137 [D loss: 0.474456, acc.: 78.12%] [G loss: 1.481605]\n",
            "3138 [D loss: 0.453541, acc.: 76.56%] [G loss: 1.493199]\n",
            "3139 [D loss: 0.415215, acc.: 78.12%] [G loss: 1.438560]\n",
            "3140 [D loss: 0.408745, acc.: 81.25%] [G loss: 1.949279]\n",
            "3141 [D loss: 0.463698, acc.: 75.00%] [G loss: 1.506719]\n",
            "3142 [D loss: 0.411032, acc.: 81.25%] [G loss: 1.939252]\n",
            "3143 [D loss: 0.480790, acc.: 70.31%] [G loss: 1.569639]\n",
            "3144 [D loss: 0.425874, acc.: 79.69%] [G loss: 1.887875]\n",
            "3145 [D loss: 0.429080, acc.: 79.69%] [G loss: 2.040668]\n",
            "3146 [D loss: 0.441312, acc.: 76.56%] [G loss: 1.549036]\n",
            "3147 [D loss: 0.406048, acc.: 76.56%] [G loss: 1.599685]\n",
            "3148 [D loss: 0.387000, acc.: 76.56%] [G loss: 2.072508]\n",
            "3149 [D loss: 0.389755, acc.: 81.25%] [G loss: 1.599347]\n",
            "3150 [D loss: 0.453549, acc.: 76.56%] [G loss: 1.791636]\n",
            "3151 [D loss: 0.346339, acc.: 81.25%] [G loss: 2.196018]\n",
            "3152 [D loss: 0.494709, acc.: 73.44%] [G loss: 1.515699]\n",
            "3153 [D loss: 0.349847, acc.: 76.56%] [G loss: 2.285315]\n",
            "3154 [D loss: 0.444406, acc.: 78.12%] [G loss: 1.358425]\n",
            "3155 [D loss: 0.498716, acc.: 75.00%] [G loss: 1.563354]\n",
            "3156 [D loss: 0.359523, acc.: 82.81%] [G loss: 2.333448]\n",
            "3157 [D loss: 0.460999, acc.: 75.00%] [G loss: 1.204881]\n",
            "3158 [D loss: 0.387151, acc.: 78.12%] [G loss: 1.780990]\n",
            "3159 [D loss: 0.387224, acc.: 76.56%] [G loss: 1.627454]\n",
            "3160 [D loss: 0.461505, acc.: 73.44%] [G loss: 1.527399]\n",
            "3161 [D loss: 0.364011, acc.: 79.69%] [G loss: 2.156757]\n",
            "3162 [D loss: 0.460055, acc.: 70.31%] [G loss: 2.037794]\n",
            "3163 [D loss: 0.492281, acc.: 75.00%] [G loss: 1.419035]\n",
            "3164 [D loss: 0.394856, acc.: 78.12%] [G loss: 2.145887]\n",
            "3165 [D loss: 0.415088, acc.: 76.56%] [G loss: 1.644136]\n",
            "3166 [D loss: 0.465856, acc.: 75.00%] [G loss: 1.824877]\n",
            "3167 [D loss: 0.460134, acc.: 71.88%] [G loss: 2.331057]\n",
            "3168 [D loss: 0.523177, acc.: 78.12%] [G loss: 1.646993]\n",
            "3169 [D loss: 0.341899, acc.: 79.69%] [G loss: 2.017847]\n",
            "3170 [D loss: 0.434850, acc.: 75.00%] [G loss: 1.852909]\n",
            "3171 [D loss: 0.416930, acc.: 78.12%] [G loss: 1.955285]\n",
            "3172 [D loss: 0.361289, acc.: 75.00%] [G loss: 2.056069]\n",
            "3173 [D loss: 0.474003, acc.: 68.75%] [G loss: 1.540534]\n",
            "3174 [D loss: 0.415356, acc.: 78.12%] [G loss: 1.948304]\n",
            "3175 [D loss: 0.367001, acc.: 81.25%] [G loss: 1.739896]\n",
            "3176 [D loss: 0.471258, acc.: 78.12%] [G loss: 1.556068]\n",
            "3177 [D loss: 0.443915, acc.: 73.44%] [G loss: 1.942830]\n",
            "3178 [D loss: 0.457111, acc.: 76.56%] [G loss: 1.312014]\n",
            "3179 [D loss: 0.468938, acc.: 73.44%] [G loss: 1.577891]\n",
            "3180 [D loss: 0.399506, acc.: 78.12%] [G loss: 2.022267]\n",
            "3181 [D loss: 0.442202, acc.: 76.56%] [G loss: 1.414692]\n",
            "3182 [D loss: 0.392587, acc.: 76.56%] [G loss: 1.865774]\n",
            "3183 [D loss: 0.513060, acc.: 70.31%] [G loss: 1.854130]\n",
            "3184 [D loss: 0.487900, acc.: 71.88%] [G loss: 1.541053]\n",
            "3185 [D loss: 0.425793, acc.: 78.12%] [G loss: 1.574749]\n",
            "3186 [D loss: 0.380761, acc.: 79.69%] [G loss: 1.924578]\n",
            "3187 [D loss: 0.462253, acc.: 73.44%] [G loss: 1.730240]\n",
            "3188 [D loss: 0.360655, acc.: 76.56%] [G loss: 2.171726]\n",
            "3189 [D loss: 0.530438, acc.: 68.75%] [G loss: 1.473937]\n",
            "3190 [D loss: 0.445048, acc.: 73.44%] [G loss: 1.496365]\n",
            "3191 [D loss: 0.440698, acc.: 76.56%] [G loss: 1.711613]\n",
            "3192 [D loss: 0.487551, acc.: 78.12%] [G loss: 1.395525]\n",
            "3193 [D loss: 0.420217, acc.: 75.00%] [G loss: 1.484380]\n",
            "3194 [D loss: 0.430135, acc.: 71.88%] [G loss: 1.748297]\n",
            "3195 [D loss: 0.428191, acc.: 76.56%] [G loss: 1.468523]\n",
            "3196 [D loss: 0.418478, acc.: 78.12%] [G loss: 1.720322]\n",
            "3197 [D loss: 0.513009, acc.: 75.00%] [G loss: 1.585311]\n",
            "3198 [D loss: 0.489571, acc.: 75.00%] [G loss: 1.389213]\n",
            "3199 [D loss: 0.395036, acc.: 78.12%] [G loss: 1.496451]\n",
            "3200 [D loss: 0.433719, acc.: 79.69%] [G loss: 1.303866]\n",
            "generated_data\n",
            "3201 [D loss: 0.434620, acc.: 75.00%] [G loss: 1.702325]\n",
            "3202 [D loss: 0.479657, acc.: 73.44%] [G loss: 1.284985]\n",
            "3203 [D loss: 0.401985, acc.: 79.69%] [G loss: 1.692856]\n",
            "3204 [D loss: 0.408207, acc.: 81.25%] [G loss: 1.297581]\n",
            "3205 [D loss: 0.442991, acc.: 76.56%] [G loss: 1.367373]\n",
            "3206 [D loss: 0.403558, acc.: 76.56%] [G loss: 1.683196]\n",
            "3207 [D loss: 0.547205, acc.: 71.88%] [G loss: 1.495265]\n",
            "3208 [D loss: 0.426147, acc.: 78.12%] [G loss: 1.507437]\n",
            "3209 [D loss: 0.446627, acc.: 79.69%] [G loss: 1.590346]\n",
            "3210 [D loss: 0.483854, acc.: 76.56%] [G loss: 1.550999]\n",
            "3211 [D loss: 0.429347, acc.: 76.56%] [G loss: 1.513200]\n",
            "3212 [D loss: 0.429641, acc.: 78.12%] [G loss: 1.510644]\n",
            "3213 [D loss: 0.382402, acc.: 79.69%] [G loss: 1.655531]\n",
            "3214 [D loss: 0.441966, acc.: 78.12%] [G loss: 1.508802]\n",
            "3215 [D loss: 0.459025, acc.: 78.12%] [G loss: 1.351809]\n",
            "3216 [D loss: 0.459900, acc.: 76.56%] [G loss: 1.490161]\n",
            "3217 [D loss: 0.451284, acc.: 75.00%] [G loss: 1.229764]\n",
            "3218 [D loss: 0.414435, acc.: 76.56%] [G loss: 1.499449]\n",
            "3219 [D loss: 0.412943, acc.: 75.00%] [G loss: 1.420363]\n",
            "3220 [D loss: 0.426929, acc.: 75.00%] [G loss: 1.401566]\n",
            "3221 [D loss: 0.443795, acc.: 78.12%] [G loss: 1.459612]\n",
            "3222 [D loss: 0.551709, acc.: 71.88%] [G loss: 1.498495]\n",
            "3223 [D loss: 0.418955, acc.: 75.00%] [G loss: 1.800207]\n",
            "3224 [D loss: 0.373722, acc.: 78.12%] [G loss: 1.724181]\n",
            "3225 [D loss: 0.458241, acc.: 78.12%] [G loss: 1.560323]\n",
            "3226 [D loss: 0.383782, acc.: 82.81%] [G loss: 1.721865]\n",
            "3227 [D loss: 0.449974, acc.: 75.00%] [G loss: 1.521328]\n",
            "3228 [D loss: 0.365168, acc.: 78.12%] [G loss: 2.194996]\n",
            "3229 [D loss: 0.473007, acc.: 73.44%] [G loss: 1.366340]\n",
            "3230 [D loss: 0.449670, acc.: 76.56%] [G loss: 1.768686]\n",
            "3231 [D loss: 0.456182, acc.: 75.00%] [G loss: 1.435584]\n",
            "3232 [D loss: 0.404360, acc.: 78.12%] [G loss: 1.509760]\n",
            "3233 [D loss: 0.422451, acc.: 78.12%] [G loss: 1.529772]\n",
            "3234 [D loss: 0.380301, acc.: 76.56%] [G loss: 1.479351]\n",
            "3235 [D loss: 0.417804, acc.: 75.00%] [G loss: 1.443435]\n",
            "3236 [D loss: 0.474157, acc.: 75.00%] [G loss: 1.731624]\n",
            "3237 [D loss: 0.486489, acc.: 78.12%] [G loss: 1.858127]\n",
            "3238 [D loss: 0.487753, acc.: 76.56%] [G loss: 1.394101]\n",
            "3239 [D loss: 0.460236, acc.: 78.12%] [G loss: 1.476311]\n",
            "3240 [D loss: 0.372966, acc.: 78.12%] [G loss: 1.916791]\n",
            "3241 [D loss: 0.544912, acc.: 76.56%] [G loss: 1.923147]\n",
            "3242 [D loss: 0.501068, acc.: 78.12%] [G loss: 1.454345]\n",
            "3243 [D loss: 0.476098, acc.: 73.44%] [G loss: 1.585110]\n",
            "3244 [D loss: 0.446906, acc.: 76.56%] [G loss: 1.382894]\n",
            "3245 [D loss: 0.465947, acc.: 76.56%] [G loss: 1.473965]\n",
            "3246 [D loss: 0.486381, acc.: 73.44%] [G loss: 1.475170]\n",
            "3247 [D loss: 0.451723, acc.: 76.56%] [G loss: 1.272220]\n",
            "3248 [D loss: 0.440427, acc.: 76.56%] [G loss: 1.428510]\n",
            "3249 [D loss: 0.464723, acc.: 71.88%] [G loss: 1.418444]\n",
            "3250 [D loss: 0.465134, acc.: 75.00%] [G loss: 1.333584]\n",
            "3251 [D loss: 0.431308, acc.: 76.56%] [G loss: 1.527996]\n",
            "3252 [D loss: 0.445508, acc.: 79.69%] [G loss: 1.448226]\n",
            "3253 [D loss: 0.371059, acc.: 78.12%] [G loss: 1.902329]\n",
            "3254 [D loss: 0.425683, acc.: 78.12%] [G loss: 1.482969]\n",
            "3255 [D loss: 0.435022, acc.: 78.12%] [G loss: 1.530944]\n",
            "3256 [D loss: 0.416889, acc.: 79.69%] [G loss: 1.615215]\n",
            "3257 [D loss: 0.444286, acc.: 76.56%] [G loss: 1.414633]\n",
            "3258 [D loss: 0.397992, acc.: 76.56%] [G loss: 1.725781]\n",
            "3259 [D loss: 0.428434, acc.: 78.12%] [G loss: 1.203441]\n",
            "3260 [D loss: 0.450390, acc.: 78.12%] [G loss: 1.765693]\n",
            "3261 [D loss: 0.447279, acc.: 75.00%] [G loss: 1.827042]\n",
            "3262 [D loss: 0.492177, acc.: 76.56%] [G loss: 2.066506]\n",
            "3263 [D loss: 0.428788, acc.: 79.69%] [G loss: 1.567526]\n",
            "3264 [D loss: 0.456292, acc.: 75.00%] [G loss: 1.479520]\n",
            "3265 [D loss: 0.436101, acc.: 76.56%] [G loss: 1.690207]\n",
            "3266 [D loss: 0.488147, acc.: 73.44%] [G loss: 1.658544]\n",
            "3267 [D loss: 0.407855, acc.: 79.69%] [G loss: 1.686985]\n",
            "3268 [D loss: 0.405688, acc.: 76.56%] [G loss: 1.789152]\n",
            "3269 [D loss: 0.408111, acc.: 78.12%] [G loss: 1.529295]\n",
            "3270 [D loss: 0.411992, acc.: 82.81%] [G loss: 1.761408]\n",
            "3271 [D loss: 0.419028, acc.: 79.69%] [G loss: 1.550493]\n",
            "3272 [D loss: 0.467282, acc.: 76.56%] [G loss: 1.609482]\n",
            "3273 [D loss: 0.629473, acc.: 73.44%] [G loss: 1.719438]\n",
            "3274 [D loss: 0.493992, acc.: 70.31%] [G loss: 1.536389]\n",
            "3275 [D loss: 0.448468, acc.: 78.12%] [G loss: 1.605609]\n",
            "3276 [D loss: 0.435352, acc.: 81.25%] [G loss: 1.563481]\n",
            "3277 [D loss: 0.444462, acc.: 76.56%] [G loss: 1.481833]\n",
            "3278 [D loss: 0.376678, acc.: 78.12%] [G loss: 1.664314]\n",
            "3279 [D loss: 0.415540, acc.: 76.56%] [G loss: 1.581765]\n",
            "3280 [D loss: 0.413232, acc.: 75.00%] [G loss: 1.985730]\n",
            "3281 [D loss: 0.404405, acc.: 76.56%] [G loss: 2.175436]\n",
            "3282 [D loss: 0.460974, acc.: 76.56%] [G loss: 1.953125]\n",
            "3283 [D loss: 0.374199, acc.: 76.56%] [G loss: 1.955868]\n",
            "3284 [D loss: 0.516815, acc.: 75.00%] [G loss: 1.846380]\n",
            "3285 [D loss: 0.419676, acc.: 75.00%] [G loss: 1.456600]\n",
            "3286 [D loss: 0.395582, acc.: 78.12%] [G loss: 1.646124]\n",
            "3287 [D loss: 0.488215, acc.: 78.12%] [G loss: 1.456139]\n",
            "3288 [D loss: 0.353489, acc.: 78.12%] [G loss: 1.622991]\n",
            "3289 [D loss: 0.469786, acc.: 76.56%] [G loss: 1.454972]\n",
            "3290 [D loss: 0.429342, acc.: 76.56%] [G loss: 1.376918]\n",
            "3291 [D loss: 0.473809, acc.: 75.00%] [G loss: 1.152099]\n",
            "3292 [D loss: 0.494372, acc.: 70.31%] [G loss: 1.223446]\n",
            "3293 [D loss: 0.381446, acc.: 82.81%] [G loss: 1.561164]\n",
            "3294 [D loss: 0.476728, acc.: 75.00%] [G loss: 1.430370]\n",
            "3295 [D loss: 0.476712, acc.: 78.12%] [G loss: 1.527080]\n",
            "3296 [D loss: 0.436235, acc.: 78.12%] [G loss: 1.327146]\n",
            "3297 [D loss: 0.482282, acc.: 73.44%] [G loss: 1.096586]\n",
            "3298 [D loss: 0.408250, acc.: 81.25%] [G loss: 1.611032]\n",
            "3299 [D loss: 0.449102, acc.: 78.12%] [G loss: 1.754210]\n",
            "3300 [D loss: 0.404098, acc.: 78.12%] [G loss: 1.562463]\n",
            "generated_data\n",
            "3301 [D loss: 0.482556, acc.: 76.56%] [G loss: 1.552462]\n",
            "3302 [D loss: 0.361141, acc.: 78.12%] [G loss: 2.325305]\n",
            "3303 [D loss: 0.484931, acc.: 68.75%] [G loss: 1.423199]\n",
            "3304 [D loss: 0.372973, acc.: 79.69%] [G loss: 1.873631]\n",
            "3305 [D loss: 0.376429, acc.: 81.25%] [G loss: 2.015075]\n",
            "3306 [D loss: 0.440300, acc.: 75.00%] [G loss: 2.170684]\n",
            "3307 [D loss: 0.413626, acc.: 75.00%] [G loss: 1.983768]\n",
            "3308 [D loss: 0.429876, acc.: 76.56%] [G loss: 1.373030]\n",
            "3309 [D loss: 0.382619, acc.: 79.69%] [G loss: 2.121087]\n",
            "3310 [D loss: 0.431237, acc.: 78.12%] [G loss: 1.809088]\n",
            "3311 [D loss: 0.484527, acc.: 75.00%] [G loss: 1.865042]\n",
            "3312 [D loss: 0.401055, acc.: 73.44%] [G loss: 1.463613]\n",
            "3313 [D loss: 0.431897, acc.: 79.69%] [G loss: 1.625975]\n",
            "3314 [D loss: 0.413159, acc.: 76.56%] [G loss: 2.056195]\n",
            "3315 [D loss: 0.470655, acc.: 78.12%] [G loss: 1.696160]\n",
            "3316 [D loss: 0.426075, acc.: 78.12%] [G loss: 1.906721]\n",
            "3317 [D loss: 0.619879, acc.: 67.19%] [G loss: 1.678855]\n",
            "3318 [D loss: 0.443770, acc.: 75.00%] [G loss: 1.615499]\n",
            "3319 [D loss: 0.474689, acc.: 67.19%] [G loss: 1.488791]\n",
            "3320 [D loss: 0.461973, acc.: 75.00%] [G loss: 1.419462]\n",
            "3321 [D loss: 0.466020, acc.: 71.88%] [G loss: 1.282889]\n",
            "3322 [D loss: 0.473200, acc.: 75.00%] [G loss: 1.573450]\n",
            "3323 [D loss: 0.446197, acc.: 73.44%] [G loss: 1.687964]\n",
            "3324 [D loss: 0.390981, acc.: 76.56%] [G loss: 1.861233]\n",
            "3325 [D loss: 0.541174, acc.: 73.44%] [G loss: 1.850801]\n",
            "3326 [D loss: 0.464386, acc.: 76.56%] [G loss: 1.791368]\n",
            "3327 [D loss: 0.479945, acc.: 78.12%] [G loss: 1.633837]\n",
            "3328 [D loss: 0.385150, acc.: 78.12%] [G loss: 2.095633]\n",
            "3329 [D loss: 0.483399, acc.: 76.56%] [G loss: 2.156655]\n",
            "3330 [D loss: 0.542456, acc.: 75.00%] [G loss: 1.440939]\n",
            "3331 [D loss: 0.456018, acc.: 76.56%] [G loss: 1.632643]\n",
            "3332 [D loss: 0.511251, acc.: 76.56%] [G loss: 1.413384]\n",
            "3333 [D loss: 0.468648, acc.: 78.12%] [G loss: 1.313471]\n",
            "3334 [D loss: 0.450184, acc.: 73.44%] [G loss: 1.384659]\n",
            "3335 [D loss: 0.466132, acc.: 76.56%] [G loss: 1.382722]\n",
            "3336 [D loss: 0.487836, acc.: 73.44%] [G loss: 1.285660]\n",
            "3337 [D loss: 0.489020, acc.: 73.44%] [G loss: 1.404706]\n",
            "3338 [D loss: 0.450532, acc.: 76.56%] [G loss: 1.653071]\n",
            "3339 [D loss: 0.475246, acc.: 75.00%] [G loss: 1.333889]\n",
            "3340 [D loss: 0.454666, acc.: 75.00%] [G loss: 1.353199]\n",
            "3341 [D loss: 0.386886, acc.: 78.12%] [G loss: 1.532382]\n",
            "3342 [D loss: 0.449261, acc.: 79.69%] [G loss: 1.421858]\n",
            "3343 [D loss: 0.492579, acc.: 73.44%] [G loss: 1.463571]\n",
            "3344 [D loss: 0.425546, acc.: 79.69%] [G loss: 1.946046]\n",
            "3345 [D loss: 0.486131, acc.: 75.00%] [G loss: 1.703160]\n",
            "3346 [D loss: 0.505387, acc.: 79.69%] [G loss: 1.587207]\n",
            "3347 [D loss: 0.411972, acc.: 76.56%] [G loss: 1.895041]\n",
            "3348 [D loss: 0.447732, acc.: 71.88%] [G loss: 1.226907]\n",
            "3349 [D loss: 0.470981, acc.: 70.31%] [G loss: 1.592017]\n",
            "3350 [D loss: 0.410158, acc.: 75.00%] [G loss: 1.648356]\n",
            "3351 [D loss: 0.438555, acc.: 76.56%] [G loss: 1.585366]\n",
            "3352 [D loss: 0.441957, acc.: 81.25%] [G loss: 1.503502]\n",
            "3353 [D loss: 0.397832, acc.: 81.25%] [G loss: 2.072497]\n",
            "3354 [D loss: 0.488891, acc.: 75.00%] [G loss: 1.721088]\n",
            "3355 [D loss: 0.364809, acc.: 78.12%] [G loss: 1.771444]\n",
            "3356 [D loss: 0.436558, acc.: 75.00%] [G loss: 1.598657]\n",
            "3357 [D loss: 0.431612, acc.: 76.56%] [G loss: 1.504641]\n",
            "3358 [D loss: 0.449428, acc.: 76.56%] [G loss: 1.313438]\n",
            "3359 [D loss: 0.450640, acc.: 76.56%] [G loss: 1.307269]\n",
            "3360 [D loss: 0.438739, acc.: 78.12%] [G loss: 1.627715]\n",
            "3361 [D loss: 0.449140, acc.: 76.56%] [G loss: 1.684480]\n",
            "3362 [D loss: 0.495778, acc.: 78.12%] [G loss: 1.211083]\n",
            "3363 [D loss: 0.448040, acc.: 78.12%] [G loss: 1.394292]\n",
            "3364 [D loss: 0.396825, acc.: 78.12%] [G loss: 1.325843]\n",
            "3365 [D loss: 0.481895, acc.: 75.00%] [G loss: 1.423195]\n",
            "3366 [D loss: 0.483553, acc.: 79.69%] [G loss: 1.142390]\n",
            "3367 [D loss: 0.451472, acc.: 78.12%] [G loss: 1.665770]\n",
            "3368 [D loss: 0.430569, acc.: 76.56%] [G loss: 1.646019]\n",
            "3369 [D loss: 0.427468, acc.: 75.00%] [G loss: 1.553973]\n",
            "3370 [D loss: 0.438726, acc.: 78.12%] [G loss: 1.834238]\n",
            "3371 [D loss: 0.382755, acc.: 79.69%] [G loss: 1.874721]\n",
            "3372 [D loss: 0.484262, acc.: 75.00%] [G loss: 1.546159]\n",
            "3373 [D loss: 0.412597, acc.: 73.44%] [G loss: 1.853154]\n",
            "3374 [D loss: 0.533137, acc.: 71.88%] [G loss: 1.474850]\n",
            "3375 [D loss: 0.505549, acc.: 76.56%] [G loss: 1.376411]\n",
            "3376 [D loss: 0.381765, acc.: 81.25%] [G loss: 1.653880]\n",
            "3377 [D loss: 0.380340, acc.: 76.56%] [G loss: 1.828127]\n",
            "3378 [D loss: 0.507500, acc.: 76.56%] [G loss: 1.947770]\n",
            "3379 [D loss: 0.481176, acc.: 68.75%] [G loss: 1.714406]\n",
            "3380 [D loss: 0.447831, acc.: 75.00%] [G loss: 1.563614]\n",
            "3381 [D loss: 0.436420, acc.: 78.12%] [G loss: 1.559711]\n",
            "3382 [D loss: 0.365986, acc.: 76.56%] [G loss: 1.766078]\n",
            "3383 [D loss: 0.401972, acc.: 79.69%] [G loss: 1.780553]\n",
            "3384 [D loss: 0.497353, acc.: 75.00%] [G loss: 1.477569]\n",
            "3385 [D loss: 0.400143, acc.: 78.12%] [G loss: 1.744930]\n",
            "3386 [D loss: 0.399883, acc.: 76.56%] [G loss: 1.415397]\n",
            "3387 [D loss: 0.495541, acc.: 70.31%] [G loss: 1.546957]\n",
            "3388 [D loss: 0.380394, acc.: 81.25%] [G loss: 1.714800]\n",
            "3389 [D loss: 0.493083, acc.: 70.31%] [G loss: 1.713498]\n",
            "3390 [D loss: 0.457372, acc.: 79.69%] [G loss: 1.801598]\n",
            "3391 [D loss: 0.416171, acc.: 78.12%] [G loss: 2.080349]\n",
            "3392 [D loss: 0.454231, acc.: 75.00%] [G loss: 1.273765]\n",
            "3393 [D loss: 0.404400, acc.: 73.44%] [G loss: 2.028292]\n",
            "3394 [D loss: 0.418370, acc.: 78.12%] [G loss: 1.609598]\n",
            "3395 [D loss: 0.496305, acc.: 70.31%] [G loss: 1.696399]\n",
            "3396 [D loss: 0.370530, acc.: 76.56%] [G loss: 1.726150]\n",
            "3397 [D loss: 0.416173, acc.: 75.00%] [G loss: 1.492557]\n",
            "3398 [D loss: 0.411052, acc.: 79.69%] [G loss: 1.597597]\n",
            "3399 [D loss: 0.359453, acc.: 79.69%] [G loss: 1.790924]\n",
            "3400 [D loss: 0.461160, acc.: 76.56%] [G loss: 1.762431]\n",
            "generated_data\n",
            "3401 [D loss: 0.431822, acc.: 76.56%] [G loss: 2.423883]\n",
            "3402 [D loss: 0.473593, acc.: 78.12%] [G loss: 1.505516]\n",
            "3403 [D loss: 0.438307, acc.: 79.69%] [G loss: 1.506845]\n",
            "3404 [D loss: 0.375123, acc.: 79.69%] [G loss: 1.554033]\n",
            "3405 [D loss: 0.394971, acc.: 76.56%] [G loss: 1.655452]\n",
            "3406 [D loss: 0.449853, acc.: 73.44%] [G loss: 1.559150]\n",
            "3407 [D loss: 0.464192, acc.: 78.12%] [G loss: 1.651783]\n",
            "3408 [D loss: 0.418344, acc.: 78.12%] [G loss: 1.485355]\n",
            "3409 [D loss: 0.525034, acc.: 75.00%] [G loss: 1.382121]\n",
            "3410 [D loss: 0.424915, acc.: 78.12%] [G loss: 1.787472]\n",
            "3411 [D loss: 0.410826, acc.: 79.69%] [G loss: 1.493286]\n",
            "3412 [D loss: 0.518570, acc.: 76.56%] [G loss: 1.563875]\n",
            "3413 [D loss: 0.490842, acc.: 78.12%] [G loss: 1.766443]\n",
            "3414 [D loss: 0.440963, acc.: 76.56%] [G loss: 1.211221]\n",
            "3415 [D loss: 0.394495, acc.: 76.56%] [G loss: 1.682903]\n",
            "3416 [D loss: 0.427116, acc.: 78.12%] [G loss: 1.533451]\n",
            "3417 [D loss: 0.453786, acc.: 71.88%] [G loss: 1.844426]\n",
            "3418 [D loss: 0.399345, acc.: 78.12%] [G loss: 1.895015]\n",
            "3419 [D loss: 0.390827, acc.: 78.12%] [G loss: 1.821215]\n",
            "3420 [D loss: 0.538136, acc.: 71.88%] [G loss: 1.389991]\n",
            "3421 [D loss: 0.413110, acc.: 76.56%] [G loss: 1.583612]\n",
            "3422 [D loss: 0.490770, acc.: 78.12%] [G loss: 1.471433]\n",
            "3423 [D loss: 0.480810, acc.: 75.00%] [G loss: 1.778822]\n",
            "3424 [D loss: 0.468368, acc.: 73.44%] [G loss: 1.766338]\n",
            "3425 [D loss: 0.426791, acc.: 76.56%] [G loss: 1.682680]\n",
            "3426 [D loss: 0.565859, acc.: 68.75%] [G loss: 1.318392]\n",
            "3427 [D loss: 0.409221, acc.: 73.44%] [G loss: 1.081038]\n",
            "3428 [D loss: 0.426155, acc.: 78.12%] [G loss: 1.450269]\n",
            "3429 [D loss: 0.432557, acc.: 75.00%] [G loss: 1.499988]\n",
            "3430 [D loss: 0.475287, acc.: 71.88%] [G loss: 1.329997]\n",
            "3431 [D loss: 0.412947, acc.: 78.12%] [G loss: 1.854473]\n",
            "3432 [D loss: 0.471741, acc.: 76.56%] [G loss: 1.353828]\n",
            "3433 [D loss: 0.446429, acc.: 75.00%] [G loss: 1.308469]\n",
            "3434 [D loss: 0.451929, acc.: 73.44%] [G loss: 1.604643]\n",
            "3435 [D loss: 0.474552, acc.: 75.00%] [G loss: 1.653851]\n",
            "3436 [D loss: 0.434795, acc.: 82.81%] [G loss: 1.790948]\n",
            "3437 [D loss: 0.494338, acc.: 76.56%] [G loss: 1.417932]\n",
            "3438 [D loss: 0.383005, acc.: 76.56%] [G loss: 1.730756]\n",
            "3439 [D loss: 0.417885, acc.: 78.12%] [G loss: 1.500471]\n",
            "3440 [D loss: 0.493997, acc.: 76.56%] [G loss: 1.543061]\n",
            "3441 [D loss: 0.371205, acc.: 78.12%] [G loss: 1.670951]\n",
            "3442 [D loss: 0.475818, acc.: 76.56%] [G loss: 1.442821]\n",
            "3443 [D loss: 0.395979, acc.: 78.12%] [G loss: 1.767174]\n",
            "3444 [D loss: 0.379762, acc.: 82.81%] [G loss: 1.416510]\n",
            "3445 [D loss: 0.458392, acc.: 71.88%] [G loss: 1.587237]\n",
            "3446 [D loss: 0.408817, acc.: 78.12%] [G loss: 2.051865]\n",
            "3447 [D loss: 0.493699, acc.: 70.31%] [G loss: 1.265698]\n",
            "3448 [D loss: 0.438545, acc.: 75.00%] [G loss: 1.594132]\n",
            "3449 [D loss: 0.474999, acc.: 76.56%] [G loss: 1.516778]\n",
            "3450 [D loss: 0.513661, acc.: 76.56%] [G loss: 1.449228]\n",
            "3451 [D loss: 0.443867, acc.: 78.12%] [G loss: 1.605576]\n",
            "3452 [D loss: 0.429238, acc.: 78.12%] [G loss: 1.635903]\n",
            "3453 [D loss: 0.395386, acc.: 76.56%] [G loss: 1.611220]\n",
            "3454 [D loss: 0.368549, acc.: 81.25%] [G loss: 1.826094]\n",
            "3455 [D loss: 0.522835, acc.: 73.44%] [G loss: 1.407735]\n",
            "3456 [D loss: 0.437879, acc.: 75.00%] [G loss: 1.319655]\n",
            "3457 [D loss: 0.436674, acc.: 76.56%] [G loss: 1.426398]\n",
            "3458 [D loss: 0.405317, acc.: 78.12%] [G loss: 1.454108]\n",
            "3459 [D loss: 0.431049, acc.: 78.12%] [G loss: 1.740486]\n",
            "3460 [D loss: 0.431845, acc.: 76.56%] [G loss: 1.288222]\n",
            "3461 [D loss: 0.439667, acc.: 75.00%] [G loss: 1.501644]\n",
            "3462 [D loss: 0.446144, acc.: 73.44%] [G loss: 1.466610]\n",
            "3463 [D loss: 0.474582, acc.: 73.44%] [G loss: 1.604353]\n",
            "3464 [D loss: 0.390576, acc.: 73.44%] [G loss: 1.408624]\n",
            "3465 [D loss: 0.402513, acc.: 78.12%] [G loss: 1.434205]\n",
            "3466 [D loss: 0.423871, acc.: 76.56%] [G loss: 1.168147]\n",
            "3467 [D loss: 0.562044, acc.: 75.00%] [G loss: 1.376634]\n",
            "3468 [D loss: 0.547221, acc.: 71.88%] [G loss: 1.658765]\n",
            "3469 [D loss: 0.453261, acc.: 71.88%] [G loss: 1.376314]\n",
            "3470 [D loss: 0.476059, acc.: 73.44%] [G loss: 1.882357]\n",
            "3471 [D loss: 0.426343, acc.: 78.12%] [G loss: 1.518715]\n",
            "3472 [D loss: 0.472457, acc.: 75.00%] [G loss: 1.664052]\n",
            "3473 [D loss: 0.453571, acc.: 76.56%] [G loss: 1.578237]\n",
            "3474 [D loss: 0.441560, acc.: 75.00%] [G loss: 1.457273]\n",
            "3475 [D loss: 0.390642, acc.: 78.12%] [G loss: 1.883198]\n",
            "3476 [D loss: 0.436736, acc.: 78.12%] [G loss: 2.022241]\n",
            "3477 [D loss: 0.483238, acc.: 73.44%] [G loss: 1.598703]\n",
            "3478 [D loss: 0.424585, acc.: 78.12%] [G loss: 1.590455]\n",
            "3479 [D loss: 0.503541, acc.: 67.19%] [G loss: 1.658533]\n",
            "3480 [D loss: 0.458111, acc.: 79.69%] [G loss: 1.615740]\n",
            "3481 [D loss: 0.451698, acc.: 78.12%] [G loss: 1.709834]\n",
            "3482 [D loss: 0.433488, acc.: 73.44%] [G loss: 1.712875]\n",
            "3483 [D loss: 0.436734, acc.: 75.00%] [G loss: 1.708440]\n",
            "3484 [D loss: 0.445962, acc.: 76.56%] [G loss: 1.380039]\n",
            "3485 [D loss: 0.422115, acc.: 78.12%] [G loss: 1.715525]\n",
            "3486 [D loss: 0.386440, acc.: 78.12%] [G loss: 1.738953]\n",
            "3487 [D loss: 0.491874, acc.: 75.00%] [G loss: 1.741634]\n",
            "3488 [D loss: 0.474204, acc.: 76.56%] [G loss: 1.797500]\n",
            "3489 [D loss: 0.395184, acc.: 79.69%] [G loss: 1.780413]\n",
            "3490 [D loss: 0.468224, acc.: 76.56%] [G loss: 1.496961]\n",
            "3491 [D loss: 0.472324, acc.: 76.56%] [G loss: 1.747552]\n",
            "3492 [D loss: 0.384462, acc.: 76.56%] [G loss: 1.860677]\n",
            "3493 [D loss: 0.516486, acc.: 76.56%] [G loss: 1.476538]\n",
            "3494 [D loss: 0.469994, acc.: 75.00%] [G loss: 1.640390]\n",
            "3495 [D loss: 0.686692, acc.: 71.88%] [G loss: 1.143239]\n",
            "3496 [D loss: 0.436789, acc.: 75.00%] [G loss: 1.811856]\n",
            "3497 [D loss: 0.454071, acc.: 76.56%] [G loss: 1.699724]\n",
            "3498 [D loss: 0.521525, acc.: 75.00%] [G loss: 1.781540]\n",
            "3499 [D loss: 0.383120, acc.: 78.12%] [G loss: 1.984658]\n",
            "3500 [D loss: 0.444098, acc.: 79.69%] [G loss: 1.545693]\n",
            "generated_data\n",
            "3501 [D loss: 0.378516, acc.: 79.69%] [G loss: 1.512439]\n",
            "3502 [D loss: 0.460494, acc.: 71.88%] [G loss: 1.824910]\n",
            "3503 [D loss: 0.486494, acc.: 73.44%] [G loss: 1.366665]\n",
            "3504 [D loss: 0.412731, acc.: 78.12%] [G loss: 1.867639]\n",
            "3505 [D loss: 0.440785, acc.: 73.44%] [G loss: 2.358807]\n",
            "3506 [D loss: 0.566705, acc.: 73.44%] [G loss: 1.887884]\n",
            "3507 [D loss: 0.402725, acc.: 78.12%] [G loss: 1.697749]\n",
            "3508 [D loss: 0.478521, acc.: 73.44%] [G loss: 1.555556]\n",
            "3509 [D loss: 0.497387, acc.: 73.44%] [G loss: 1.764238]\n",
            "3510 [D loss: 0.433048, acc.: 75.00%] [G loss: 1.825844]\n",
            "3511 [D loss: 0.530476, acc.: 78.12%] [G loss: 1.341060]\n",
            "3512 [D loss: 0.509693, acc.: 70.31%] [G loss: 1.555643]\n",
            "3513 [D loss: 0.392960, acc.: 76.56%] [G loss: 1.629432]\n",
            "3514 [D loss: 0.425142, acc.: 76.56%] [G loss: 1.746177]\n",
            "3515 [D loss: 0.414266, acc.: 75.00%] [G loss: 1.581807]\n",
            "3516 [D loss: 0.390836, acc.: 78.12%] [G loss: 1.728947]\n",
            "3517 [D loss: 0.448792, acc.: 78.12%] [G loss: 1.214142]\n",
            "3518 [D loss: 0.429142, acc.: 81.25%] [G loss: 1.496497]\n",
            "3519 [D loss: 0.395659, acc.: 76.56%] [G loss: 1.817601]\n",
            "3520 [D loss: 0.442176, acc.: 75.00%] [G loss: 1.933876]\n",
            "3521 [D loss: 0.421865, acc.: 78.12%] [G loss: 1.804771]\n",
            "3522 [D loss: 0.442400, acc.: 76.56%] [G loss: 1.518828]\n",
            "3523 [D loss: 0.428216, acc.: 75.00%] [G loss: 1.746170]\n",
            "3524 [D loss: 0.421427, acc.: 78.12%] [G loss: 1.557016]\n",
            "3525 [D loss: 0.518228, acc.: 76.56%] [G loss: 1.531745]\n",
            "3526 [D loss: 0.412199, acc.: 78.12%] [G loss: 1.526753]\n",
            "3527 [D loss: 0.457314, acc.: 76.56%] [G loss: 1.066921]\n",
            "3528 [D loss: 0.384058, acc.: 79.69%] [G loss: 1.652931]\n",
            "3529 [D loss: 0.480258, acc.: 73.44%] [G loss: 1.256169]\n",
            "3530 [D loss: 0.435062, acc.: 75.00%] [G loss: 1.568080]\n",
            "3531 [D loss: 0.432215, acc.: 76.56%] [G loss: 1.408243]\n",
            "3532 [D loss: 0.650189, acc.: 71.88%] [G loss: 1.540769]\n",
            "3533 [D loss: 0.416715, acc.: 79.69%] [G loss: 1.432417]\n",
            "3534 [D loss: 0.449877, acc.: 78.12%] [G loss: 1.286533]\n",
            "3535 [D loss: 0.465449, acc.: 76.56%] [G loss: 1.629780]\n",
            "3536 [D loss: 0.421971, acc.: 79.69%] [G loss: 1.990084]\n",
            "3537 [D loss: 0.408047, acc.: 79.69%] [G loss: 1.386660]\n",
            "3538 [D loss: 0.480992, acc.: 71.88%] [G loss: 1.549594]\n",
            "3539 [D loss: 0.399662, acc.: 76.56%] [G loss: 1.420528]\n",
            "3540 [D loss: 0.526552, acc.: 71.88%] [G loss: 1.587054]\n",
            "3541 [D loss: 0.436508, acc.: 78.12%] [G loss: 1.591558]\n",
            "3542 [D loss: 0.462632, acc.: 75.00%] [G loss: 1.387412]\n",
            "3543 [D loss: 0.460771, acc.: 75.00%] [G loss: 1.545012]\n",
            "3544 [D loss: 0.402023, acc.: 79.69%] [G loss: 1.372972]\n",
            "3545 [D loss: 0.493353, acc.: 78.12%] [G loss: 1.316483]\n",
            "3546 [D loss: 0.404917, acc.: 75.00%] [G loss: 1.993311]\n",
            "3547 [D loss: 0.460938, acc.: 73.44%] [G loss: 1.646787]\n",
            "3548 [D loss: 0.453624, acc.: 76.56%] [G loss: 1.566007]\n",
            "3549 [D loss: 0.493623, acc.: 79.69%] [G loss: 1.618942]\n",
            "3550 [D loss: 0.432037, acc.: 79.69%] [G loss: 1.437272]\n",
            "3551 [D loss: 0.417831, acc.: 76.56%] [G loss: 1.402282]\n",
            "3552 [D loss: 0.437864, acc.: 78.12%] [G loss: 1.417485]\n",
            "3553 [D loss: 0.454867, acc.: 78.12%] [G loss: 1.403276]\n",
            "3554 [D loss: 0.437875, acc.: 78.12%] [G loss: 1.499488]\n",
            "3555 [D loss: 0.394177, acc.: 81.25%] [G loss: 1.372885]\n",
            "3556 [D loss: 0.449094, acc.: 78.12%] [G loss: 1.434738]\n",
            "3557 [D loss: 0.397731, acc.: 79.69%] [G loss: 1.491818]\n",
            "3558 [D loss: 0.437625, acc.: 71.88%] [G loss: 1.536941]\n",
            "3559 [D loss: 0.448130, acc.: 82.81%] [G loss: 1.509206]\n",
            "3560 [D loss: 0.417726, acc.: 78.12%] [G loss: 1.388976]\n",
            "3561 [D loss: 0.449989, acc.: 78.12%] [G loss: 1.554507]\n",
            "3562 [D loss: 0.460933, acc.: 76.56%] [G loss: 1.519711]\n",
            "3563 [D loss: 0.428106, acc.: 75.00%] [G loss: 1.636779]\n",
            "3564 [D loss: 0.613215, acc.: 73.44%] [G loss: 1.415475]\n",
            "3565 [D loss: 0.430748, acc.: 78.12%] [G loss: 1.792078]\n",
            "3566 [D loss: 0.494319, acc.: 76.56%] [G loss: 1.928139]\n",
            "3567 [D loss: 0.419764, acc.: 78.12%] [G loss: 1.847221]\n",
            "3568 [D loss: 0.419799, acc.: 78.12%] [G loss: 1.417027]\n",
            "3569 [D loss: 0.430684, acc.: 75.00%] [G loss: 1.780716]\n",
            "3570 [D loss: 0.434060, acc.: 79.69%] [G loss: 1.356513]\n",
            "3571 [D loss: 0.437378, acc.: 76.56%] [G loss: 1.544249]\n",
            "3572 [D loss: 0.399184, acc.: 76.56%] [G loss: 1.563393]\n",
            "3573 [D loss: 0.428530, acc.: 78.12%] [G loss: 1.434671]\n",
            "3574 [D loss: 0.370628, acc.: 79.69%] [G loss: 2.001340]\n",
            "3575 [D loss: 0.502909, acc.: 75.00%] [G loss: 1.410971]\n",
            "3576 [D loss: 0.452328, acc.: 75.00%] [G loss: 1.690194]\n",
            "3577 [D loss: 0.416404, acc.: 78.12%] [G loss: 1.404638]\n",
            "3578 [D loss: 0.474440, acc.: 78.12%] [G loss: 1.881549]\n",
            "3579 [D loss: 0.414060, acc.: 79.69%] [G loss: 1.700617]\n",
            "3580 [D loss: 0.474860, acc.: 73.44%] [G loss: 1.638197]\n",
            "3581 [D loss: 0.421673, acc.: 75.00%] [G loss: 1.444565]\n",
            "3582 [D loss: 0.434119, acc.: 78.12%] [G loss: 1.375600]\n",
            "3583 [D loss: 0.483044, acc.: 75.00%] [G loss: 1.360382]\n",
            "3584 [D loss: 0.421474, acc.: 76.56%] [G loss: 1.335334]\n",
            "3585 [D loss: 0.458025, acc.: 76.56%] [G loss: 1.374918]\n",
            "3586 [D loss: 0.431987, acc.: 78.12%] [G loss: 1.637081]\n",
            "3587 [D loss: 0.465125, acc.: 79.69%] [G loss: 1.179669]\n",
            "3588 [D loss: 0.407396, acc.: 78.12%] [G loss: 1.596462]\n",
            "3589 [D loss: 0.457823, acc.: 75.00%] [G loss: 1.364712]\n",
            "3590 [D loss: 0.406191, acc.: 73.44%] [G loss: 1.513989]\n",
            "3591 [D loss: 0.433731, acc.: 76.56%] [G loss: 1.321950]\n",
            "3592 [D loss: 0.393544, acc.: 78.12%] [G loss: 2.032167]\n",
            "3593 [D loss: 0.443257, acc.: 76.56%] [G loss: 2.538510]\n",
            "3594 [D loss: 0.533995, acc.: 70.31%] [G loss: 1.512340]\n",
            "3595 [D loss: 0.397462, acc.: 76.56%] [G loss: 1.966588]\n",
            "3596 [D loss: 0.454433, acc.: 78.12%] [G loss: 1.680395]\n",
            "3597 [D loss: 0.445489, acc.: 76.56%] [G loss: 1.462505]\n",
            "3598 [D loss: 0.447306, acc.: 73.44%] [G loss: 1.654181]\n",
            "3599 [D loss: 0.425530, acc.: 82.81%] [G loss: 1.290978]\n",
            "3600 [D loss: 0.437707, acc.: 75.00%] [G loss: 1.282025]\n",
            "generated_data\n",
            "3601 [D loss: 0.422842, acc.: 73.44%] [G loss: 1.160500]\n",
            "3602 [D loss: 0.514165, acc.: 78.12%] [G loss: 1.589140]\n",
            "3603 [D loss: 0.464262, acc.: 78.12%] [G loss: 1.253269]\n",
            "3604 [D loss: 0.412878, acc.: 81.25%] [G loss: 1.380146]\n",
            "3605 [D loss: 0.433476, acc.: 79.69%] [G loss: 1.674217]\n",
            "3606 [D loss: 0.409439, acc.: 81.25%] [G loss: 1.357637]\n",
            "3607 [D loss: 0.431222, acc.: 79.69%] [G loss: 1.421514]\n",
            "3608 [D loss: 0.524587, acc.: 76.56%] [G loss: 1.358106]\n",
            "3609 [D loss: 0.407825, acc.: 81.25%] [G loss: 1.775436]\n",
            "3610 [D loss: 0.524817, acc.: 75.00%] [G loss: 1.421169]\n",
            "3611 [D loss: 0.425232, acc.: 81.25%] [G loss: 1.664236]\n",
            "3612 [D loss: 0.412715, acc.: 78.12%] [G loss: 1.440028]\n",
            "3613 [D loss: 0.437581, acc.: 73.44%] [G loss: 1.528990]\n",
            "3614 [D loss: 0.463468, acc.: 75.00%] [G loss: 1.438170]\n",
            "3615 [D loss: 0.421351, acc.: 79.69%] [G loss: 1.366175]\n",
            "3616 [D loss: 0.385815, acc.: 78.12%] [G loss: 1.976929]\n",
            "3617 [D loss: 0.465254, acc.: 78.12%] [G loss: 1.464455]\n",
            "3618 [D loss: 0.386582, acc.: 78.12%] [G loss: 1.639183]\n",
            "3619 [D loss: 0.353155, acc.: 81.25%] [G loss: 1.530811]\n",
            "3620 [D loss: 0.448354, acc.: 79.69%] [G loss: 1.252578]\n",
            "3621 [D loss: 0.405203, acc.: 79.69%] [G loss: 1.829412]\n",
            "3622 [D loss: 0.513730, acc.: 73.44%] [G loss: 1.355996]\n",
            "3623 [D loss: 0.497383, acc.: 73.44%] [G loss: 1.629962]\n",
            "3624 [D loss: 0.491510, acc.: 76.56%] [G loss: 1.484486]\n",
            "3625 [D loss: 0.413828, acc.: 79.69%] [G loss: 1.519629]\n",
            "3626 [D loss: 0.364776, acc.: 79.69%] [G loss: 1.891199]\n",
            "3627 [D loss: 0.462412, acc.: 75.00%] [G loss: 1.572137]\n",
            "3628 [D loss: 0.434460, acc.: 75.00%] [G loss: 1.944955]\n",
            "3629 [D loss: 0.456593, acc.: 75.00%] [G loss: 1.614503]\n",
            "3630 [D loss: 0.454561, acc.: 76.56%] [G loss: 1.438651]\n",
            "3631 [D loss: 0.406419, acc.: 79.69%] [G loss: 1.508209]\n",
            "3632 [D loss: 0.419907, acc.: 79.69%] [G loss: 1.254891]\n",
            "3633 [D loss: 0.424899, acc.: 75.00%] [G loss: 1.806225]\n",
            "3634 [D loss: 0.478104, acc.: 75.00%] [G loss: 1.434059]\n",
            "3635 [D loss: 0.493417, acc.: 76.56%] [G loss: 1.550952]\n",
            "3636 [D loss: 0.381903, acc.: 79.69%] [G loss: 1.486891]\n",
            "3637 [D loss: 0.465204, acc.: 75.00%] [G loss: 1.444603]\n",
            "3638 [D loss: 0.422554, acc.: 79.69%] [G loss: 1.494701]\n",
            "3639 [D loss: 0.468500, acc.: 76.56%] [G loss: 1.220790]\n",
            "3640 [D loss: 0.417000, acc.: 75.00%] [G loss: 1.442780]\n",
            "3641 [D loss: 0.398704, acc.: 79.69%] [G loss: 1.253999]\n",
            "3642 [D loss: 0.404445, acc.: 78.12%] [G loss: 1.305475]\n",
            "3643 [D loss: 0.414149, acc.: 79.69%] [G loss: 1.564947]\n",
            "3644 [D loss: 0.432701, acc.: 79.69%] [G loss: 1.391270]\n",
            "3645 [D loss: 0.394404, acc.: 78.12%] [G loss: 1.431034]\n",
            "3646 [D loss: 0.405627, acc.: 79.69%] [G loss: 1.382873]\n",
            "3647 [D loss: 0.407660, acc.: 78.12%] [G loss: 2.051370]\n",
            "3648 [D loss: 0.430640, acc.: 78.12%] [G loss: 1.307858]\n",
            "3649 [D loss: 0.413341, acc.: 79.69%] [G loss: 1.480302]\n",
            "3650 [D loss: 0.423870, acc.: 78.12%] [G loss: 1.460940]\n",
            "3651 [D loss: 0.418859, acc.: 76.56%] [G loss: 1.664852]\n",
            "3652 [D loss: 0.380128, acc.: 78.12%] [G loss: 1.931855]\n",
            "3653 [D loss: 0.416217, acc.: 78.12%] [G loss: 1.648092]\n",
            "3654 [D loss: 0.464661, acc.: 76.56%] [G loss: 1.573240]\n",
            "3655 [D loss: 0.400114, acc.: 76.56%] [G loss: 2.039590]\n",
            "3656 [D loss: 0.443921, acc.: 78.12%] [G loss: 1.561369]\n",
            "3657 [D loss: 0.421664, acc.: 76.56%] [G loss: 1.327014]\n",
            "3658 [D loss: 0.441699, acc.: 76.56%] [G loss: 1.502289]\n",
            "3659 [D loss: 0.477190, acc.: 71.88%] [G loss: 1.653591]\n",
            "3660 [D loss: 0.415296, acc.: 78.12%] [G loss: 1.787730]\n",
            "3661 [D loss: 0.454618, acc.: 78.12%] [G loss: 1.848159]\n",
            "3662 [D loss: 0.450297, acc.: 75.00%] [G loss: 1.866870]\n",
            "3663 [D loss: 0.457314, acc.: 75.00%] [G loss: 1.525984]\n",
            "3664 [D loss: 0.454586, acc.: 71.88%] [G loss: 2.031926]\n",
            "3665 [D loss: 0.476665, acc.: 75.00%] [G loss: 1.515526]\n",
            "3666 [D loss: 0.439804, acc.: 81.25%] [G loss: 1.606253]\n",
            "3667 [D loss: 0.377476, acc.: 78.12%] [G loss: 1.853951]\n",
            "3668 [D loss: 0.464195, acc.: 75.00%] [G loss: 1.653566]\n",
            "3669 [D loss: 0.466012, acc.: 75.00%] [G loss: 1.726989]\n",
            "3670 [D loss: 0.418265, acc.: 78.12%] [G loss: 1.410378]\n",
            "3671 [D loss: 0.454132, acc.: 78.12%] [G loss: 1.507935]\n",
            "3672 [D loss: 0.400528, acc.: 75.00%] [G loss: 2.338044]\n",
            "3673 [D loss: 0.412576, acc.: 79.69%] [G loss: 1.465069]\n",
            "3674 [D loss: 0.414337, acc.: 78.12%] [G loss: 1.587654]\n",
            "3675 [D loss: 0.435445, acc.: 78.12%] [G loss: 2.056077]\n",
            "3676 [D loss: 0.519325, acc.: 75.00%] [G loss: 1.760035]\n",
            "3677 [D loss: 0.525377, acc.: 73.44%] [G loss: 1.476421]\n",
            "3678 [D loss: 0.400337, acc.: 76.56%] [G loss: 1.467770]\n",
            "3679 [D loss: 0.431489, acc.: 79.69%] [G loss: 1.514949]\n",
            "3680 [D loss: 0.456608, acc.: 75.00%] [G loss: 1.550528]\n",
            "3681 [D loss: 0.444347, acc.: 75.00%] [G loss: 1.522436]\n",
            "3682 [D loss: 0.421434, acc.: 78.12%] [G loss: 1.539199]\n",
            "3683 [D loss: 0.432206, acc.: 75.00%] [G loss: 1.842540]\n",
            "3684 [D loss: 0.339798, acc.: 81.25%] [G loss: 1.733907]\n",
            "3685 [D loss: 0.542281, acc.: 76.56%] [G loss: 1.554945]\n",
            "3686 [D loss: 0.393316, acc.: 75.00%] [G loss: 1.857120]\n",
            "3687 [D loss: 0.464761, acc.: 70.31%] [G loss: 1.639712]\n",
            "3688 [D loss: 0.476702, acc.: 75.00%] [G loss: 1.094790]\n",
            "3689 [D loss: 0.388980, acc.: 78.12%] [G loss: 1.598454]\n",
            "3690 [D loss: 0.575446, acc.: 73.44%] [G loss: 1.347400]\n",
            "3691 [D loss: 0.417223, acc.: 79.69%] [G loss: 1.662802]\n",
            "3692 [D loss: 0.404677, acc.: 81.25%] [G loss: 1.727780]\n",
            "3693 [D loss: 0.489227, acc.: 78.12%] [G loss: 1.775042]\n",
            "3694 [D loss: 0.422612, acc.: 76.56%] [G loss: 1.354251]\n",
            "3695 [D loss: 0.507022, acc.: 75.00%] [G loss: 1.392747]\n",
            "3696 [D loss: 0.394909, acc.: 78.12%] [G loss: 2.171864]\n",
            "3697 [D loss: 0.449856, acc.: 78.12%] [G loss: 1.594899]\n",
            "3698 [D loss: 0.402542, acc.: 76.56%] [G loss: 1.845788]\n",
            "3699 [D loss: 0.387398, acc.: 79.69%] [G loss: 1.634092]\n",
            "3700 [D loss: 0.419587, acc.: 79.69%] [G loss: 1.313280]\n",
            "generated_data\n",
            "3701 [D loss: 0.391923, acc.: 79.69%] [G loss: 1.869581]\n",
            "3702 [D loss: 0.475689, acc.: 75.00%] [G loss: 1.631478]\n",
            "3703 [D loss: 0.362961, acc.: 76.56%] [G loss: 1.870691]\n",
            "3704 [D loss: 0.608711, acc.: 71.88%] [G loss: 1.405208]\n",
            "3705 [D loss: 0.426788, acc.: 79.69%] [G loss: 1.664380]\n",
            "3706 [D loss: 0.408581, acc.: 76.56%] [G loss: 1.523542]\n",
            "3707 [D loss: 0.475994, acc.: 76.56%] [G loss: 1.320057]\n",
            "3708 [D loss: 0.399494, acc.: 76.56%] [G loss: 1.732491]\n",
            "3709 [D loss: 0.481582, acc.: 76.56%] [G loss: 1.609494]\n",
            "3710 [D loss: 0.411687, acc.: 78.12%] [G loss: 1.903410]\n",
            "3711 [D loss: 0.477267, acc.: 76.56%] [G loss: 1.308948]\n",
            "3712 [D loss: 0.454127, acc.: 78.12%] [G loss: 1.562515]\n",
            "3713 [D loss: 0.440110, acc.: 79.69%] [G loss: 1.582928]\n",
            "3714 [D loss: 0.492227, acc.: 75.00%] [G loss: 1.303513]\n",
            "3715 [D loss: 0.405178, acc.: 76.56%] [G loss: 1.459692]\n",
            "3716 [D loss: 0.475934, acc.: 73.44%] [G loss: 1.615492]\n",
            "3717 [D loss: 0.478060, acc.: 76.56%] [G loss: 1.539893]\n",
            "3718 [D loss: 0.426766, acc.: 73.44%] [G loss: 2.039703]\n",
            "3719 [D loss: 0.491292, acc.: 75.00%] [G loss: 1.355442]\n",
            "3720 [D loss: 0.403121, acc.: 78.12%] [G loss: 1.752442]\n",
            "3721 [D loss: 0.478446, acc.: 76.56%] [G loss: 1.662977]\n",
            "3722 [D loss: 0.501749, acc.: 76.56%] [G loss: 1.392854]\n",
            "3723 [D loss: 0.391617, acc.: 76.56%] [G loss: 1.842900]\n",
            "3724 [D loss: 0.487337, acc.: 75.00%] [G loss: 1.396057]\n",
            "3725 [D loss: 0.417726, acc.: 76.56%] [G loss: 1.687986]\n",
            "3726 [D loss: 0.408971, acc.: 78.12%] [G loss: 1.765344]\n",
            "3727 [D loss: 0.453370, acc.: 75.00%] [G loss: 1.451256]\n",
            "3728 [D loss: 0.399431, acc.: 81.25%] [G loss: 1.659199]\n",
            "3729 [D loss: 0.444702, acc.: 75.00%] [G loss: 1.576624]\n",
            "3730 [D loss: 0.445280, acc.: 71.88%] [G loss: 1.638845]\n",
            "3731 [D loss: 0.447015, acc.: 76.56%] [G loss: 1.665460]\n",
            "3732 [D loss: 0.514749, acc.: 75.00%] [G loss: 1.370536]\n",
            "3733 [D loss: 0.435930, acc.: 76.56%] [G loss: 1.363204]\n",
            "3734 [D loss: 0.385627, acc.: 76.56%] [G loss: 1.645134]\n",
            "3735 [D loss: 0.479115, acc.: 75.00%] [G loss: 1.703152]\n",
            "3736 [D loss: 0.359912, acc.: 79.69%] [G loss: 1.649174]\n",
            "3737 [D loss: 0.435783, acc.: 76.56%] [G loss: 1.701185]\n",
            "3738 [D loss: 0.429796, acc.: 79.69%] [G loss: 1.537072]\n",
            "3739 [D loss: 0.413507, acc.: 79.69%] [G loss: 1.444445]\n",
            "3740 [D loss: 0.406315, acc.: 75.00%] [G loss: 1.691339]\n",
            "3741 [D loss: 0.410732, acc.: 79.69%] [G loss: 1.745544]\n",
            "3742 [D loss: 0.429118, acc.: 79.69%] [G loss: 1.581413]\n",
            "3743 [D loss: 0.527937, acc.: 73.44%] [G loss: 1.640144]\n",
            "3744 [D loss: 0.377817, acc.: 81.25%] [G loss: 1.755412]\n",
            "3745 [D loss: 0.469202, acc.: 75.00%] [G loss: 1.792133]\n",
            "3746 [D loss: 0.556038, acc.: 71.88%] [G loss: 1.359794]\n",
            "3747 [D loss: 0.459069, acc.: 76.56%] [G loss: 1.628187]\n",
            "3748 [D loss: 0.439598, acc.: 79.69%] [G loss: 1.582181]\n",
            "3749 [D loss: 0.460036, acc.: 75.00%] [G loss: 1.427337]\n",
            "3750 [D loss: 0.449568, acc.: 73.44%] [G loss: 1.309110]\n",
            "3751 [D loss: 0.505048, acc.: 73.44%] [G loss: 1.281809]\n",
            "3752 [D loss: 0.418692, acc.: 76.56%] [G loss: 1.317184]\n",
            "3753 [D loss: 0.428786, acc.: 76.56%] [G loss: 1.472966]\n",
            "3754 [D loss: 0.473269, acc.: 81.25%] [G loss: 1.342684]\n",
            "3755 [D loss: 0.427557, acc.: 79.69%] [G loss: 1.264474]\n",
            "3756 [D loss: 0.403278, acc.: 79.69%] [G loss: 1.336119]\n",
            "3757 [D loss: 0.511840, acc.: 76.56%] [G loss: 1.674206]\n",
            "3758 [D loss: 0.482236, acc.: 75.00%] [G loss: 1.485665]\n",
            "3759 [D loss: 0.425530, acc.: 78.12%] [G loss: 1.829640]\n",
            "3760 [D loss: 0.437902, acc.: 78.12%] [G loss: 1.671346]\n",
            "3761 [D loss: 0.478479, acc.: 71.88%] [G loss: 1.425791]\n",
            "3762 [D loss: 0.382550, acc.: 81.25%] [G loss: 1.317027]\n",
            "3763 [D loss: 0.498444, acc.: 75.00%] [G loss: 1.457631]\n",
            "3764 [D loss: 0.439260, acc.: 81.25%] [G loss: 1.248700]\n",
            "3765 [D loss: 0.435024, acc.: 82.81%] [G loss: 1.457864]\n",
            "3766 [D loss: 0.479953, acc.: 79.69%] [G loss: 1.350559]\n",
            "3767 [D loss: 0.414543, acc.: 78.12%] [G loss: 1.223947]\n",
            "3768 [D loss: 0.513198, acc.: 73.44%] [G loss: 1.389772]\n",
            "3769 [D loss: 0.453890, acc.: 76.56%] [G loss: 1.454677]\n",
            "3770 [D loss: 0.416261, acc.: 76.56%] [G loss: 1.461644]\n",
            "3771 [D loss: 0.452684, acc.: 78.12%] [G loss: 1.436471]\n",
            "3772 [D loss: 0.442826, acc.: 78.12%] [G loss: 1.199675]\n",
            "3773 [D loss: 0.409315, acc.: 79.69%] [G loss: 1.366838]\n",
            "3774 [D loss: 0.429547, acc.: 78.12%] [G loss: 1.810711]\n",
            "3775 [D loss: 0.479778, acc.: 75.00%] [G loss: 1.705736]\n",
            "3776 [D loss: 0.473645, acc.: 76.56%] [G loss: 1.536507]\n",
            "3777 [D loss: 0.482038, acc.: 76.56%] [G loss: 1.829173]\n",
            "3778 [D loss: 0.481289, acc.: 76.56%] [G loss: 1.578190]\n",
            "3779 [D loss: 0.503646, acc.: 76.56%] [G loss: 1.334620]\n",
            "3780 [D loss: 0.368676, acc.: 81.25%] [G loss: 1.533146]\n",
            "3781 [D loss: 0.458692, acc.: 73.44%] [G loss: 1.492783]\n",
            "3782 [D loss: 0.473066, acc.: 73.44%] [G loss: 1.455170]\n",
            "3783 [D loss: 0.430360, acc.: 79.69%] [G loss: 1.833928]\n",
            "3784 [D loss: 0.383690, acc.: 78.12%] [G loss: 1.943709]\n",
            "3785 [D loss: 0.411329, acc.: 79.69%] [G loss: 1.577207]\n",
            "3786 [D loss: 0.380616, acc.: 81.25%] [G loss: 1.526791]\n",
            "3787 [D loss: 0.403524, acc.: 78.12%] [G loss: 1.893658]\n",
            "3788 [D loss: 0.424429, acc.: 78.12%] [G loss: 1.781558]\n",
            "3789 [D loss: 0.433201, acc.: 76.56%] [G loss: 1.420565]\n",
            "3790 [D loss: 0.498554, acc.: 76.56%] [G loss: 1.611547]\n",
            "3791 [D loss: 0.392656, acc.: 79.69%] [G loss: 1.493079]\n",
            "3792 [D loss: 0.438684, acc.: 75.00%] [G loss: 1.514877]\n",
            "3793 [D loss: 0.397284, acc.: 76.56%] [G loss: 1.898293]\n",
            "3794 [D loss: 0.509404, acc.: 78.12%] [G loss: 1.022182]\n",
            "3795 [D loss: 0.483660, acc.: 75.00%] [G loss: 1.401721]\n",
            "3796 [D loss: 0.427081, acc.: 78.12%] [G loss: 1.274204]\n",
            "3797 [D loss: 0.432238, acc.: 79.69%] [G loss: 1.374905]\n",
            "3798 [D loss: 0.428849, acc.: 79.69%] [G loss: 1.289031]\n",
            "3799 [D loss: 0.433352, acc.: 78.12%] [G loss: 1.322388]\n",
            "3800 [D loss: 0.447595, acc.: 76.56%] [G loss: 1.257342]\n",
            "generated_data\n",
            "3801 [D loss: 0.450135, acc.: 73.44%] [G loss: 1.220625]\n",
            "3802 [D loss: 0.454768, acc.: 78.12%] [G loss: 1.549010]\n",
            "3803 [D loss: 0.453035, acc.: 76.56%] [G loss: 1.660037]\n",
            "3804 [D loss: 0.475152, acc.: 76.56%] [G loss: 1.391410]\n",
            "3805 [D loss: 0.404368, acc.: 78.12%] [G loss: 1.581415]\n",
            "3806 [D loss: 0.484204, acc.: 75.00%] [G loss: 1.418294]\n",
            "3807 [D loss: 0.441076, acc.: 79.69%] [G loss: 1.322459]\n",
            "3808 [D loss: 0.453170, acc.: 75.00%] [G loss: 1.578315]\n",
            "3809 [D loss: 0.430817, acc.: 75.00%] [G loss: 1.380829]\n",
            "3810 [D loss: 0.492901, acc.: 75.00%] [G loss: 1.288504]\n",
            "3811 [D loss: 0.467508, acc.: 73.44%] [G loss: 1.517888]\n",
            "3812 [D loss: 0.440725, acc.: 78.12%] [G loss: 1.629729]\n",
            "3813 [D loss: 0.415904, acc.: 79.69%] [G loss: 1.562032]\n",
            "3814 [D loss: 0.391111, acc.: 78.12%] [G loss: 1.616931]\n",
            "3815 [D loss: 0.455523, acc.: 76.56%] [G loss: 1.519122]\n",
            "3816 [D loss: 0.431664, acc.: 78.12%] [G loss: 1.567071]\n",
            "3817 [D loss: 0.427096, acc.: 78.12%] [G loss: 1.565272]\n",
            "3818 [D loss: 0.497130, acc.: 76.56%] [G loss: 1.399462]\n",
            "3819 [D loss: 0.402532, acc.: 79.69%] [G loss: 1.532497]\n",
            "3820 [D loss: 0.444057, acc.: 75.00%] [G loss: 1.521768]\n",
            "3821 [D loss: 0.416821, acc.: 75.00%] [G loss: 1.730236]\n",
            "3822 [D loss: 0.472511, acc.: 76.56%] [G loss: 1.475165]\n",
            "3823 [D loss: 0.476415, acc.: 76.56%] [G loss: 1.432399]\n",
            "3824 [D loss: 0.418730, acc.: 78.12%] [G loss: 1.781316]\n",
            "3825 [D loss: 0.511792, acc.: 76.56%] [G loss: 1.369350]\n",
            "3826 [D loss: 0.444325, acc.: 78.12%] [G loss: 1.301724]\n",
            "3827 [D loss: 0.449457, acc.: 78.12%] [G loss: 1.552112]\n",
            "3828 [D loss: 0.430737, acc.: 78.12%] [G loss: 1.458062]\n",
            "3829 [D loss: 0.381332, acc.: 78.12%] [G loss: 1.574130]\n",
            "3830 [D loss: 0.539628, acc.: 71.88%] [G loss: 1.454625]\n",
            "3831 [D loss: 0.420718, acc.: 78.12%] [G loss: 1.452789]\n",
            "3832 [D loss: 0.433792, acc.: 76.56%] [G loss: 1.417828]\n",
            "3833 [D loss: 0.512436, acc.: 73.44%] [G loss: 1.082846]\n",
            "3834 [D loss: 0.480768, acc.: 73.44%] [G loss: 1.643198]\n",
            "3835 [D loss: 0.440531, acc.: 75.00%] [G loss: 1.644378]\n",
            "3836 [D loss: 0.420681, acc.: 78.12%] [G loss: 1.514205]\n",
            "3837 [D loss: 0.441376, acc.: 73.44%] [G loss: 1.286897]\n",
            "3838 [D loss: 0.412822, acc.: 78.12%] [G loss: 2.277646]\n",
            "3839 [D loss: 0.465410, acc.: 71.88%] [G loss: 1.537370]\n",
            "3840 [D loss: 0.446697, acc.: 78.12%] [G loss: 1.499286]\n",
            "3841 [D loss: 0.561189, acc.: 70.31%] [G loss: 1.471734]\n",
            "3842 [D loss: 0.426707, acc.: 78.12%] [G loss: 1.544591]\n",
            "3843 [D loss: 0.394973, acc.: 78.12%] [G loss: 1.652351]\n",
            "3844 [D loss: 0.449556, acc.: 76.56%] [G loss: 1.523336]\n",
            "3845 [D loss: 0.420995, acc.: 76.56%] [G loss: 1.684796]\n",
            "3846 [D loss: 0.443466, acc.: 78.12%] [G loss: 1.646699]\n",
            "3847 [D loss: 0.484653, acc.: 75.00%] [G loss: 1.394340]\n",
            "3848 [D loss: 0.462466, acc.: 75.00%] [G loss: 1.202204]\n",
            "3849 [D loss: 0.575798, acc.: 71.88%] [G loss: 1.787410]\n",
            "3850 [D loss: 0.597428, acc.: 73.44%] [G loss: 1.557508]\n",
            "3851 [D loss: 0.531057, acc.: 71.88%] [G loss: 1.753265]\n",
            "3852 [D loss: 0.452243, acc.: 75.00%] [G loss: 1.545218]\n",
            "3853 [D loss: 0.432851, acc.: 81.25%] [G loss: 1.774534]\n",
            "3854 [D loss: 0.519061, acc.: 76.56%] [G loss: 1.174150]\n",
            "3855 [D loss: 0.447308, acc.: 76.56%] [G loss: 1.465297]\n",
            "3856 [D loss: 0.464834, acc.: 75.00%] [G loss: 1.509315]\n",
            "3857 [D loss: 0.471111, acc.: 78.12%] [G loss: 1.222370]\n",
            "3858 [D loss: 0.438632, acc.: 78.12%] [G loss: 1.328472]\n",
            "3859 [D loss: 0.459891, acc.: 76.56%] [G loss: 1.467605]\n",
            "3860 [D loss: 0.420461, acc.: 73.44%] [G loss: 1.753423]\n",
            "3861 [D loss: 0.450030, acc.: 78.12%] [G loss: 1.505642]\n",
            "3862 [D loss: 0.459653, acc.: 75.00%] [G loss: 1.499520]\n",
            "3863 [D loss: 0.425848, acc.: 78.12%] [G loss: 1.475895]\n",
            "3864 [D loss: 0.514346, acc.: 75.00%] [G loss: 1.544626]\n",
            "3865 [D loss: 0.426964, acc.: 78.12%] [G loss: 1.215365]\n",
            "3866 [D loss: 0.462825, acc.: 76.56%] [G loss: 1.501116]\n",
            "3867 [D loss: 0.437938, acc.: 75.00%] [G loss: 1.549885]\n",
            "3868 [D loss: 0.494191, acc.: 78.12%] [G loss: 1.637745]\n",
            "3869 [D loss: 0.386077, acc.: 79.69%] [G loss: 1.732458]\n",
            "3870 [D loss: 0.474385, acc.: 76.56%] [G loss: 1.380352]\n",
            "3871 [D loss: 0.473122, acc.: 73.44%] [G loss: 1.307634]\n",
            "3872 [D loss: 0.422759, acc.: 79.69%] [G loss: 1.236810]\n",
            "3873 [D loss: 0.383832, acc.: 81.25%] [G loss: 1.618865]\n",
            "3874 [D loss: 0.467022, acc.: 78.12%] [G loss: 1.677281]\n",
            "3875 [D loss: 0.445710, acc.: 79.69%] [G loss: 1.381680]\n",
            "3876 [D loss: 0.469391, acc.: 76.56%] [G loss: 1.522046]\n",
            "3877 [D loss: 0.460768, acc.: 75.00%] [G loss: 1.213803]\n",
            "3878 [D loss: 0.437025, acc.: 78.12%] [G loss: 1.206630]\n",
            "3879 [D loss: 0.476967, acc.: 76.56%] [G loss: 1.582078]\n",
            "3880 [D loss: 0.394521, acc.: 79.69%] [G loss: 1.400687]\n",
            "3881 [D loss: 0.451277, acc.: 75.00%] [G loss: 1.492629]\n",
            "3882 [D loss: 0.512228, acc.: 76.56%] [G loss: 1.408615]\n",
            "3883 [D loss: 0.432417, acc.: 76.56%] [G loss: 1.363196]\n",
            "3884 [D loss: 0.543365, acc.: 73.44%] [G loss: 1.593457]\n",
            "3885 [D loss: 0.426338, acc.: 76.56%] [G loss: 2.174482]\n",
            "3886 [D loss: 0.564231, acc.: 73.44%] [G loss: 2.078860]\n",
            "3887 [D loss: 0.430794, acc.: 78.12%] [G loss: 2.180882]\n",
            "3888 [D loss: 0.509249, acc.: 73.44%] [G loss: 1.901922]\n",
            "3889 [D loss: 0.432784, acc.: 78.12%] [G loss: 1.200483]\n",
            "3890 [D loss: 0.480884, acc.: 75.00%] [G loss: 1.397119]\n",
            "3891 [D loss: 0.493874, acc.: 76.56%] [G loss: 1.421887]\n",
            "3892 [D loss: 0.409269, acc.: 76.56%] [G loss: 1.441754]\n",
            "3893 [D loss: 0.437648, acc.: 76.56%] [G loss: 1.458189]\n",
            "3894 [D loss: 0.430852, acc.: 76.56%] [G loss: 1.284648]\n",
            "3895 [D loss: 0.441611, acc.: 78.12%] [G loss: 1.346645]\n",
            "3896 [D loss: 0.469406, acc.: 78.12%] [G loss: 1.558107]\n",
            "3897 [D loss: 0.404226, acc.: 75.00%] [G loss: 1.897490]\n",
            "3898 [D loss: 0.437599, acc.: 75.00%] [G loss: 1.338730]\n",
            "3899 [D loss: 0.464705, acc.: 75.00%] [G loss: 1.554055]\n",
            "3900 [D loss: 0.403471, acc.: 76.56%] [G loss: 1.617528]\n",
            "generated_data\n",
            "3901 [D loss: 0.490216, acc.: 76.56%] [G loss: 1.317951]\n",
            "3902 [D loss: 0.411686, acc.: 79.69%] [G loss: 1.408908]\n",
            "3903 [D loss: 0.448400, acc.: 78.12%] [G loss: 1.708267]\n",
            "3904 [D loss: 0.433517, acc.: 79.69%] [G loss: 1.583636]\n",
            "3905 [D loss: 0.403329, acc.: 76.56%] [G loss: 1.981001]\n",
            "3906 [D loss: 0.476251, acc.: 76.56%] [G loss: 1.438976]\n",
            "3907 [D loss: 0.447951, acc.: 76.56%] [G loss: 1.410652]\n",
            "3908 [D loss: 0.408241, acc.: 79.69%] [G loss: 1.452951]\n",
            "3909 [D loss: 0.477891, acc.: 76.56%] [G loss: 1.464720]\n",
            "3910 [D loss: 0.425130, acc.: 79.69%] [G loss: 1.433194]\n",
            "3911 [D loss: 0.438522, acc.: 78.12%] [G loss: 1.595149]\n",
            "3912 [D loss: 0.426083, acc.: 79.69%] [G loss: 1.475826]\n",
            "3913 [D loss: 0.465627, acc.: 75.00%] [G loss: 1.721895]\n",
            "3914 [D loss: 0.446525, acc.: 79.69%] [G loss: 1.660048]\n",
            "3915 [D loss: 0.502804, acc.: 73.44%] [G loss: 1.559075]\n",
            "3916 [D loss: 0.494057, acc.: 79.69%] [G loss: 1.406123]\n",
            "3917 [D loss: 0.445887, acc.: 78.12%] [G loss: 1.995131]\n",
            "3918 [D loss: 0.517128, acc.: 76.56%] [G loss: 1.572834]\n",
            "3919 [D loss: 0.401863, acc.: 76.56%] [G loss: 1.423905]\n",
            "3920 [D loss: 0.429934, acc.: 78.12%] [G loss: 1.399481]\n",
            "3921 [D loss: 0.449861, acc.: 78.12%] [G loss: 1.572567]\n",
            "3922 [D loss: 0.495115, acc.: 75.00%] [G loss: 1.234745]\n",
            "3923 [D loss: 0.396822, acc.: 76.56%] [G loss: 1.553993]\n",
            "3924 [D loss: 0.380756, acc.: 78.12%] [G loss: 1.659060]\n",
            "3925 [D loss: 0.442328, acc.: 78.12%] [G loss: 1.316738]\n",
            "3926 [D loss: 0.408541, acc.: 78.12%] [G loss: 1.853683]\n",
            "3927 [D loss: 0.486175, acc.: 75.00%] [G loss: 1.389430]\n",
            "3928 [D loss: 0.420409, acc.: 76.56%] [G loss: 1.452496]\n",
            "3929 [D loss: 0.431764, acc.: 78.12%] [G loss: 1.391899]\n",
            "3930 [D loss: 0.456098, acc.: 78.12%] [G loss: 1.218079]\n",
            "3931 [D loss: 0.435273, acc.: 76.56%] [G loss: 1.704821]\n",
            "3932 [D loss: 0.377685, acc.: 79.69%] [G loss: 1.551185]\n",
            "3933 [D loss: 0.523266, acc.: 70.31%] [G loss: 1.464604]\n",
            "3934 [D loss: 0.446809, acc.: 78.12%] [G loss: 1.439676]\n",
            "3935 [D loss: 0.455250, acc.: 75.00%] [G loss: 1.513454]\n",
            "3936 [D loss: 0.425680, acc.: 76.56%] [G loss: 1.472391]\n",
            "3937 [D loss: 0.447417, acc.: 78.12%] [G loss: 1.368067]\n",
            "3938 [D loss: 0.423517, acc.: 76.56%] [G loss: 1.849587]\n",
            "3939 [D loss: 0.422385, acc.: 79.69%] [G loss: 1.495931]\n",
            "3940 [D loss: 0.433854, acc.: 79.69%] [G loss: 1.584835]\n",
            "3941 [D loss: 0.472422, acc.: 76.56%] [G loss: 1.718740]\n",
            "3942 [D loss: 0.434295, acc.: 76.56%] [G loss: 1.403170]\n",
            "3943 [D loss: 0.410320, acc.: 79.69%] [G loss: 1.622390]\n",
            "3944 [D loss: 0.389049, acc.: 79.69%] [G loss: 1.496479]\n",
            "3945 [D loss: 0.438552, acc.: 75.00%] [G loss: 1.379737]\n",
            "3946 [D loss: 0.415358, acc.: 76.56%] [G loss: 1.647250]\n",
            "3947 [D loss: 0.426365, acc.: 78.12%] [G loss: 1.553977]\n",
            "3948 [D loss: 0.449369, acc.: 79.69%] [G loss: 1.393798]\n",
            "3949 [D loss: 0.449473, acc.: 78.12%] [G loss: 1.453692]\n",
            "3950 [D loss: 0.382051, acc.: 79.69%] [G loss: 1.810564]\n",
            "3951 [D loss: 0.466367, acc.: 78.12%] [G loss: 1.557776]\n",
            "3952 [D loss: 0.414303, acc.: 78.12%] [G loss: 1.674357]\n",
            "3953 [D loss: 0.445704, acc.: 73.44%] [G loss: 1.526710]\n",
            "3954 [D loss: 0.439250, acc.: 76.56%] [G loss: 1.535716]\n",
            "3955 [D loss: 0.411438, acc.: 76.56%] [G loss: 2.057660]\n",
            "3956 [D loss: 0.492832, acc.: 73.44%] [G loss: 1.471112]\n",
            "3957 [D loss: 0.463751, acc.: 78.12%] [G loss: 1.391319]\n",
            "3958 [D loss: 0.446897, acc.: 78.12%] [G loss: 1.421365]\n",
            "3959 [D loss: 0.424792, acc.: 78.12%] [G loss: 1.651998]\n",
            "3960 [D loss: 0.420638, acc.: 78.12%] [G loss: 1.568776]\n",
            "3961 [D loss: 0.391919, acc.: 78.12%] [G loss: 1.553051]\n",
            "3962 [D loss: 0.449353, acc.: 75.00%] [G loss: 1.369730]\n",
            "3963 [D loss: 0.409343, acc.: 79.69%] [G loss: 1.679544]\n",
            "3964 [D loss: 0.430552, acc.: 79.69%] [G loss: 1.518907]\n",
            "3965 [D loss: 0.420481, acc.: 78.12%] [G loss: 1.716764]\n",
            "3966 [D loss: 0.432803, acc.: 76.56%] [G loss: 1.658808]\n",
            "3967 [D loss: 0.409861, acc.: 76.56%] [G loss: 1.527282]\n",
            "3968 [D loss: 0.467923, acc.: 73.44%] [G loss: 1.364908]\n",
            "3969 [D loss: 0.429882, acc.: 79.69%] [G loss: 1.347887]\n",
            "3970 [D loss: 0.437235, acc.: 75.00%] [G loss: 1.574650]\n",
            "3971 [D loss: 0.435710, acc.: 79.69%] [G loss: 1.539999]\n",
            "3972 [D loss: 0.459360, acc.: 71.88%] [G loss: 1.464707]\n",
            "3973 [D loss: 0.439599, acc.: 79.69%] [G loss: 1.674599]\n",
            "3974 [D loss: 0.454171, acc.: 78.12%] [G loss: 1.165221]\n",
            "3975 [D loss: 0.426712, acc.: 78.12%] [G loss: 1.573375]\n",
            "3976 [D loss: 0.510651, acc.: 73.44%] [G loss: 1.658208]\n",
            "3977 [D loss: 0.439528, acc.: 76.56%] [G loss: 1.611690]\n",
            "3978 [D loss: 0.453614, acc.: 76.56%] [G loss: 1.176293]\n",
            "3979 [D loss: 0.447905, acc.: 76.56%] [G loss: 1.430732]\n",
            "3980 [D loss: 0.391783, acc.: 79.69%] [G loss: 1.507012]\n",
            "3981 [D loss: 0.434538, acc.: 78.12%] [G loss: 1.586218]\n",
            "3982 [D loss: 0.491977, acc.: 73.44%] [G loss: 1.656193]\n",
            "3983 [D loss: 0.456621, acc.: 78.12%] [G loss: 1.679935]\n",
            "3984 [D loss: 0.444953, acc.: 78.12%] [G loss: 1.244582]\n",
            "3985 [D loss: 0.425781, acc.: 79.69%] [G loss: 1.450427]\n",
            "3986 [D loss: 0.425314, acc.: 76.56%] [G loss: 1.566586]\n",
            "3987 [D loss: 0.437391, acc.: 78.12%] [G loss: 1.372809]\n",
            "3988 [D loss: 0.436308, acc.: 81.25%] [G loss: 1.659524]\n",
            "3989 [D loss: 0.423277, acc.: 79.69%] [G loss: 1.803913]\n",
            "3990 [D loss: 0.419622, acc.: 79.69%] [G loss: 1.696528]\n",
            "3991 [D loss: 0.494197, acc.: 75.00%] [G loss: 1.822833]\n",
            "3992 [D loss: 0.384343, acc.: 79.69%] [G loss: 1.549715]\n",
            "3993 [D loss: 0.413325, acc.: 78.12%] [G loss: 1.368628]\n",
            "3994 [D loss: 0.419568, acc.: 79.69%] [G loss: 1.371831]\n",
            "3995 [D loss: 0.429270, acc.: 76.56%] [G loss: 1.473704]\n",
            "3996 [D loss: 0.383243, acc.: 79.69%] [G loss: 1.140301]\n",
            "3997 [D loss: 0.394605, acc.: 76.56%] [G loss: 1.553279]\n",
            "3998 [D loss: 0.454087, acc.: 75.00%] [G loss: 1.476152]\n",
            "3999 [D loss: 0.465743, acc.: 71.88%] [G loss: 1.604415]\n",
            "4000 [D loss: 0.432574, acc.: 75.00%] [G loss: 1.357935]\n",
            "generated_data\n",
            "4001 [D loss: 0.409706, acc.: 76.56%] [G loss: 1.373374]\n",
            "4002 [D loss: 0.431312, acc.: 78.12%] [G loss: 1.439587]\n",
            "4003 [D loss: 0.409950, acc.: 78.12%] [G loss: 1.333243]\n",
            "4004 [D loss: 0.444091, acc.: 79.69%] [G loss: 1.517540]\n",
            "4005 [D loss: 0.440133, acc.: 76.56%] [G loss: 1.745639]\n",
            "4006 [D loss: 0.457258, acc.: 78.12%] [G loss: 1.651606]\n",
            "4007 [D loss: 0.389398, acc.: 78.12%] [G loss: 1.674539]\n",
            "4008 [D loss: 0.385578, acc.: 79.69%] [G loss: 1.848630]\n",
            "4009 [D loss: 0.484871, acc.: 75.00%] [G loss: 1.526082]\n",
            "4010 [D loss: 0.413115, acc.: 78.12%] [G loss: 2.185486]\n",
            "4011 [D loss: 0.524574, acc.: 71.88%] [G loss: 1.558926]\n",
            "4012 [D loss: 0.401253, acc.: 79.69%] [G loss: 1.281211]\n",
            "4013 [D loss: 0.429789, acc.: 79.69%] [G loss: 1.365089]\n",
            "4014 [D loss: 0.450814, acc.: 75.00%] [G loss: 1.543353]\n",
            "4015 [D loss: 0.429823, acc.: 79.69%] [G loss: 1.253825]\n",
            "4016 [D loss: 0.412445, acc.: 79.69%] [G loss: 1.684058]\n",
            "4017 [D loss: 0.420319, acc.: 78.12%] [G loss: 1.762534]\n",
            "4018 [D loss: 0.400810, acc.: 81.25%] [G loss: 1.511815]\n",
            "4019 [D loss: 0.394636, acc.: 76.56%] [G loss: 1.474130]\n",
            "4020 [D loss: 0.409339, acc.: 78.12%] [G loss: 1.527452]\n",
            "4021 [D loss: 0.424981, acc.: 75.00%] [G loss: 1.353110]\n",
            "4022 [D loss: 0.518146, acc.: 76.56%] [G loss: 1.300881]\n",
            "4023 [D loss: 0.445660, acc.: 81.25%] [G loss: 1.354351]\n",
            "4024 [D loss: 0.468592, acc.: 78.12%] [G loss: 1.813782]\n",
            "4025 [D loss: 0.457476, acc.: 78.12%] [G loss: 1.435735]\n",
            "4026 [D loss: 0.437698, acc.: 78.12%] [G loss: 1.491426]\n",
            "4027 [D loss: 0.411337, acc.: 79.69%] [G loss: 1.556126]\n",
            "4028 [D loss: 0.438435, acc.: 78.12%] [G loss: 1.684951]\n",
            "4029 [D loss: 0.455071, acc.: 76.56%] [G loss: 1.456101]\n",
            "4030 [D loss: 0.421497, acc.: 78.12%] [G loss: 1.768559]\n",
            "4031 [D loss: 0.401502, acc.: 79.69%] [G loss: 1.490940]\n",
            "4032 [D loss: 0.469796, acc.: 75.00%] [G loss: 2.069595]\n",
            "4033 [D loss: 0.468820, acc.: 76.56%] [G loss: 1.539770]\n",
            "4034 [D loss: 0.468143, acc.: 78.12%] [G loss: 1.498399]\n",
            "4035 [D loss: 0.457558, acc.: 76.56%] [G loss: 1.583219]\n",
            "4036 [D loss: 0.435009, acc.: 76.56%] [G loss: 1.294487]\n",
            "4037 [D loss: 0.383683, acc.: 79.69%] [G loss: 1.259565]\n",
            "4038 [D loss: 0.440549, acc.: 76.56%] [G loss: 1.500166]\n",
            "4039 [D loss: 0.411260, acc.: 79.69%] [G loss: 1.312828]\n",
            "4040 [D loss: 0.479138, acc.: 76.56%] [G loss: 1.334910]\n",
            "4041 [D loss: 0.376263, acc.: 78.12%] [G loss: 1.726268]\n",
            "4042 [D loss: 0.426450, acc.: 78.12%] [G loss: 1.423500]\n",
            "4043 [D loss: 0.443098, acc.: 78.12%] [G loss: 1.382298]\n",
            "4044 [D loss: 0.379796, acc.: 79.69%] [G loss: 1.420752]\n",
            "4045 [D loss: 0.369702, acc.: 79.69%] [G loss: 1.553796]\n",
            "4046 [D loss: 0.527216, acc.: 75.00%] [G loss: 1.411849]\n",
            "4047 [D loss: 0.520769, acc.: 75.00%] [G loss: 1.429108]\n",
            "4048 [D loss: 0.463203, acc.: 79.69%] [G loss: 1.562945]\n",
            "4049 [D loss: 0.455866, acc.: 76.56%] [G loss: 1.506379]\n",
            "4050 [D loss: 0.445443, acc.: 78.12%] [G loss: 1.149716]\n",
            "4051 [D loss: 0.475089, acc.: 78.12%] [G loss: 1.423589]\n",
            "4052 [D loss: 0.417522, acc.: 79.69%] [G loss: 1.304785]\n",
            "4053 [D loss: 0.426552, acc.: 78.12%] [G loss: 1.264408]\n",
            "4054 [D loss: 0.389263, acc.: 79.69%] [G loss: 1.628376]\n",
            "4055 [D loss: 0.467733, acc.: 75.00%] [G loss: 1.428119]\n",
            "4056 [D loss: 0.446621, acc.: 78.12%] [G loss: 1.408485]\n",
            "4057 [D loss: 0.452834, acc.: 79.69%] [G loss: 1.444986]\n",
            "4058 [D loss: 0.432678, acc.: 75.00%] [G loss: 1.207988]\n",
            "4059 [D loss: 0.473712, acc.: 78.12%] [G loss: 1.441714]\n",
            "4060 [D loss: 0.439398, acc.: 75.00%] [G loss: 1.610500]\n",
            "4061 [D loss: 0.436712, acc.: 79.69%] [G loss: 1.808302]\n",
            "4062 [D loss: 0.537265, acc.: 75.00%] [G loss: 1.414667]\n",
            "4063 [D loss: 0.407480, acc.: 75.00%] [G loss: 1.688035]\n",
            "4064 [D loss: 0.468832, acc.: 79.69%] [G loss: 1.540965]\n",
            "4065 [D loss: 0.425177, acc.: 78.12%] [G loss: 1.611953]\n",
            "4066 [D loss: 0.444540, acc.: 76.56%] [G loss: 1.261973]\n",
            "4067 [D loss: 0.418224, acc.: 79.69%] [G loss: 1.486088]\n",
            "4068 [D loss: 0.434074, acc.: 76.56%] [G loss: 1.746058]\n",
            "4069 [D loss: 0.435694, acc.: 78.12%] [G loss: 1.490196]\n",
            "4070 [D loss: 0.431703, acc.: 76.56%] [G loss: 1.376085]\n",
            "4071 [D loss: 0.434628, acc.: 75.00%] [G loss: 1.388077]\n",
            "4072 [D loss: 0.404474, acc.: 79.69%] [G loss: 1.689953]\n",
            "4073 [D loss: 0.475973, acc.: 76.56%] [G loss: 1.664815]\n",
            "4074 [D loss: 0.424494, acc.: 78.12%] [G loss: 1.489104]\n",
            "4075 [D loss: 0.450444, acc.: 78.12%] [G loss: 1.501230]\n",
            "4076 [D loss: 0.452176, acc.: 76.56%] [G loss: 1.551656]\n",
            "4077 [D loss: 0.434452, acc.: 76.56%] [G loss: 1.481008]\n",
            "4078 [D loss: 0.467247, acc.: 75.00%] [G loss: 1.481576]\n",
            "4079 [D loss: 0.393578, acc.: 78.12%] [G loss: 1.476425]\n",
            "4080 [D loss: 0.462229, acc.: 78.12%] [G loss: 1.342266]\n",
            "4081 [D loss: 0.428337, acc.: 78.12%] [G loss: 1.666242]\n",
            "4082 [D loss: 0.384714, acc.: 79.69%] [G loss: 1.542998]\n",
            "4083 [D loss: 0.416615, acc.: 79.69%] [G loss: 1.569108]\n",
            "4084 [D loss: 0.423919, acc.: 78.12%] [G loss: 1.596319]\n",
            "4085 [D loss: 0.412882, acc.: 79.69%] [G loss: 1.389435]\n",
            "4086 [D loss: 0.430202, acc.: 78.12%] [G loss: 1.537910]\n",
            "4087 [D loss: 0.439842, acc.: 76.56%] [G loss: 1.255195]\n",
            "4088 [D loss: 0.448221, acc.: 75.00%] [G loss: 1.345112]\n",
            "4089 [D loss: 0.469859, acc.: 76.56%] [G loss: 1.677886]\n",
            "4090 [D loss: 0.465525, acc.: 76.56%] [G loss: 1.171663]\n",
            "4091 [D loss: 0.462303, acc.: 73.44%] [G loss: 1.336359]\n",
            "4092 [D loss: 0.406733, acc.: 79.69%] [G loss: 1.847799]\n",
            "4093 [D loss: 0.416102, acc.: 78.12%] [G loss: 1.415962]\n",
            "4094 [D loss: 0.429384, acc.: 76.56%] [G loss: 1.562115]\n",
            "4095 [D loss: 0.507510, acc.: 75.00%] [G loss: 1.583015]\n",
            "4096 [D loss: 0.430281, acc.: 78.12%] [G loss: 1.748754]\n",
            "4097 [D loss: 0.439427, acc.: 71.88%] [G loss: 1.545795]\n",
            "4098 [D loss: 0.429891, acc.: 79.69%] [G loss: 1.407002]\n",
            "4099 [D loss: 0.420632, acc.: 76.56%] [G loss: 1.439593]\n",
            "4100 [D loss: 0.490057, acc.: 71.88%] [G loss: 1.547672]\n",
            "generated_data\n",
            "4101 [D loss: 0.436839, acc.: 76.56%] [G loss: 1.409552]\n",
            "4102 [D loss: 0.422654, acc.: 76.56%] [G loss: 1.496239]\n",
            "4103 [D loss: 0.407108, acc.: 79.69%] [G loss: 1.383043]\n",
            "4104 [D loss: 0.429577, acc.: 79.69%] [G loss: 1.620754]\n",
            "4105 [D loss: 0.459960, acc.: 73.44%] [G loss: 1.468183]\n",
            "4106 [D loss: 0.406642, acc.: 79.69%] [G loss: 1.918931]\n",
            "4107 [D loss: 0.430621, acc.: 76.56%] [G loss: 1.875168]\n",
            "4108 [D loss: 0.492359, acc.: 79.69%] [G loss: 1.398366]\n",
            "4109 [D loss: 0.393888, acc.: 78.12%] [G loss: 1.690070]\n",
            "4110 [D loss: 0.448148, acc.: 79.69%] [G loss: 1.394021]\n",
            "4111 [D loss: 0.403559, acc.: 78.12%] [G loss: 1.257587]\n",
            "4112 [D loss: 0.417637, acc.: 79.69%] [G loss: 1.391443]\n",
            "4113 [D loss: 0.382122, acc.: 79.69%] [G loss: 1.380194]\n",
            "4114 [D loss: 0.426998, acc.: 76.56%] [G loss: 1.624276]\n",
            "4115 [D loss: 0.419824, acc.: 76.56%] [G loss: 1.555128]\n",
            "4116 [D loss: 0.485522, acc.: 76.56%] [G loss: 1.721805]\n",
            "4117 [D loss: 0.485933, acc.: 78.12%] [G loss: 1.787914]\n",
            "4118 [D loss: 0.453318, acc.: 79.69%] [G loss: 1.678613]\n",
            "4119 [D loss: 0.410146, acc.: 79.69%] [G loss: 1.702494]\n",
            "4120 [D loss: 0.412127, acc.: 78.12%] [G loss: 1.460338]\n",
            "4121 [D loss: 0.386365, acc.: 79.69%] [G loss: 1.732939]\n",
            "4122 [D loss: 0.496236, acc.: 76.56%] [G loss: 1.430192]\n",
            "4123 [D loss: 0.445262, acc.: 79.69%] [G loss: 1.394264]\n",
            "4124 [D loss: 0.405382, acc.: 79.69%] [G loss: 1.156296]\n",
            "4125 [D loss: 0.453938, acc.: 76.56%] [G loss: 1.243543]\n",
            "4126 [D loss: 0.422408, acc.: 78.12%] [G loss: 1.365606]\n",
            "4127 [D loss: 0.407617, acc.: 79.69%] [G loss: 1.284988]\n",
            "4128 [D loss: 0.428299, acc.: 76.56%] [G loss: 1.713264]\n",
            "4129 [D loss: 0.418797, acc.: 76.56%] [G loss: 1.449175]\n",
            "4130 [D loss: 0.452441, acc.: 76.56%] [G loss: 1.637821]\n",
            "4131 [D loss: 0.503279, acc.: 75.00%] [G loss: 1.839236]\n",
            "4132 [D loss: 0.498380, acc.: 73.44%] [G loss: 1.463242]\n",
            "4133 [D loss: 0.404059, acc.: 79.69%] [G loss: 1.752471]\n",
            "4134 [D loss: 0.391123, acc.: 79.69%] [G loss: 2.025452]\n",
            "4135 [D loss: 0.394979, acc.: 79.69%] [G loss: 1.589943]\n",
            "4136 [D loss: 0.455418, acc.: 79.69%] [G loss: 1.589777]\n",
            "4137 [D loss: 0.433502, acc.: 76.56%] [G loss: 1.509405]\n",
            "4138 [D loss: 0.424711, acc.: 79.69%] [G loss: 1.760600]\n",
            "4139 [D loss: 0.390488, acc.: 76.56%] [G loss: 1.605276]\n",
            "4140 [D loss: 0.437304, acc.: 76.56%] [G loss: 1.811582]\n",
            "4141 [D loss: 0.470171, acc.: 79.69%] [G loss: 1.650380]\n",
            "4142 [D loss: 0.398476, acc.: 79.69%] [G loss: 1.698262]\n",
            "4143 [D loss: 0.457353, acc.: 79.69%] [G loss: 1.441683]\n",
            "4144 [D loss: 0.473274, acc.: 78.12%] [G loss: 1.534849]\n",
            "4145 [D loss: 0.422064, acc.: 81.25%] [G loss: 1.673520]\n",
            "4146 [D loss: 0.402294, acc.: 78.12%] [G loss: 1.820755]\n",
            "4147 [D loss: 0.457693, acc.: 79.69%] [G loss: 1.397090]\n",
            "4148 [D loss: 0.393570, acc.: 79.69%] [G loss: 1.324617]\n",
            "4149 [D loss: 0.562285, acc.: 75.00%] [G loss: 1.279791]\n",
            "4150 [D loss: 0.397893, acc.: 79.69%] [G loss: 1.600547]\n",
            "4151 [D loss: 0.518513, acc.: 75.00%] [G loss: 1.383571]\n",
            "4152 [D loss: 0.473519, acc.: 78.12%] [G loss: 1.340776]\n",
            "4153 [D loss: 0.445582, acc.: 78.12%] [G loss: 1.606513]\n",
            "4154 [D loss: 0.446066, acc.: 78.12%] [G loss: 1.434808]\n",
            "4155 [D loss: 0.439278, acc.: 78.12%] [G loss: 1.592524]\n",
            "4156 [D loss: 0.404234, acc.: 79.69%] [G loss: 1.441371]\n",
            "4157 [D loss: 0.417280, acc.: 78.12%] [G loss: 1.800939]\n",
            "4158 [D loss: 0.417741, acc.: 76.56%] [G loss: 1.777591]\n",
            "4159 [D loss: 0.391095, acc.: 79.69%] [G loss: 1.463539]\n",
            "4160 [D loss: 0.385098, acc.: 78.12%] [G loss: 1.835374]\n",
            "4161 [D loss: 0.411720, acc.: 78.12%] [G loss: 2.024084]\n",
            "4162 [D loss: 0.421777, acc.: 78.12%] [G loss: 1.668157]\n",
            "4163 [D loss: 0.427717, acc.: 75.00%] [G loss: 1.779941]\n",
            "4164 [D loss: 0.411617, acc.: 78.12%] [G loss: 1.590699]\n",
            "4165 [D loss: 0.459491, acc.: 78.12%] [G loss: 1.456388]\n",
            "4166 [D loss: 0.381774, acc.: 78.12%] [G loss: 1.847400]\n",
            "4167 [D loss: 0.465618, acc.: 78.12%] [G loss: 1.307877]\n",
            "4168 [D loss: 0.414958, acc.: 76.56%] [G loss: 1.720426]\n",
            "4169 [D loss: 0.414206, acc.: 75.00%] [G loss: 1.831330]\n",
            "4170 [D loss: 0.483531, acc.: 76.56%] [G loss: 1.717964]\n",
            "4171 [D loss: 0.434376, acc.: 75.00%] [G loss: 1.434608]\n",
            "4172 [D loss: 0.390054, acc.: 79.69%] [G loss: 1.637779]\n",
            "4173 [D loss: 0.413756, acc.: 79.69%] [G loss: 1.617911]\n",
            "4174 [D loss: 0.418496, acc.: 79.69%] [G loss: 1.486307]\n",
            "4175 [D loss: 0.443091, acc.: 76.56%] [G loss: 1.589814]\n",
            "4176 [D loss: 0.444725, acc.: 75.00%] [G loss: 1.324368]\n",
            "4177 [D loss: 0.466847, acc.: 78.12%] [G loss: 1.740939]\n",
            "4178 [D loss: 0.361444, acc.: 78.12%] [G loss: 2.001702]\n",
            "4179 [D loss: 0.412192, acc.: 79.69%] [G loss: 1.533403]\n",
            "4180 [D loss: 0.399078, acc.: 78.12%] [G loss: 1.895471]\n",
            "4181 [D loss: 0.480263, acc.: 76.56%] [G loss: 1.658871]\n",
            "4182 [D loss: 0.425074, acc.: 78.12%] [G loss: 1.757826]\n",
            "4183 [D loss: 0.483261, acc.: 78.12%] [G loss: 1.541954]\n",
            "4184 [D loss: 0.429801, acc.: 78.12%] [G loss: 1.682651]\n",
            "4185 [D loss: 0.454070, acc.: 78.12%] [G loss: 1.368642]\n",
            "4186 [D loss: 0.465670, acc.: 75.00%] [G loss: 1.406288]\n",
            "4187 [D loss: 0.512474, acc.: 78.12%] [G loss: 1.576378]\n",
            "4188 [D loss: 0.439995, acc.: 75.00%] [G loss: 1.775901]\n",
            "4189 [D loss: 0.468633, acc.: 76.56%] [G loss: 1.320329]\n",
            "4190 [D loss: 0.482422, acc.: 76.56%] [G loss: 1.357944]\n",
            "4191 [D loss: 0.423904, acc.: 78.12%] [G loss: 1.216652]\n",
            "4192 [D loss: 0.383033, acc.: 78.12%] [G loss: 1.674617]\n",
            "4193 [D loss: 0.417134, acc.: 76.56%] [G loss: 1.378869]\n",
            "4194 [D loss: 0.432155, acc.: 78.12%] [G loss: 1.798454]\n",
            "4195 [D loss: 0.440005, acc.: 78.12%] [G loss: 1.993183]\n",
            "4196 [D loss: 0.464185, acc.: 78.12%] [G loss: 1.695501]\n",
            "4197 [D loss: 0.503002, acc.: 78.12%] [G loss: 1.422358]\n",
            "4198 [D loss: 0.407810, acc.: 79.69%] [G loss: 1.399983]\n",
            "4199 [D loss: 0.430700, acc.: 78.12%] [G loss: 1.644688]\n",
            "4200 [D loss: 0.412027, acc.: 79.69%] [G loss: 1.225891]\n",
            "generated_data\n",
            "4201 [D loss: 0.392067, acc.: 78.12%] [G loss: 1.540990]\n",
            "4202 [D loss: 0.480460, acc.: 78.12%] [G loss: 1.367329]\n",
            "4203 [D loss: 0.442690, acc.: 78.12%] [G loss: 1.394490]\n",
            "4204 [D loss: 0.394197, acc.: 78.12%] [G loss: 1.664033]\n",
            "4205 [D loss: 0.464217, acc.: 78.12%] [G loss: 1.510402]\n",
            "4206 [D loss: 0.410124, acc.: 78.12%] [G loss: 1.651517]\n",
            "4207 [D loss: 0.480541, acc.: 75.00%] [G loss: 1.724830]\n",
            "4208 [D loss: 0.474215, acc.: 78.12%] [G loss: 1.685948]\n",
            "4209 [D loss: 0.433635, acc.: 76.56%] [G loss: 1.757576]\n",
            "4210 [D loss: 0.487852, acc.: 76.56%] [G loss: 1.510368]\n",
            "4211 [D loss: 0.444823, acc.: 75.00%] [G loss: 1.529980]\n",
            "4212 [D loss: 0.461670, acc.: 78.12%] [G loss: 1.431955]\n",
            "4213 [D loss: 0.411474, acc.: 81.25%] [G loss: 1.180726]\n",
            "4214 [D loss: 0.460030, acc.: 76.56%] [G loss: 1.724282]\n",
            "4215 [D loss: 0.454410, acc.: 76.56%] [G loss: 1.439240]\n",
            "4216 [D loss: 0.489806, acc.: 70.31%] [G loss: 1.410656]\n",
            "4217 [D loss: 0.425625, acc.: 76.56%] [G loss: 1.288394]\n",
            "4218 [D loss: 0.407146, acc.: 79.69%] [G loss: 1.878283]\n",
            "4219 [D loss: 0.451316, acc.: 78.12%] [G loss: 1.593724]\n",
            "4220 [D loss: 0.461258, acc.: 73.44%] [G loss: 1.310164]\n",
            "4221 [D loss: 0.483293, acc.: 76.56%] [G loss: 1.583607]\n",
            "4222 [D loss: 0.427819, acc.: 78.12%] [G loss: 1.872630]\n",
            "4223 [D loss: 0.482737, acc.: 76.56%] [G loss: 1.564078]\n",
            "4224 [D loss: 0.431483, acc.: 76.56%] [G loss: 1.504950]\n",
            "4225 [D loss: 0.424468, acc.: 78.12%] [G loss: 1.429962]\n",
            "4226 [D loss: 0.451571, acc.: 76.56%] [G loss: 1.351792]\n",
            "4227 [D loss: 0.428533, acc.: 79.69%] [G loss: 1.396632]\n",
            "4228 [D loss: 0.409273, acc.: 79.69%] [G loss: 1.763650]\n",
            "4229 [D loss: 0.448592, acc.: 75.00%] [G loss: 1.423768]\n",
            "4230 [D loss: 0.404712, acc.: 78.12%] [G loss: 1.698218]\n",
            "4231 [D loss: 0.482783, acc.: 78.12%] [G loss: 1.399778]\n",
            "4232 [D loss: 0.442933, acc.: 73.44%] [G loss: 1.558256]\n",
            "4233 [D loss: 0.415067, acc.: 75.00%] [G loss: 1.463576]\n",
            "4234 [D loss: 0.454706, acc.: 81.25%] [G loss: 1.465296]\n",
            "4235 [D loss: 0.354528, acc.: 79.69%] [G loss: 1.803018]\n",
            "4236 [D loss: 0.421424, acc.: 76.56%] [G loss: 1.448117]\n",
            "4237 [D loss: 0.466141, acc.: 75.00%] [G loss: 1.466988]\n",
            "4238 [D loss: 0.382031, acc.: 76.56%] [G loss: 1.904860]\n",
            "4239 [D loss: 0.454069, acc.: 78.12%] [G loss: 2.087772]\n",
            "4240 [D loss: 0.439313, acc.: 79.69%] [G loss: 1.893111]\n",
            "4241 [D loss: 0.540272, acc.: 75.00%] [G loss: 1.334968]\n",
            "4242 [D loss: 0.450230, acc.: 78.12%] [G loss: 1.313289]\n",
            "4243 [D loss: 0.438228, acc.: 81.25%] [G loss: 1.292778]\n",
            "4244 [D loss: 0.521564, acc.: 71.88%] [G loss: 1.225136]\n",
            "4245 [D loss: 0.434872, acc.: 75.00%] [G loss: 1.574939]\n",
            "4246 [D loss: 0.426720, acc.: 78.12%] [G loss: 1.361229]\n",
            "4247 [D loss: 0.468381, acc.: 76.56%] [G loss: 1.835721]\n",
            "4248 [D loss: 0.405096, acc.: 78.12%] [G loss: 1.773337]\n",
            "4249 [D loss: 0.479983, acc.: 78.12%] [G loss: 1.341654]\n",
            "4250 [D loss: 0.455689, acc.: 76.56%] [G loss: 1.710585]\n",
            "4251 [D loss: 0.432236, acc.: 78.12%] [G loss: 1.891741]\n",
            "4252 [D loss: 0.412872, acc.: 76.56%] [G loss: 1.816061]\n",
            "4253 [D loss: 0.398811, acc.: 78.12%] [G loss: 1.906227]\n",
            "4254 [D loss: 0.514238, acc.: 73.44%] [G loss: 1.353513]\n",
            "4255 [D loss: 0.484096, acc.: 78.12%] [G loss: 1.589275]\n",
            "4256 [D loss: 0.453069, acc.: 76.56%] [G loss: 1.455370]\n",
            "4257 [D loss: 0.399419, acc.: 78.12%] [G loss: 1.604066]\n",
            "4258 [D loss: 0.417630, acc.: 78.12%] [G loss: 1.467671]\n",
            "4259 [D loss: 0.388063, acc.: 78.12%] [G loss: 1.760050]\n",
            "4260 [D loss: 0.454367, acc.: 75.00%] [G loss: 1.515540]\n",
            "4261 [D loss: 0.521030, acc.: 73.44%] [G loss: 1.259864]\n",
            "4262 [D loss: 0.370703, acc.: 79.69%] [G loss: 1.521038]\n",
            "4263 [D loss: 0.402620, acc.: 78.12%] [G loss: 1.351795]\n",
            "4264 [D loss: 0.411733, acc.: 76.56%] [G loss: 1.824598]\n",
            "4265 [D loss: 0.393794, acc.: 81.25%] [G loss: 1.789714]\n",
            "4266 [D loss: 0.431294, acc.: 78.12%] [G loss: 1.654058]\n",
            "4267 [D loss: 0.398690, acc.: 76.56%] [G loss: 1.846692]\n",
            "4268 [D loss: 0.432123, acc.: 79.69%] [G loss: 1.546904]\n",
            "4269 [D loss: 0.432079, acc.: 76.56%] [G loss: 1.812928]\n",
            "4270 [D loss: 0.440251, acc.: 75.00%] [G loss: 1.363546]\n",
            "4271 [D loss: 0.418909, acc.: 79.69%] [G loss: 1.642457]\n",
            "4272 [D loss: 0.483316, acc.: 75.00%] [G loss: 1.890663]\n",
            "4273 [D loss: 0.504995, acc.: 75.00%] [G loss: 1.441941]\n",
            "4274 [D loss: 0.424755, acc.: 78.12%] [G loss: 1.868394]\n",
            "4275 [D loss: 0.468995, acc.: 76.56%] [G loss: 1.502103]\n",
            "4276 [D loss: 0.409523, acc.: 78.12%] [G loss: 1.510763]\n",
            "4277 [D loss: 0.412629, acc.: 81.25%] [G loss: 1.481218]\n",
            "4278 [D loss: 0.389375, acc.: 79.69%] [G loss: 1.783164]\n",
            "4279 [D loss: 0.445720, acc.: 76.56%] [G loss: 1.468810]\n",
            "4280 [D loss: 0.466975, acc.: 76.56%] [G loss: 1.774651]\n",
            "4281 [D loss: 0.485031, acc.: 76.56%] [G loss: 1.916704]\n",
            "4282 [D loss: 0.524654, acc.: 71.88%] [G loss: 1.616304]\n",
            "4283 [D loss: 0.430266, acc.: 76.56%] [G loss: 1.613405]\n",
            "4284 [D loss: 0.479903, acc.: 78.12%] [G loss: 1.487826]\n",
            "4285 [D loss: 0.399965, acc.: 79.69%] [G loss: 1.493707]\n",
            "4286 [D loss: 0.410908, acc.: 79.69%] [G loss: 1.891085]\n",
            "4287 [D loss: 0.408396, acc.: 79.69%] [G loss: 1.263414]\n",
            "4288 [D loss: 0.393133, acc.: 78.12%] [G loss: 2.148949]\n",
            "4289 [D loss: 0.434809, acc.: 79.69%] [G loss: 1.571962]\n",
            "4290 [D loss: 0.369303, acc.: 79.69%] [G loss: 1.733423]\n",
            "4291 [D loss: 0.354063, acc.: 81.25%] [G loss: 1.747455]\n",
            "4292 [D loss: 0.410650, acc.: 79.69%] [G loss: 1.369838]\n",
            "4293 [D loss: 0.377173, acc.: 79.69%] [G loss: 1.938660]\n",
            "4294 [D loss: 0.487135, acc.: 78.12%] [G loss: 1.601354]\n",
            "4295 [D loss: 0.393776, acc.: 79.69%] [G loss: 1.551362]\n",
            "4296 [D loss: 0.415787, acc.: 76.56%] [G loss: 1.791918]\n",
            "4297 [D loss: 0.423173, acc.: 79.69%] [G loss: 1.263768]\n",
            "4298 [D loss: 0.398175, acc.: 78.12%] [G loss: 2.037389]\n",
            "4299 [D loss: 0.417334, acc.: 78.12%] [G loss: 1.419066]\n",
            "4300 [D loss: 0.413916, acc.: 78.12%] [G loss: 1.917162]\n",
            "generated_data\n",
            "4301 [D loss: 0.406240, acc.: 76.56%] [G loss: 1.922778]\n",
            "4302 [D loss: 0.400387, acc.: 79.69%] [G loss: 1.284896]\n",
            "4303 [D loss: 0.403900, acc.: 81.25%] [G loss: 1.599456]\n",
            "4304 [D loss: 0.472802, acc.: 76.56%] [G loss: 1.463201]\n",
            "4305 [D loss: 0.435016, acc.: 78.12%] [G loss: 1.362276]\n",
            "4306 [D loss: 0.418188, acc.: 81.25%] [G loss: 1.540502]\n",
            "4307 [D loss: 0.417193, acc.: 79.69%] [G loss: 1.507658]\n",
            "4308 [D loss: 0.411429, acc.: 79.69%] [G loss: 1.580047]\n",
            "4309 [D loss: 0.443098, acc.: 78.12%] [G loss: 2.059623]\n",
            "4310 [D loss: 0.416292, acc.: 76.56%] [G loss: 2.032518]\n",
            "4311 [D loss: 0.436072, acc.: 79.69%] [G loss: 1.526019]\n",
            "4312 [D loss: 0.373703, acc.: 81.25%] [G loss: 1.954813]\n",
            "4313 [D loss: 0.433211, acc.: 76.56%] [G loss: 1.281708]\n",
            "4314 [D loss: 0.376148, acc.: 78.12%] [G loss: 1.870865]\n",
            "4315 [D loss: 0.415129, acc.: 78.12%] [G loss: 1.690453]\n",
            "4316 [D loss: 0.532066, acc.: 75.00%] [G loss: 1.694517]\n",
            "4317 [D loss: 0.402611, acc.: 79.69%] [G loss: 1.631279]\n",
            "4318 [D loss: 0.383534, acc.: 79.69%] [G loss: 1.641176]\n",
            "4319 [D loss: 0.420355, acc.: 79.69%] [G loss: 1.627311]\n",
            "4320 [D loss: 0.433888, acc.: 79.69%] [G loss: 1.513352]\n",
            "4321 [D loss: 0.410100, acc.: 78.12%] [G loss: 1.314575]\n",
            "4322 [D loss: 0.380605, acc.: 79.69%] [G loss: 1.573831]\n",
            "4323 [D loss: 0.515062, acc.: 70.31%] [G loss: 1.664292]\n",
            "4324 [D loss: 0.453679, acc.: 78.12%] [G loss: 1.359098]\n",
            "4325 [D loss: 0.396664, acc.: 79.69%] [G loss: 1.886622]\n",
            "4326 [D loss: 0.457131, acc.: 78.12%] [G loss: 1.257281]\n",
            "4327 [D loss: 0.429381, acc.: 76.56%] [G loss: 1.403475]\n",
            "4328 [D loss: 0.434048, acc.: 79.69%] [G loss: 1.563108]\n",
            "4329 [D loss: 0.418809, acc.: 78.12%] [G loss: 1.662232]\n",
            "4330 [D loss: 0.450270, acc.: 75.00%] [G loss: 1.322747]\n",
            "4331 [D loss: 0.382196, acc.: 78.12%] [G loss: 1.559714]\n",
            "4332 [D loss: 0.493474, acc.: 71.88%] [G loss: 1.482595]\n",
            "4333 [D loss: 0.443534, acc.: 76.56%] [G loss: 1.407838]\n",
            "4334 [D loss: 0.416793, acc.: 76.56%] [G loss: 1.475318]\n",
            "4335 [D loss: 0.476210, acc.: 78.12%] [G loss: 1.432842]\n",
            "4336 [D loss: 0.418308, acc.: 79.69%] [G loss: 1.634888]\n",
            "4337 [D loss: 0.469421, acc.: 79.69%] [G loss: 1.314455]\n",
            "4338 [D loss: 0.452435, acc.: 76.56%] [G loss: 1.397408]\n",
            "4339 [D loss: 0.431561, acc.: 75.00%] [G loss: 1.447728]\n",
            "4340 [D loss: 0.409166, acc.: 79.69%] [G loss: 1.897955]\n",
            "4341 [D loss: 0.437544, acc.: 78.12%] [G loss: 1.243558]\n",
            "4342 [D loss: 0.457177, acc.: 79.69%] [G loss: 1.383669]\n",
            "4343 [D loss: 0.433485, acc.: 78.12%] [G loss: 1.638981]\n",
            "4344 [D loss: 0.410171, acc.: 79.69%] [G loss: 1.292463]\n",
            "4345 [D loss: 0.434017, acc.: 78.12%] [G loss: 1.391629]\n",
            "4346 [D loss: 0.442260, acc.: 78.12%] [G loss: 1.380479]\n",
            "4347 [D loss: 0.467909, acc.: 76.56%] [G loss: 1.499408]\n",
            "4348 [D loss: 0.403888, acc.: 78.12%] [G loss: 1.414856]\n",
            "4349 [D loss: 0.407020, acc.: 78.12%] [G loss: 1.680504]\n",
            "4350 [D loss: 0.408015, acc.: 79.69%] [G loss: 1.605509]\n",
            "4351 [D loss: 0.441228, acc.: 78.12%] [G loss: 1.756137]\n",
            "4352 [D loss: 0.485710, acc.: 75.00%] [G loss: 1.621285]\n",
            "4353 [D loss: 0.431342, acc.: 78.12%] [G loss: 1.517292]\n",
            "4354 [D loss: 0.425231, acc.: 79.69%] [G loss: 1.369617]\n",
            "4355 [D loss: 0.460240, acc.: 76.56%] [G loss: 1.532359]\n",
            "4356 [D loss: 0.438287, acc.: 76.56%] [G loss: 1.675946]\n",
            "4357 [D loss: 0.430347, acc.: 78.12%] [G loss: 1.355041]\n",
            "4358 [D loss: 0.456692, acc.: 76.56%] [G loss: 1.585784]\n",
            "4359 [D loss: 0.456194, acc.: 76.56%] [G loss: 1.392611]\n",
            "4360 [D loss: 0.383283, acc.: 78.12%] [G loss: 1.719948]\n",
            "4361 [D loss: 0.441363, acc.: 75.00%] [G loss: 1.646635]\n",
            "4362 [D loss: 0.424936, acc.: 79.69%] [G loss: 1.360524]\n",
            "4363 [D loss: 0.478836, acc.: 78.12%] [G loss: 1.603433]\n",
            "4364 [D loss: 0.414556, acc.: 78.12%] [G loss: 1.284732]\n",
            "4365 [D loss: 0.469829, acc.: 75.00%] [G loss: 1.491685]\n",
            "4366 [D loss: 0.388957, acc.: 79.69%] [G loss: 2.018219]\n",
            "4367 [D loss: 0.393861, acc.: 81.25%] [G loss: 1.625192]\n",
            "4368 [D loss: 0.368112, acc.: 78.12%] [G loss: 2.152977]\n",
            "4369 [D loss: 0.406731, acc.: 79.69%] [G loss: 1.366602]\n",
            "4370 [D loss: 0.405690, acc.: 78.12%] [G loss: 1.232626]\n",
            "4371 [D loss: 0.463068, acc.: 75.00%] [G loss: 1.534885]\n",
            "4372 [D loss: 0.448613, acc.: 76.56%] [G loss: 1.367698]\n",
            "4373 [D loss: 0.429172, acc.: 78.12%] [G loss: 1.720864]\n",
            "4374 [D loss: 0.464237, acc.: 75.00%] [G loss: 1.365720]\n",
            "4375 [D loss: 0.404757, acc.: 78.12%] [G loss: 1.689680]\n",
            "4376 [D loss: 0.411571, acc.: 79.69%] [G loss: 1.547103]\n",
            "4377 [D loss: 0.453103, acc.: 76.56%] [G loss: 1.562815]\n",
            "4378 [D loss: 0.422370, acc.: 76.56%] [G loss: 1.696407]\n",
            "4379 [D loss: 0.459988, acc.: 75.00%] [G loss: 1.496114]\n",
            "4380 [D loss: 0.424058, acc.: 79.69%] [G loss: 1.462804]\n",
            "4381 [D loss: 0.414983, acc.: 79.69%] [G loss: 1.410649]\n",
            "4382 [D loss: 0.411865, acc.: 78.12%] [G loss: 1.417598]\n",
            "4383 [D loss: 0.525188, acc.: 78.12%] [G loss: 1.239452]\n",
            "4384 [D loss: 0.388424, acc.: 79.69%] [G loss: 1.774933]\n",
            "4385 [D loss: 0.550676, acc.: 73.44%] [G loss: 1.323088]\n",
            "4386 [D loss: 0.398631, acc.: 78.12%] [G loss: 1.474471]\n",
            "4387 [D loss: 0.439577, acc.: 76.56%] [G loss: 1.408290]\n",
            "4388 [D loss: 0.477794, acc.: 76.56%] [G loss: 1.402134]\n",
            "4389 [D loss: 0.430340, acc.: 79.69%] [G loss: 1.277605]\n",
            "4390 [D loss: 0.460170, acc.: 78.12%] [G loss: 1.596435]\n",
            "4391 [D loss: 0.436110, acc.: 75.00%] [G loss: 2.083313]\n",
            "4392 [D loss: 0.402002, acc.: 76.56%] [G loss: 1.674797]\n",
            "4393 [D loss: 0.413094, acc.: 78.12%] [G loss: 1.693799]\n",
            "4394 [D loss: 0.534474, acc.: 75.00%] [G loss: 1.460298]\n",
            "4395 [D loss: 0.381688, acc.: 78.12%] [G loss: 1.349606]\n",
            "4396 [D loss: 0.444389, acc.: 76.56%] [G loss: 1.537083]\n",
            "4397 [D loss: 0.455722, acc.: 79.69%] [G loss: 1.474795]\n",
            "4398 [D loss: 0.418774, acc.: 76.56%] [G loss: 1.553286]\n",
            "4399 [D loss: 0.438039, acc.: 78.12%] [G loss: 1.498668]\n",
            "4400 [D loss: 0.363753, acc.: 81.25%] [G loss: 1.829608]\n",
            "generated_data\n",
            "4401 [D loss: 0.458180, acc.: 78.12%] [G loss: 1.540961]\n",
            "4402 [D loss: 0.417090, acc.: 78.12%] [G loss: 1.702601]\n",
            "4403 [D loss: 0.404509, acc.: 79.69%] [G loss: 1.678973]\n",
            "4404 [D loss: 0.460960, acc.: 78.12%] [G loss: 1.698703]\n",
            "4405 [D loss: 0.439967, acc.: 79.69%] [G loss: 1.701075]\n",
            "4406 [D loss: 0.394874, acc.: 79.69%] [G loss: 1.671933]\n",
            "4407 [D loss: 0.424901, acc.: 79.69%] [G loss: 1.364782]\n",
            "4408 [D loss: 0.422968, acc.: 78.12%] [G loss: 1.720237]\n",
            "4409 [D loss: 0.499120, acc.: 76.56%] [G loss: 1.631217]\n",
            "4410 [D loss: 0.448195, acc.: 79.69%] [G loss: 1.325061]\n",
            "4411 [D loss: 0.393413, acc.: 79.69%] [G loss: 1.603101]\n",
            "4412 [D loss: 0.397656, acc.: 78.12%] [G loss: 1.520043]\n",
            "4413 [D loss: 0.390828, acc.: 78.12%] [G loss: 1.576412]\n",
            "4414 [D loss: 0.468452, acc.: 76.56%] [G loss: 1.493953]\n",
            "4415 [D loss: 0.416764, acc.: 76.56%] [G loss: 1.567971]\n",
            "4416 [D loss: 0.579694, acc.: 75.00%] [G loss: 2.023795]\n",
            "4417 [D loss: 0.478442, acc.: 76.56%] [G loss: 1.521837]\n",
            "4418 [D loss: 0.448477, acc.: 79.69%] [G loss: 1.511048]\n",
            "4419 [D loss: 0.461881, acc.: 78.12%] [G loss: 1.444940]\n",
            "4420 [D loss: 0.462430, acc.: 76.56%] [G loss: 1.698130]\n",
            "4421 [D loss: 0.418227, acc.: 78.12%] [G loss: 1.555743]\n",
            "4422 [D loss: 0.450175, acc.: 76.56%] [G loss: 1.608663]\n",
            "4423 [D loss: 0.436678, acc.: 78.12%] [G loss: 1.164483]\n",
            "4424 [D loss: 0.412902, acc.: 78.12%] [G loss: 1.618718]\n",
            "4425 [D loss: 0.426302, acc.: 79.69%] [G loss: 1.356391]\n",
            "4426 [D loss: 0.426970, acc.: 78.12%] [G loss: 1.659250]\n",
            "4427 [D loss: 0.474777, acc.: 75.00%] [G loss: 1.736656]\n",
            "4428 [D loss: 0.395806, acc.: 79.69%] [G loss: 1.739583]\n",
            "4429 [D loss: 0.433249, acc.: 76.56%] [G loss: 1.885628]\n",
            "4430 [D loss: 0.430072, acc.: 78.12%] [G loss: 1.354494]\n",
            "4431 [D loss: 0.400229, acc.: 79.69%] [G loss: 1.648388]\n",
            "4432 [D loss: 0.463824, acc.: 76.56%] [G loss: 1.643265]\n",
            "4433 [D loss: 0.404863, acc.: 79.69%] [G loss: 1.598522]\n",
            "4434 [D loss: 0.478804, acc.: 75.00%] [G loss: 1.490410]\n",
            "4435 [D loss: 0.370864, acc.: 79.69%] [G loss: 1.586583]\n",
            "4436 [D loss: 0.389587, acc.: 78.12%] [G loss: 1.602118]\n",
            "4437 [D loss: 0.433921, acc.: 79.69%] [G loss: 1.338030]\n",
            "4438 [D loss: 0.421003, acc.: 81.25%] [G loss: 1.258239]\n",
            "4439 [D loss: 0.396660, acc.: 79.69%] [G loss: 1.879116]\n",
            "4440 [D loss: 0.410832, acc.: 79.69%] [G loss: 1.468412]\n",
            "4441 [D loss: 0.445013, acc.: 81.25%] [G loss: 1.311768]\n",
            "4442 [D loss: 0.425606, acc.: 76.56%] [G loss: 1.563457]\n",
            "4443 [D loss: 0.450459, acc.: 76.56%] [G loss: 1.388076]\n",
            "4444 [D loss: 0.378998, acc.: 79.69%] [G loss: 1.605054]\n",
            "4445 [D loss: 0.359624, acc.: 81.25%] [G loss: 1.683743]\n",
            "4446 [D loss: 0.401862, acc.: 79.69%] [G loss: 1.725846]\n",
            "4447 [D loss: 0.395659, acc.: 78.12%] [G loss: 1.867643]\n",
            "4448 [D loss: 0.411403, acc.: 76.56%] [G loss: 1.900818]\n",
            "4449 [D loss: 0.545660, acc.: 71.88%] [G loss: 1.662217]\n",
            "4450 [D loss: 0.393446, acc.: 79.69%] [G loss: 1.554768]\n",
            "4451 [D loss: 0.452662, acc.: 78.12%] [G loss: 1.535559]\n",
            "4452 [D loss: 0.415008, acc.: 79.69%] [G loss: 1.676643]\n",
            "4453 [D loss: 0.418241, acc.: 78.12%] [G loss: 1.737873]\n",
            "4454 [D loss: 0.412148, acc.: 75.00%] [G loss: 1.543652]\n",
            "4455 [D loss: 0.394828, acc.: 79.69%] [G loss: 1.818832]\n",
            "4456 [D loss: 0.389897, acc.: 78.12%] [G loss: 1.638405]\n",
            "4457 [D loss: 0.428762, acc.: 76.56%] [G loss: 1.761708]\n",
            "4458 [D loss: 0.498944, acc.: 73.44%] [G loss: 1.297165]\n",
            "4459 [D loss: 0.465164, acc.: 75.00%] [G loss: 1.782079]\n",
            "4460 [D loss: 0.442180, acc.: 76.56%] [G loss: 1.704942]\n",
            "4461 [D loss: 0.459270, acc.: 79.69%] [G loss: 1.321351]\n",
            "4462 [D loss: 0.391302, acc.: 78.12%] [G loss: 1.786670]\n",
            "4463 [D loss: 0.382750, acc.: 79.69%] [G loss: 1.549022]\n",
            "4464 [D loss: 0.438919, acc.: 76.56%] [G loss: 1.877322]\n",
            "4465 [D loss: 0.361411, acc.: 78.12%] [G loss: 1.643239]\n",
            "4466 [D loss: 0.462788, acc.: 78.12%] [G loss: 1.312636]\n",
            "4467 [D loss: 0.514205, acc.: 73.44%] [G loss: 1.551982]\n",
            "4468 [D loss: 0.407779, acc.: 75.00%] [G loss: 1.788447]\n",
            "4469 [D loss: 0.415212, acc.: 79.69%] [G loss: 1.633873]\n",
            "4470 [D loss: 0.515048, acc.: 76.56%] [G loss: 1.455429]\n",
            "4471 [D loss: 0.426062, acc.: 78.12%] [G loss: 1.621396]\n",
            "4472 [D loss: 0.418414, acc.: 78.12%] [G loss: 1.513699]\n",
            "4473 [D loss: 0.399620, acc.: 79.69%] [G loss: 1.566134]\n",
            "4474 [D loss: 0.421202, acc.: 78.12%] [G loss: 1.616830]\n",
            "4475 [D loss: 0.451463, acc.: 76.56%] [G loss: 1.619966]\n",
            "4476 [D loss: 0.402248, acc.: 78.12%] [G loss: 1.310991]\n",
            "4477 [D loss: 0.435259, acc.: 78.12%] [G loss: 1.330611]\n",
            "4478 [D loss: 0.418964, acc.: 78.12%] [G loss: 1.650514]\n",
            "4479 [D loss: 0.440314, acc.: 75.00%] [G loss: 1.788367]\n",
            "4480 [D loss: 0.479903, acc.: 75.00%] [G loss: 1.477411]\n",
            "4481 [D loss: 0.408405, acc.: 78.12%] [G loss: 1.540664]\n",
            "4482 [D loss: 0.403738, acc.: 76.56%] [G loss: 1.609536]\n",
            "4483 [D loss: 0.403131, acc.: 78.12%] [G loss: 1.640867]\n",
            "4484 [D loss: 0.446714, acc.: 76.56%] [G loss: 1.331011]\n",
            "4485 [D loss: 0.413573, acc.: 79.69%] [G loss: 1.478094]\n",
            "4486 [D loss: 0.479849, acc.: 71.88%] [G loss: 1.536587]\n",
            "4487 [D loss: 0.398561, acc.: 78.12%] [G loss: 1.740423]\n",
            "4488 [D loss: 0.438919, acc.: 78.12%] [G loss: 1.613838]\n",
            "4489 [D loss: 0.451385, acc.: 78.12%] [G loss: 2.022194]\n",
            "4490 [D loss: 0.389775, acc.: 78.12%] [G loss: 1.809730]\n",
            "4491 [D loss: 0.354048, acc.: 81.25%] [G loss: 1.816276]\n",
            "4492 [D loss: 0.405195, acc.: 78.12%] [G loss: 1.707325]\n",
            "4493 [D loss: 0.418934, acc.: 76.56%] [G loss: 1.454621]\n",
            "4494 [D loss: 0.434216, acc.: 78.12%] [G loss: 1.390321]\n",
            "4495 [D loss: 0.395054, acc.: 79.69%] [G loss: 1.616281]\n",
            "4496 [D loss: 0.369453, acc.: 79.69%] [G loss: 1.604822]\n",
            "4497 [D loss: 0.566134, acc.: 73.44%] [G loss: 1.497162]\n",
            "4498 [D loss: 0.458431, acc.: 78.12%] [G loss: 1.776468]\n",
            "4499 [D loss: 0.476055, acc.: 78.12%] [G loss: 1.531506]\n",
            "4500 [D loss: 0.478068, acc.: 76.56%] [G loss: 1.429245]\n",
            "generated_data\n",
            "4501 [D loss: 0.423491, acc.: 76.56%] [G loss: 1.661423]\n",
            "4502 [D loss: 0.446224, acc.: 78.12%] [G loss: 1.522804]\n",
            "4503 [D loss: 0.400421, acc.: 78.12%] [G loss: 1.514184]\n",
            "4504 [D loss: 0.437224, acc.: 75.00%] [G loss: 1.480137]\n",
            "4505 [D loss: 0.427012, acc.: 79.69%] [G loss: 1.403600]\n",
            "4506 [D loss: 0.392818, acc.: 79.69%] [G loss: 1.389908]\n",
            "4507 [D loss: 0.422739, acc.: 76.56%] [G loss: 2.118248]\n",
            "4508 [D loss: 0.452590, acc.: 79.69%] [G loss: 1.593369]\n",
            "4509 [D loss: 0.416538, acc.: 76.56%] [G loss: 1.450260]\n",
            "4510 [D loss: 0.441829, acc.: 78.12%] [G loss: 1.367791]\n",
            "4511 [D loss: 0.432279, acc.: 78.12%] [G loss: 1.476190]\n",
            "4512 [D loss: 0.421813, acc.: 78.12%] [G loss: 1.511420]\n",
            "4513 [D loss: 0.414476, acc.: 75.00%] [G loss: 1.538639]\n",
            "4514 [D loss: 0.447554, acc.: 75.00%] [G loss: 1.295784]\n",
            "4515 [D loss: 0.442491, acc.: 76.56%] [G loss: 1.184463]\n",
            "4516 [D loss: 0.414859, acc.: 78.12%] [G loss: 1.497256]\n",
            "4517 [D loss: 0.440548, acc.: 79.69%] [G loss: 1.280627]\n",
            "4518 [D loss: 0.443204, acc.: 78.12%] [G loss: 1.462702]\n",
            "4519 [D loss: 0.476831, acc.: 78.12%] [G loss: 1.462886]\n",
            "4520 [D loss: 0.430093, acc.: 76.56%] [G loss: 1.230902]\n",
            "4521 [D loss: 0.397748, acc.: 78.12%] [G loss: 1.381252]\n",
            "4522 [D loss: 0.384029, acc.: 79.69%] [G loss: 1.688021]\n",
            "4523 [D loss: 0.437919, acc.: 78.12%] [G loss: 1.473408]\n",
            "4524 [D loss: 0.492461, acc.: 76.56%] [G loss: 1.452215]\n",
            "4525 [D loss: 0.434663, acc.: 79.69%] [G loss: 1.616293]\n",
            "4526 [D loss: 0.391366, acc.: 79.69%] [G loss: 1.594719]\n",
            "4527 [D loss: 0.426101, acc.: 78.12%] [G loss: 1.706544]\n",
            "4528 [D loss: 0.429819, acc.: 76.56%] [G loss: 1.841195]\n",
            "4529 [D loss: 0.480850, acc.: 75.00%] [G loss: 1.624878]\n",
            "4530 [D loss: 0.425868, acc.: 78.12%] [G loss: 1.483678]\n",
            "4531 [D loss: 0.412484, acc.: 78.12%] [G loss: 1.396651]\n",
            "4532 [D loss: 0.440834, acc.: 78.12%] [G loss: 1.351088]\n",
            "4533 [D loss: 0.449917, acc.: 75.00%] [G loss: 1.452609]\n",
            "4534 [D loss: 0.407120, acc.: 78.12%] [G loss: 1.410532]\n",
            "4535 [D loss: 0.452471, acc.: 76.56%] [G loss: 1.668509]\n",
            "4536 [D loss: 0.526576, acc.: 75.00%] [G loss: 1.370657]\n",
            "4537 [D loss: 0.413879, acc.: 76.56%] [G loss: 1.472190]\n",
            "4538 [D loss: 0.451334, acc.: 75.00%] [G loss: 1.592148]\n",
            "4539 [D loss: 0.447560, acc.: 76.56%] [G loss: 1.482739]\n",
            "4540 [D loss: 0.421374, acc.: 79.69%] [G loss: 1.615670]\n",
            "4541 [D loss: 0.408758, acc.: 78.12%] [G loss: 1.641227]\n",
            "4542 [D loss: 0.467687, acc.: 76.56%] [G loss: 1.341130]\n",
            "4543 [D loss: 0.388063, acc.: 79.69%] [G loss: 1.663642]\n",
            "4544 [D loss: 0.424559, acc.: 78.12%] [G loss: 1.440320]\n",
            "4545 [D loss: 0.414309, acc.: 79.69%] [G loss: 1.672075]\n",
            "4546 [D loss: 0.397454, acc.: 79.69%] [G loss: 2.092811]\n",
            "4547 [D loss: 0.465739, acc.: 76.56%] [G loss: 1.598675]\n",
            "4548 [D loss: 0.397647, acc.: 78.12%] [G loss: 1.447915]\n",
            "4549 [D loss: 0.413426, acc.: 79.69%] [G loss: 1.595003]\n",
            "4550 [D loss: 0.459868, acc.: 76.56%] [G loss: 1.448678]\n",
            "4551 [D loss: 0.399094, acc.: 79.69%] [G loss: 1.742493]\n",
            "4552 [D loss: 0.427438, acc.: 79.69%] [G loss: 1.504528]\n",
            "4553 [D loss: 0.375892, acc.: 78.12%] [G loss: 1.910864]\n",
            "4554 [D loss: 0.412545, acc.: 78.12%] [G loss: 1.534220]\n",
            "4555 [D loss: 0.442929, acc.: 75.00%] [G loss: 1.474561]\n",
            "4556 [D loss: 0.405135, acc.: 78.12%] [G loss: 1.806651]\n",
            "4557 [D loss: 0.407479, acc.: 78.12%] [G loss: 1.960282]\n",
            "4558 [D loss: 0.430940, acc.: 78.12%] [G loss: 1.697738]\n",
            "4559 [D loss: 0.409895, acc.: 76.56%] [G loss: 1.808292]\n",
            "4560 [D loss: 0.361685, acc.: 78.12%] [G loss: 1.780790]\n",
            "4561 [D loss: 0.446621, acc.: 79.69%] [G loss: 1.623502]\n",
            "4562 [D loss: 0.405154, acc.: 79.69%] [G loss: 1.822701]\n",
            "4563 [D loss: 0.439156, acc.: 76.56%] [G loss: 1.418471]\n",
            "4564 [D loss: 0.371617, acc.: 79.69%] [G loss: 1.868418]\n",
            "4565 [D loss: 0.371112, acc.: 79.69%] [G loss: 1.920602]\n",
            "4566 [D loss: 0.473626, acc.: 76.56%] [G loss: 1.571868]\n",
            "4567 [D loss: 0.338523, acc.: 81.25%] [G loss: 1.751014]\n",
            "4568 [D loss: 0.372771, acc.: 81.25%] [G loss: 1.541919]\n",
            "4569 [D loss: 0.381315, acc.: 81.25%] [G loss: 1.633059]\n",
            "4570 [D loss: 0.403605, acc.: 78.12%] [G loss: 1.584330]\n",
            "4571 [D loss: 0.430312, acc.: 79.69%] [G loss: 1.247187]\n",
            "4572 [D loss: 0.387826, acc.: 79.69%] [G loss: 1.917459]\n",
            "4573 [D loss: 0.322019, acc.: 79.69%] [G loss: 2.496422]\n",
            "4574 [D loss: 0.413677, acc.: 81.25%] [G loss: 1.316180]\n",
            "4575 [D loss: 0.417974, acc.: 75.00%] [G loss: 2.198942]\n",
            "4576 [D loss: 0.427197, acc.: 75.00%] [G loss: 2.016155]\n",
            "4577 [D loss: 0.398840, acc.: 79.69%] [G loss: 1.501942]\n",
            "4578 [D loss: 0.444996, acc.: 79.69%] [G loss: 1.400842]\n",
            "4579 [D loss: 0.419472, acc.: 76.56%] [G loss: 2.210588]\n",
            "4580 [D loss: 0.431938, acc.: 78.12%] [G loss: 1.470807]\n",
            "4581 [D loss: 0.344863, acc.: 79.69%] [G loss: 1.893804]\n",
            "4582 [D loss: 0.413355, acc.: 76.56%] [G loss: 1.643490]\n",
            "4583 [D loss: 0.454295, acc.: 78.12%] [G loss: 1.767115]\n",
            "4584 [D loss: 0.531672, acc.: 76.56%] [G loss: 1.393350]\n",
            "4585 [D loss: 0.363229, acc.: 79.69%] [G loss: 1.762557]\n",
            "4586 [D loss: 0.444266, acc.: 78.12%] [G loss: 1.797812]\n",
            "4587 [D loss: 0.438979, acc.: 76.56%] [G loss: 1.317230]\n",
            "4588 [D loss: 0.428049, acc.: 79.69%] [G loss: 1.836073]\n",
            "4589 [D loss: 0.392205, acc.: 78.12%] [G loss: 1.969755]\n",
            "4590 [D loss: 0.489778, acc.: 76.56%] [G loss: 1.554840]\n",
            "4591 [D loss: 0.481087, acc.: 76.56%] [G loss: 1.711894]\n",
            "4592 [D loss: 0.425986, acc.: 78.12%] [G loss: 1.660093]\n",
            "4593 [D loss: 0.405437, acc.: 78.12%] [G loss: 1.746204]\n",
            "4594 [D loss: 0.372117, acc.: 78.12%] [G loss: 2.091241]\n",
            "4595 [D loss: 0.394643, acc.: 82.81%] [G loss: 1.851080]\n",
            "4596 [D loss: 0.410210, acc.: 78.12%] [G loss: 1.558783]\n",
            "4597 [D loss: 0.424586, acc.: 78.12%] [G loss: 2.024795]\n",
            "4598 [D loss: 0.424271, acc.: 73.44%] [G loss: 1.522437]\n",
            "4599 [D loss: 0.439550, acc.: 78.12%] [G loss: 1.553021]\n",
            "4600 [D loss: 0.464547, acc.: 75.00%] [G loss: 1.643880]\n",
            "generated_data\n",
            "4601 [D loss: 0.480418, acc.: 71.88%] [G loss: 1.364088]\n",
            "4602 [D loss: 0.439749, acc.: 81.25%] [G loss: 1.540048]\n",
            "4603 [D loss: 0.430661, acc.: 78.12%] [G loss: 1.386161]\n",
            "4604 [D loss: 0.446995, acc.: 78.12%] [G loss: 1.459063]\n",
            "4605 [D loss: 0.406508, acc.: 78.12%] [G loss: 1.515624]\n",
            "4606 [D loss: 0.446273, acc.: 76.56%] [G loss: 1.542528]\n",
            "4607 [D loss: 0.422336, acc.: 78.12%] [G loss: 1.337106]\n",
            "4608 [D loss: 0.441784, acc.: 79.69%] [G loss: 1.617601]\n",
            "4609 [D loss: 0.399943, acc.: 78.12%] [G loss: 1.607828]\n",
            "4610 [D loss: 0.455013, acc.: 79.69%] [G loss: 1.353423]\n",
            "4611 [D loss: 0.414207, acc.: 76.56%] [G loss: 1.609238]\n",
            "4612 [D loss: 0.377009, acc.: 79.69%] [G loss: 1.570013]\n",
            "4613 [D loss: 0.450949, acc.: 76.56%] [G loss: 1.716388]\n",
            "4614 [D loss: 0.396623, acc.: 78.12%] [G loss: 1.694363]\n",
            "4615 [D loss: 0.392911, acc.: 79.69%] [G loss: 1.480179]\n",
            "4616 [D loss: 0.438358, acc.: 75.00%] [G loss: 1.379957]\n",
            "4617 [D loss: 0.398314, acc.: 82.81%] [G loss: 1.740699]\n",
            "4618 [D loss: 0.443495, acc.: 78.12%] [G loss: 1.473723]\n",
            "4619 [D loss: 0.422909, acc.: 78.12%] [G loss: 1.596652]\n",
            "4620 [D loss: 0.444737, acc.: 76.56%] [G loss: 1.535170]\n",
            "4621 [D loss: 0.374192, acc.: 79.69%] [G loss: 1.976242]\n",
            "4622 [D loss: 0.406284, acc.: 79.69%] [G loss: 1.447041]\n",
            "4623 [D loss: 0.401782, acc.: 79.69%] [G loss: 1.905973]\n",
            "4624 [D loss: 0.406995, acc.: 76.56%] [G loss: 1.984657]\n",
            "4625 [D loss: 0.401029, acc.: 79.69%] [G loss: 1.866184]\n",
            "4626 [D loss: 0.418367, acc.: 78.12%] [G loss: 1.686696]\n",
            "4627 [D loss: 0.423786, acc.: 75.00%] [G loss: 1.776258]\n",
            "4628 [D loss: 0.375387, acc.: 79.69%] [G loss: 1.621568]\n",
            "4629 [D loss: 0.437919, acc.: 78.12%] [G loss: 1.723372]\n",
            "4630 [D loss: 0.435685, acc.: 76.56%] [G loss: 1.567820]\n",
            "4631 [D loss: 0.492974, acc.: 78.12%] [G loss: 1.533927]\n",
            "4632 [D loss: 0.442926, acc.: 75.00%] [G loss: 1.348090]\n",
            "4633 [D loss: 0.436044, acc.: 78.12%] [G loss: 1.333646]\n",
            "4634 [D loss: 0.469261, acc.: 76.56%] [G loss: 1.398607]\n",
            "4635 [D loss: 0.498093, acc.: 78.12%] [G loss: 1.632831]\n",
            "4636 [D loss: 0.386804, acc.: 78.12%] [G loss: 1.934417]\n",
            "4637 [D loss: 0.461706, acc.: 79.69%] [G loss: 1.603869]\n",
            "4638 [D loss: 0.397423, acc.: 78.12%] [G loss: 1.983736]\n",
            "4639 [D loss: 0.480836, acc.: 78.12%] [G loss: 1.640529]\n",
            "4640 [D loss: 0.432661, acc.: 79.69%] [G loss: 1.456669]\n",
            "4641 [D loss: 0.416654, acc.: 79.69%] [G loss: 1.605483]\n",
            "4642 [D loss: 0.453115, acc.: 79.69%] [G loss: 1.228037]\n",
            "4643 [D loss: 0.451419, acc.: 78.12%] [G loss: 1.378498]\n",
            "4644 [D loss: 0.375668, acc.: 79.69%] [G loss: 1.703074]\n",
            "4645 [D loss: 0.442103, acc.: 79.69%] [G loss: 1.679192]\n",
            "4646 [D loss: 0.409036, acc.: 79.69%] [G loss: 1.501366]\n",
            "4647 [D loss: 0.428074, acc.: 78.12%] [G loss: 1.730815]\n",
            "4648 [D loss: 0.441321, acc.: 79.69%] [G loss: 1.421512]\n",
            "4649 [D loss: 0.424859, acc.: 75.00%] [G loss: 1.492582]\n",
            "4650 [D loss: 0.432634, acc.: 79.69%] [G loss: 1.717086]\n",
            "4651 [D loss: 0.388443, acc.: 81.25%] [G loss: 1.426033]\n",
            "4652 [D loss: 0.412870, acc.: 81.25%] [G loss: 1.392073]\n",
            "4653 [D loss: 0.510250, acc.: 78.12%] [G loss: 1.826378]\n",
            "4654 [D loss: 0.462075, acc.: 76.56%] [G loss: 1.470083]\n",
            "4655 [D loss: 0.422256, acc.: 79.69%] [G loss: 1.612840]\n",
            "4656 [D loss: 0.397837, acc.: 78.12%] [G loss: 1.470635]\n",
            "4657 [D loss: 0.415381, acc.: 79.69%] [G loss: 1.331818]\n",
            "4658 [D loss: 0.404996, acc.: 78.12%] [G loss: 1.702667]\n",
            "4659 [D loss: 0.381801, acc.: 79.69%] [G loss: 1.781819]\n",
            "4660 [D loss: 0.434169, acc.: 78.12%] [G loss: 1.562961]\n",
            "4661 [D loss: 0.400682, acc.: 79.69%] [G loss: 1.553991]\n",
            "4662 [D loss: 0.499566, acc.: 76.56%] [G loss: 1.548910]\n",
            "4663 [D loss: 0.438087, acc.: 75.00%] [G loss: 1.528851]\n",
            "4664 [D loss: 0.487358, acc.: 76.56%] [G loss: 1.381264]\n",
            "4665 [D loss: 0.384862, acc.: 78.12%] [G loss: 2.042192]\n",
            "4666 [D loss: 0.415996, acc.: 76.56%] [G loss: 1.595515]\n",
            "4667 [D loss: 0.459146, acc.: 76.56%] [G loss: 1.509418]\n",
            "4668 [D loss: 0.458386, acc.: 75.00%] [G loss: 2.024954]\n",
            "4669 [D loss: 0.467120, acc.: 78.12%] [G loss: 1.573953]\n",
            "4670 [D loss: 0.403885, acc.: 78.12%] [G loss: 1.536678]\n",
            "4671 [D loss: 0.449925, acc.: 79.69%] [G loss: 1.493903]\n",
            "4672 [D loss: 0.440546, acc.: 79.69%] [G loss: 1.535462]\n",
            "4673 [D loss: 0.423143, acc.: 78.12%] [G loss: 1.601015]\n",
            "4674 [D loss: 0.452079, acc.: 78.12%] [G loss: 1.606455]\n",
            "4675 [D loss: 0.436104, acc.: 79.69%] [G loss: 1.622742]\n",
            "4676 [D loss: 0.465752, acc.: 76.56%] [G loss: 1.415668]\n",
            "4677 [D loss: 0.381749, acc.: 78.12%] [G loss: 1.723853]\n",
            "4678 [D loss: 0.384132, acc.: 81.25%] [G loss: 1.815425]\n",
            "4679 [D loss: 0.446426, acc.: 79.69%] [G loss: 1.865187]\n",
            "4680 [D loss: 0.434761, acc.: 78.12%] [G loss: 1.724539]\n",
            "4681 [D loss: 0.428441, acc.: 76.56%] [G loss: 1.813272]\n",
            "4682 [D loss: 0.464484, acc.: 75.00%] [G loss: 1.477152]\n",
            "4683 [D loss: 0.559904, acc.: 76.56%] [G loss: 1.375458]\n",
            "4684 [D loss: 0.416332, acc.: 79.69%] [G loss: 1.473645]\n",
            "4685 [D loss: 0.444742, acc.: 78.12%] [G loss: 1.240474]\n",
            "4686 [D loss: 0.413570, acc.: 79.69%] [G loss: 1.434882]\n",
            "4687 [D loss: 0.481986, acc.: 75.00%] [G loss: 1.402367]\n",
            "4688 [D loss: 0.469806, acc.: 78.12%] [G loss: 1.593256]\n",
            "4689 [D loss: 0.496753, acc.: 75.00%] [G loss: 1.766439]\n",
            "4690 [D loss: 0.446694, acc.: 78.12%] [G loss: 1.467605]\n",
            "4691 [D loss: 0.486801, acc.: 76.56%] [G loss: 1.415075]\n",
            "4692 [D loss: 0.451479, acc.: 78.12%] [G loss: 1.584277]\n",
            "4693 [D loss: 0.445403, acc.: 75.00%] [G loss: 1.321903]\n",
            "4694 [D loss: 0.472439, acc.: 78.12%] [G loss: 1.450429]\n",
            "4695 [D loss: 0.379266, acc.: 79.69%] [G loss: 1.717977]\n",
            "4696 [D loss: 0.465624, acc.: 76.56%] [G loss: 1.689312]\n",
            "4697 [D loss: 0.419728, acc.: 79.69%] [G loss: 1.753732]\n",
            "4698 [D loss: 0.416955, acc.: 79.69%] [G loss: 1.604053]\n",
            "4699 [D loss: 0.432323, acc.: 76.56%] [G loss: 1.942724]\n",
            "4700 [D loss: 0.443430, acc.: 79.69%] [G loss: 1.233890]\n",
            "generated_data\n",
            "4701 [D loss: 0.412179, acc.: 78.12%] [G loss: 1.480807]\n",
            "4702 [D loss: 0.377715, acc.: 79.69%] [G loss: 1.597792]\n",
            "4703 [D loss: 0.427290, acc.: 79.69%] [G loss: 1.919171]\n",
            "4704 [D loss: 0.394681, acc.: 82.81%] [G loss: 1.481961]\n",
            "4705 [D loss: 0.408355, acc.: 76.56%] [G loss: 2.034547]\n",
            "4706 [D loss: 0.420799, acc.: 78.12%] [G loss: 1.648524]\n",
            "4707 [D loss: 0.430892, acc.: 78.12%] [G loss: 1.430195]\n",
            "4708 [D loss: 0.439427, acc.: 79.69%] [G loss: 1.763097]\n",
            "4709 [D loss: 0.362104, acc.: 81.25%] [G loss: 1.658965]\n",
            "4710 [D loss: 0.470205, acc.: 76.56%] [G loss: 1.614859]\n",
            "4711 [D loss: 0.473406, acc.: 78.12%] [G loss: 1.767381]\n",
            "4712 [D loss: 0.376223, acc.: 78.12%] [G loss: 1.640358]\n",
            "4713 [D loss: 0.448556, acc.: 79.69%] [G loss: 1.463145]\n",
            "4714 [D loss: 0.450744, acc.: 78.12%] [G loss: 1.678519]\n",
            "4715 [D loss: 0.441597, acc.: 76.56%] [G loss: 1.836599]\n",
            "4716 [D loss: 0.402398, acc.: 82.81%] [G loss: 1.399477]\n",
            "4717 [D loss: 0.361643, acc.: 78.12%] [G loss: 1.812637]\n",
            "4718 [D loss: 0.413339, acc.: 76.56%] [G loss: 1.467641]\n",
            "4719 [D loss: 0.377096, acc.: 78.12%] [G loss: 1.985130]\n",
            "4720 [D loss: 0.417109, acc.: 79.69%] [G loss: 1.413571]\n",
            "4721 [D loss: 0.354569, acc.: 79.69%] [G loss: 2.174898]\n",
            "4722 [D loss: 0.440788, acc.: 73.44%] [G loss: 1.480151]\n",
            "4723 [D loss: 0.430666, acc.: 78.12%] [G loss: 1.965451]\n",
            "4724 [D loss: 0.414581, acc.: 76.56%] [G loss: 1.737523]\n",
            "4725 [D loss: 0.436682, acc.: 78.12%] [G loss: 1.375731]\n",
            "4726 [D loss: 0.355985, acc.: 78.12%] [G loss: 1.701092]\n",
            "4727 [D loss: 0.415787, acc.: 79.69%] [G loss: 1.666320]\n",
            "4728 [D loss: 0.433490, acc.: 79.69%] [G loss: 1.648885]\n",
            "4729 [D loss: 0.433935, acc.: 78.12%] [G loss: 1.325780]\n",
            "4730 [D loss: 0.415523, acc.: 79.69%] [G loss: 1.447036]\n",
            "4731 [D loss: 0.410147, acc.: 78.12%] [G loss: 2.090588]\n",
            "4732 [D loss: 0.425753, acc.: 78.12%] [G loss: 1.337507]\n",
            "4733 [D loss: 0.431061, acc.: 76.56%] [G loss: 1.376412]\n",
            "4734 [D loss: 0.432233, acc.: 78.12%] [G loss: 1.716334]\n",
            "4735 [D loss: 0.424588, acc.: 79.69%] [G loss: 1.730317]\n",
            "4736 [D loss: 0.436873, acc.: 78.12%] [G loss: 1.486100]\n",
            "4737 [D loss: 0.374461, acc.: 81.25%] [G loss: 1.749649]\n",
            "4738 [D loss: 0.390002, acc.: 78.12%] [G loss: 1.325780]\n",
            "4739 [D loss: 0.451476, acc.: 76.56%] [G loss: 1.463569]\n",
            "4740 [D loss: 0.436228, acc.: 78.12%] [G loss: 1.582814]\n",
            "4741 [D loss: 0.409827, acc.: 79.69%] [G loss: 1.654556]\n",
            "4742 [D loss: 0.443317, acc.: 78.12%] [G loss: 1.638016]\n",
            "4743 [D loss: 0.356861, acc.: 79.69%] [G loss: 1.953484]\n",
            "4744 [D loss: 0.401570, acc.: 76.56%] [G loss: 1.694268]\n",
            "4745 [D loss: 0.425852, acc.: 79.69%] [G loss: 1.537985]\n",
            "4746 [D loss: 0.394870, acc.: 78.12%] [G loss: 1.704630]\n",
            "4747 [D loss: 0.394996, acc.: 81.25%] [G loss: 1.590362]\n",
            "4748 [D loss: 0.381337, acc.: 79.69%] [G loss: 1.908388]\n",
            "4749 [D loss: 0.426490, acc.: 79.69%] [G loss: 1.402492]\n",
            "4750 [D loss: 0.376357, acc.: 82.81%] [G loss: 1.591622]\n",
            "4751 [D loss: 0.465694, acc.: 76.56%] [G loss: 1.693511]\n",
            "4752 [D loss: 0.392088, acc.: 78.12%] [G loss: 1.540659]\n",
            "4753 [D loss: 0.385061, acc.: 79.69%] [G loss: 1.572508]\n",
            "4754 [D loss: 0.411116, acc.: 79.69%] [G loss: 1.909656]\n",
            "4755 [D loss: 0.529915, acc.: 75.00%] [G loss: 1.384702]\n",
            "4756 [D loss: 0.392648, acc.: 79.69%] [G loss: 1.881142]\n",
            "4757 [D loss: 0.434039, acc.: 79.69%] [G loss: 1.838806]\n",
            "4758 [D loss: 0.444207, acc.: 79.69%] [G loss: 1.842418]\n",
            "4759 [D loss: 0.377622, acc.: 79.69%] [G loss: 1.945265]\n",
            "4760 [D loss: 0.447049, acc.: 76.56%] [G loss: 1.749810]\n",
            "4761 [D loss: 0.400479, acc.: 78.12%] [G loss: 1.925856]\n",
            "4762 [D loss: 0.477002, acc.: 79.69%] [G loss: 1.483540]\n",
            "4763 [D loss: 0.385434, acc.: 81.25%] [G loss: 1.646012]\n",
            "4764 [D loss: 0.446088, acc.: 78.12%] [G loss: 1.591131]\n",
            "4765 [D loss: 0.441220, acc.: 78.12%] [G loss: 1.550313]\n",
            "4766 [D loss: 0.385476, acc.: 81.25%] [G loss: 1.547779]\n",
            "4767 [D loss: 0.418051, acc.: 76.56%] [G loss: 1.390827]\n",
            "4768 [D loss: 0.387152, acc.: 76.56%] [G loss: 1.816584]\n",
            "4769 [D loss: 0.469641, acc.: 73.44%] [G loss: 1.328274]\n",
            "4770 [D loss: 0.398160, acc.: 78.12%] [G loss: 1.585920]\n",
            "4771 [D loss: 0.433633, acc.: 76.56%] [G loss: 1.490148]\n",
            "4772 [D loss: 0.387897, acc.: 79.69%] [G loss: 1.745800]\n",
            "4773 [D loss: 0.405619, acc.: 78.12%] [G loss: 1.742370]\n",
            "4774 [D loss: 0.435586, acc.: 78.12%] [G loss: 1.468239]\n",
            "4775 [D loss: 0.443101, acc.: 79.69%] [G loss: 2.194048]\n",
            "4776 [D loss: 0.437740, acc.: 78.12%] [G loss: 1.340245]\n",
            "4777 [D loss: 0.441271, acc.: 78.12%] [G loss: 1.679751]\n",
            "4778 [D loss: 0.429284, acc.: 78.12%] [G loss: 1.402104]\n",
            "4779 [D loss: 0.373087, acc.: 79.69%] [G loss: 1.767329]\n",
            "4780 [D loss: 0.426074, acc.: 78.12%] [G loss: 1.076452]\n",
            "4781 [D loss: 0.467985, acc.: 78.12%] [G loss: 1.915423]\n",
            "4782 [D loss: 0.439700, acc.: 76.56%] [G loss: 1.374026]\n",
            "4783 [D loss: 0.397002, acc.: 79.69%] [G loss: 1.493361]\n",
            "4784 [D loss: 0.383693, acc.: 81.25%] [G loss: 1.771672]\n",
            "4785 [D loss: 0.459639, acc.: 76.56%] [G loss: 1.509637]\n",
            "4786 [D loss: 0.389378, acc.: 79.69%] [G loss: 1.358055]\n",
            "4787 [D loss: 0.428926, acc.: 78.12%] [G loss: 1.540376]\n",
            "4788 [D loss: 0.406152, acc.: 79.69%] [G loss: 1.481366]\n",
            "4789 [D loss: 0.470779, acc.: 75.00%] [G loss: 1.363564]\n",
            "4790 [D loss: 0.405017, acc.: 78.12%] [G loss: 1.592260]\n",
            "4791 [D loss: 0.425830, acc.: 79.69%] [G loss: 1.512056]\n",
            "4792 [D loss: 0.486063, acc.: 75.00%] [G loss: 1.543845]\n",
            "4793 [D loss: 0.396775, acc.: 76.56%] [G loss: 1.615602]\n",
            "4794 [D loss: 0.433461, acc.: 79.69%] [G loss: 1.724877]\n",
            "4795 [D loss: 0.442139, acc.: 78.12%] [G loss: 1.458604]\n",
            "4796 [D loss: 0.461910, acc.: 76.56%] [G loss: 1.915501]\n",
            "4797 [D loss: 0.352438, acc.: 79.69%] [G loss: 1.973614]\n",
            "4798 [D loss: 0.467868, acc.: 78.12%] [G loss: 1.580045]\n",
            "4799 [D loss: 0.397420, acc.: 78.12%] [G loss: 1.817888]\n",
            "4800 [D loss: 0.374259, acc.: 79.69%] [G loss: 1.629694]\n",
            "generated_data\n",
            "4801 [D loss: 0.460388, acc.: 75.00%] [G loss: 1.404248]\n",
            "4802 [D loss: 0.496289, acc.: 73.44%] [G loss: 1.453508]\n",
            "4803 [D loss: 0.419895, acc.: 78.12%] [G loss: 1.460508]\n",
            "4804 [D loss: 0.398711, acc.: 76.56%] [G loss: 2.056472]\n",
            "4805 [D loss: 0.471787, acc.: 78.12%] [G loss: 1.527393]\n",
            "4806 [D loss: 0.406829, acc.: 79.69%] [G loss: 1.371138]\n",
            "4807 [D loss: 0.425118, acc.: 78.12%] [G loss: 1.469796]\n",
            "4808 [D loss: 0.430579, acc.: 79.69%] [G loss: 1.296222]\n",
            "4809 [D loss: 0.440357, acc.: 75.00%] [G loss: 1.440832]\n",
            "4810 [D loss: 0.412214, acc.: 76.56%] [G loss: 1.374504]\n",
            "4811 [D loss: 0.382710, acc.: 79.69%] [G loss: 1.479119]\n",
            "4812 [D loss: 0.443792, acc.: 79.69%] [G loss: 1.603951]\n",
            "4813 [D loss: 0.466472, acc.: 76.56%] [G loss: 1.724701]\n",
            "4814 [D loss: 0.402178, acc.: 79.69%] [G loss: 1.371439]\n",
            "4815 [D loss: 0.400104, acc.: 78.12%] [G loss: 1.812937]\n",
            "4816 [D loss: 0.440159, acc.: 79.69%] [G loss: 1.656868]\n",
            "4817 [D loss: 0.442162, acc.: 79.69%] [G loss: 1.559557]\n",
            "4818 [D loss: 0.403231, acc.: 79.69%] [G loss: 1.595727]\n",
            "4819 [D loss: 0.453133, acc.: 78.12%] [G loss: 1.542565]\n",
            "4820 [D loss: 0.412205, acc.: 81.25%] [G loss: 1.800856]\n",
            "4821 [D loss: 0.383047, acc.: 79.69%] [G loss: 1.596567]\n",
            "4822 [D loss: 0.422353, acc.: 76.56%] [G loss: 1.558666]\n",
            "4823 [D loss: 0.520625, acc.: 75.00%] [G loss: 1.379074]\n",
            "4824 [D loss: 0.371763, acc.: 79.69%] [G loss: 2.152330]\n",
            "4825 [D loss: 0.481262, acc.: 76.56%] [G loss: 1.607583]\n",
            "4826 [D loss: 0.448201, acc.: 75.00%] [G loss: 1.290915]\n",
            "4827 [D loss: 0.452594, acc.: 76.56%] [G loss: 1.649885]\n",
            "4828 [D loss: 0.418405, acc.: 78.12%] [G loss: 1.752074]\n",
            "4829 [D loss: 0.435192, acc.: 79.69%] [G loss: 1.304253]\n",
            "4830 [D loss: 0.419681, acc.: 79.69%] [G loss: 1.568778]\n",
            "4831 [D loss: 0.405180, acc.: 78.12%] [G loss: 1.997556]\n",
            "4832 [D loss: 0.484081, acc.: 76.56%] [G loss: 1.375655]\n",
            "4833 [D loss: 0.362548, acc.: 79.69%] [G loss: 2.093282]\n",
            "4834 [D loss: 0.463640, acc.: 82.81%] [G loss: 1.669424]\n",
            "4835 [D loss: 0.442179, acc.: 76.56%] [G loss: 1.764079]\n",
            "4836 [D loss: 0.376848, acc.: 82.81%] [G loss: 1.738108]\n",
            "4837 [D loss: 0.437785, acc.: 76.56%] [G loss: 1.816606]\n",
            "4838 [D loss: 0.399265, acc.: 79.69%] [G loss: 1.579652]\n",
            "4839 [D loss: 0.428187, acc.: 76.56%] [G loss: 1.434815]\n",
            "4840 [D loss: 0.407697, acc.: 78.12%] [G loss: 1.762464]\n",
            "4841 [D loss: 0.428302, acc.: 78.12%] [G loss: 1.410756]\n",
            "4842 [D loss: 0.397776, acc.: 79.69%] [G loss: 1.623440]\n",
            "4843 [D loss: 0.407378, acc.: 79.69%] [G loss: 1.617400]\n",
            "4844 [D loss: 0.480312, acc.: 78.12%] [G loss: 1.527301]\n",
            "4845 [D loss: 0.491775, acc.: 79.69%] [G loss: 1.417453]\n",
            "4846 [D loss: 0.427726, acc.: 78.12%] [G loss: 1.751948]\n",
            "4847 [D loss: 0.423348, acc.: 81.25%] [G loss: 1.641091]\n",
            "4848 [D loss: 0.448539, acc.: 78.12%] [G loss: 1.512779]\n",
            "4849 [D loss: 0.438192, acc.: 76.56%] [G loss: 1.541842]\n",
            "4850 [D loss: 0.430153, acc.: 76.56%] [G loss: 1.761689]\n",
            "4851 [D loss: 0.410253, acc.: 78.12%] [G loss: 1.297282]\n",
            "4852 [D loss: 0.342441, acc.: 78.12%] [G loss: 2.551753]\n",
            "4853 [D loss: 0.506008, acc.: 75.00%] [G loss: 1.799491]\n",
            "4854 [D loss: 0.389466, acc.: 76.56%] [G loss: 1.538805]\n",
            "4855 [D loss: 0.456512, acc.: 75.00%] [G loss: 1.303260]\n",
            "4856 [D loss: 0.411348, acc.: 79.69%] [G loss: 1.675745]\n",
            "4857 [D loss: 0.386910, acc.: 76.56%] [G loss: 1.467838]\n",
            "4858 [D loss: 0.539306, acc.: 75.00%] [G loss: 1.249767]\n",
            "4859 [D loss: 0.394264, acc.: 81.25%] [G loss: 1.871167]\n",
            "4860 [D loss: 0.427751, acc.: 78.12%] [G loss: 1.524833]\n",
            "4861 [D loss: 0.477551, acc.: 78.12%] [G loss: 1.828362]\n",
            "4862 [D loss: 0.419934, acc.: 78.12%] [G loss: 1.903793]\n",
            "4863 [D loss: 0.354766, acc.: 78.12%] [G loss: 1.762707]\n",
            "4864 [D loss: 0.424088, acc.: 76.56%] [G loss: 1.594784]\n",
            "4865 [D loss: 0.405338, acc.: 78.12%] [G loss: 1.843762]\n",
            "4866 [D loss: 0.426757, acc.: 78.12%] [G loss: 1.836163]\n",
            "4867 [D loss: 0.451510, acc.: 78.12%] [G loss: 1.638005]\n",
            "4868 [D loss: 0.421559, acc.: 78.12%] [G loss: 1.569342]\n",
            "4869 [D loss: 0.428557, acc.: 75.00%] [G loss: 1.836532]\n",
            "4870 [D loss: 0.442994, acc.: 81.25%] [G loss: 1.447943]\n",
            "4871 [D loss: 0.380650, acc.: 79.69%] [G loss: 1.824176]\n",
            "4872 [D loss: 0.404062, acc.: 82.81%] [G loss: 1.603103]\n",
            "4873 [D loss: 0.470854, acc.: 79.69%] [G loss: 1.341686]\n",
            "4874 [D loss: 0.392847, acc.: 78.12%] [G loss: 1.482733]\n",
            "4875 [D loss: 0.397710, acc.: 82.81%] [G loss: 1.434787]\n",
            "4876 [D loss: 0.435317, acc.: 78.12%] [G loss: 1.681023]\n",
            "4877 [D loss: 0.380692, acc.: 79.69%] [G loss: 1.611485]\n",
            "4878 [D loss: 0.381742, acc.: 78.12%] [G loss: 1.848974]\n",
            "4879 [D loss: 0.404146, acc.: 79.69%] [G loss: 1.417275]\n",
            "4880 [D loss: 0.406608, acc.: 82.81%] [G loss: 1.744627]\n",
            "4881 [D loss: 0.443997, acc.: 76.56%] [G loss: 1.321875]\n",
            "4882 [D loss: 0.441998, acc.: 79.69%] [G loss: 1.670985]\n",
            "4883 [D loss: 0.377532, acc.: 79.69%] [G loss: 1.942294]\n",
            "4884 [D loss: 0.435825, acc.: 79.69%] [G loss: 1.495062]\n",
            "4885 [D loss: 0.383994, acc.: 79.69%] [G loss: 2.093644]\n",
            "4886 [D loss: 0.411870, acc.: 78.12%] [G loss: 1.465431]\n",
            "4887 [D loss: 0.404348, acc.: 79.69%] [G loss: 1.855163]\n",
            "4888 [D loss: 0.377709, acc.: 79.69%] [G loss: 1.902321]\n",
            "4889 [D loss: 0.444625, acc.: 79.69%] [G loss: 1.885628]\n",
            "4890 [D loss: 0.440638, acc.: 79.69%] [G loss: 1.520579]\n",
            "4891 [D loss: 0.454480, acc.: 76.56%] [G loss: 1.768986]\n",
            "4892 [D loss: 0.434466, acc.: 78.12%] [G loss: 1.709500]\n",
            "4893 [D loss: 0.424123, acc.: 79.69%] [G loss: 1.590366]\n",
            "4894 [D loss: 0.408220, acc.: 79.69%] [G loss: 1.922457]\n",
            "4895 [D loss: 0.356867, acc.: 79.69%] [G loss: 2.144978]\n",
            "4896 [D loss: 0.431729, acc.: 81.25%] [G loss: 1.480529]\n",
            "4897 [D loss: 0.402207, acc.: 78.12%] [G loss: 2.002708]\n",
            "4898 [D loss: 0.400817, acc.: 79.69%] [G loss: 1.497328]\n",
            "4899 [D loss: 0.415118, acc.: 78.12%] [G loss: 1.342909]\n",
            "4900 [D loss: 0.471568, acc.: 78.12%] [G loss: 1.709444]\n",
            "generated_data\n",
            "4901 [D loss: 0.377743, acc.: 82.81%] [G loss: 1.818589]\n",
            "4902 [D loss: 0.407321, acc.: 81.25%] [G loss: 1.504618]\n",
            "4903 [D loss: 0.450057, acc.: 79.69%] [G loss: 1.574394]\n",
            "4904 [D loss: 0.380321, acc.: 79.69%] [G loss: 1.922415]\n",
            "4905 [D loss: 0.391610, acc.: 81.25%] [G loss: 1.820992]\n",
            "4906 [D loss: 0.402270, acc.: 78.12%] [G loss: 1.738584]\n",
            "4907 [D loss: 0.433600, acc.: 79.69%] [G loss: 1.642471]\n",
            "4908 [D loss: 0.396685, acc.: 79.69%] [G loss: 1.910843]\n",
            "4909 [D loss: 0.466345, acc.: 75.00%] [G loss: 1.685186]\n",
            "4910 [D loss: 0.436791, acc.: 78.12%] [G loss: 1.660093]\n",
            "4911 [D loss: 0.346549, acc.: 81.25%] [G loss: 2.157146]\n",
            "4912 [D loss: 0.458105, acc.: 76.56%] [G loss: 1.550274]\n",
            "4913 [D loss: 0.445340, acc.: 76.56%] [G loss: 1.948008]\n",
            "4914 [D loss: 0.383419, acc.: 79.69%] [G loss: 1.602529]\n",
            "4915 [D loss: 0.432611, acc.: 78.12%] [G loss: 1.328123]\n",
            "4916 [D loss: 0.429657, acc.: 73.44%] [G loss: 1.893641]\n",
            "4917 [D loss: 0.412517, acc.: 79.69%] [G loss: 1.301440]\n",
            "4918 [D loss: 0.354216, acc.: 81.25%] [G loss: 1.698048]\n",
            "4919 [D loss: 0.371796, acc.: 78.12%] [G loss: 2.019450]\n",
            "4920 [D loss: 0.439505, acc.: 78.12%] [G loss: 1.351267]\n",
            "4921 [D loss: 0.326910, acc.: 79.69%] [G loss: 2.005006]\n",
            "4922 [D loss: 0.406424, acc.: 75.00%] [G loss: 1.495156]\n",
            "4923 [D loss: 0.360983, acc.: 78.12%] [G loss: 1.819859]\n",
            "4924 [D loss: 0.399986, acc.: 78.12%] [G loss: 1.590870]\n",
            "4925 [D loss: 0.458987, acc.: 78.12%] [G loss: 1.377542]\n",
            "4926 [D loss: 0.424009, acc.: 76.56%] [G loss: 1.549411]\n",
            "4927 [D loss: 0.376313, acc.: 79.69%] [G loss: 1.585662]\n",
            "4928 [D loss: 0.456636, acc.: 79.69%] [G loss: 1.398500]\n",
            "4929 [D loss: 0.397497, acc.: 78.12%] [G loss: 1.700944]\n",
            "4930 [D loss: 0.440212, acc.: 75.00%] [G loss: 1.654542]\n",
            "4931 [D loss: 0.390070, acc.: 78.12%] [G loss: 1.933094]\n",
            "4932 [D loss: 0.389458, acc.: 81.25%] [G loss: 1.423735]\n",
            "4933 [D loss: 0.395711, acc.: 81.25%] [G loss: 1.890799]\n",
            "4934 [D loss: 0.386702, acc.: 82.81%] [G loss: 1.638910]\n",
            "4935 [D loss: 0.357478, acc.: 79.69%] [G loss: 1.791144]\n",
            "4936 [D loss: 0.447598, acc.: 75.00%] [G loss: 1.361937]\n",
            "4937 [D loss: 0.382562, acc.: 78.12%] [G loss: 1.620914]\n",
            "4938 [D loss: 0.415190, acc.: 79.69%] [G loss: 1.554621]\n",
            "4939 [D loss: 0.385293, acc.: 79.69%] [G loss: 1.465359]\n",
            "4940 [D loss: 0.443140, acc.: 73.44%] [G loss: 1.391633]\n",
            "4941 [D loss: 0.374304, acc.: 79.69%] [G loss: 1.589918]\n",
            "4942 [D loss: 0.457252, acc.: 76.56%] [G loss: 1.730636]\n",
            "4943 [D loss: 0.428877, acc.: 79.69%] [G loss: 1.787056]\n",
            "4944 [D loss: 0.445182, acc.: 78.12%] [G loss: 1.888783]\n",
            "4945 [D loss: 0.411104, acc.: 81.25%] [G loss: 1.771561]\n",
            "4946 [D loss: 0.423711, acc.: 79.69%] [G loss: 1.479528]\n",
            "4947 [D loss: 0.397305, acc.: 78.12%] [G loss: 2.067152]\n",
            "4948 [D loss: 0.427843, acc.: 79.69%] [G loss: 1.703574]\n",
            "4949 [D loss: 0.456185, acc.: 76.56%] [G loss: 1.518176]\n",
            "4950 [D loss: 0.441719, acc.: 78.12%] [G loss: 1.701443]\n",
            "4951 [D loss: 0.421447, acc.: 81.25%] [G loss: 1.400728]\n",
            "4952 [D loss: 0.451988, acc.: 81.25%] [G loss: 1.274609]\n",
            "4953 [D loss: 0.392025, acc.: 82.81%] [G loss: 1.638680]\n",
            "4954 [D loss: 0.397983, acc.: 82.81%] [G loss: 1.581888]\n",
            "4955 [D loss: 0.401829, acc.: 81.25%] [G loss: 1.338978]\n",
            "4956 [D loss: 0.386597, acc.: 79.69%] [G loss: 1.756929]\n",
            "4957 [D loss: 0.492375, acc.: 76.56%] [G loss: 1.475882]\n",
            "4958 [D loss: 0.439534, acc.: 78.12%] [G loss: 1.467634]\n",
            "4959 [D loss: 0.434044, acc.: 81.25%] [G loss: 1.335926]\n",
            "4960 [D loss: 0.450201, acc.: 75.00%] [G loss: 1.274802]\n",
            "4961 [D loss: 0.429559, acc.: 75.00%] [G loss: 1.521957]\n",
            "4962 [D loss: 0.431596, acc.: 76.56%] [G loss: 1.712727]\n",
            "4963 [D loss: 0.451958, acc.: 79.69%] [G loss: 2.027286]\n",
            "4964 [D loss: 0.458925, acc.: 78.12%] [G loss: 1.596702]\n",
            "4965 [D loss: 0.423455, acc.: 78.12%] [G loss: 1.876354]\n",
            "4966 [D loss: 0.485393, acc.: 76.56%] [G loss: 1.626891]\n",
            "4967 [D loss: 0.406439, acc.: 78.12%] [G loss: 1.646350]\n",
            "4968 [D loss: 0.391000, acc.: 79.69%] [G loss: 1.427564]\n",
            "4969 [D loss: 0.433884, acc.: 78.12%] [G loss: 1.596196]\n",
            "4970 [D loss: 0.424936, acc.: 81.25%] [G loss: 1.634296]\n",
            "4971 [D loss: 0.473706, acc.: 78.12%] [G loss: 1.730743]\n",
            "4972 [D loss: 0.407190, acc.: 79.69%] [G loss: 1.643969]\n",
            "4973 [D loss: 0.439354, acc.: 76.56%] [G loss: 1.429389]\n",
            "4974 [D loss: 0.411695, acc.: 78.12%] [G loss: 1.696206]\n",
            "4975 [D loss: 0.448825, acc.: 76.56%] [G loss: 1.455620]\n",
            "4976 [D loss: 0.458650, acc.: 76.56%] [G loss: 1.563501]\n",
            "4977 [D loss: 0.429178, acc.: 79.69%] [G loss: 1.452302]\n",
            "4978 [D loss: 0.467736, acc.: 76.56%] [G loss: 1.625175]\n",
            "4979 [D loss: 0.428524, acc.: 79.69%] [G loss: 1.448917]\n",
            "4980 [D loss: 0.403060, acc.: 78.12%] [G loss: 1.379676]\n",
            "4981 [D loss: 0.420978, acc.: 79.69%] [G loss: 1.607941]\n",
            "4982 [D loss: 0.418227, acc.: 76.56%] [G loss: 1.160761]\n",
            "4983 [D loss: 0.416576, acc.: 76.56%] [G loss: 1.478511]\n",
            "4984 [D loss: 0.402767, acc.: 79.69%] [G loss: 1.251515]\n",
            "4985 [D loss: 0.444421, acc.: 78.12%] [G loss: 1.249291]\n",
            "4986 [D loss: 0.440277, acc.: 79.69%] [G loss: 1.758364]\n",
            "4987 [D loss: 0.410261, acc.: 78.12%] [G loss: 1.446028]\n",
            "4988 [D loss: 0.470160, acc.: 78.12%] [G loss: 1.944134]\n",
            "4989 [D loss: 0.402849, acc.: 79.69%] [G loss: 1.697040]\n",
            "4990 [D loss: 0.431648, acc.: 78.12%] [G loss: 1.645851]\n",
            "4991 [D loss: 0.366273, acc.: 78.12%] [G loss: 1.819464]\n",
            "4992 [D loss: 0.421318, acc.: 78.12%] [G loss: 1.231782]\n",
            "4993 [D loss: 0.410053, acc.: 78.12%] [G loss: 1.349650]\n",
            "4994 [D loss: 0.414226, acc.: 78.12%] [G loss: 1.559906]\n",
            "4995 [D loss: 0.430516, acc.: 78.12%] [G loss: 1.269041]\n",
            "4996 [D loss: 0.428356, acc.: 81.25%] [G loss: 1.455005]\n",
            "4997 [D loss: 0.425908, acc.: 81.25%] [G loss: 1.531137]\n",
            "4998 [D loss: 0.409166, acc.: 78.12%] [G loss: 1.702396]\n",
            "4999 [D loss: 0.420364, acc.: 76.56%] [G loss: 1.739728]\n",
            "5000 [D loss: 0.439902, acc.: 79.69%] [G loss: 1.513372]\n",
            "generated_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model/gan\n",
        "!mkdir model/gan/saved"
      ],
      "metadata": {
        "id": "MAbZNvmNlAaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd08aa0-b15e-4048-c301-4a7364887f6c"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘model/gan’: File exists\n",
            "mkdir: cannot create directory ‘model/gan/saved’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synthesizer.generator.summary()"
      ],
      "metadata": {
        "id": "MevBhheslDXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cd7917-c8c3-4b3d-aa51-f2aaa439042b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(32, 32)]                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (32, 128)                 4224      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (32, 256)                 33024     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (32, 512)                 131584    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (32, 6)                   3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 171,910\n",
            "Trainable params: 171,910\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synthesizer.discriminator.summary()"
      ],
      "metadata": {
        "id": "-38FnhpylF2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49dc6815-794c-42b9-a840-6a9565579a4e"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(32, 6)]                 0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (32, 512)                 3584      \n",
            "                                                                 \n",
            " dropout (Dropout)           (32, 512)                 0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (32, 256)                 131328    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (32, 256)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (32, 128)                 32896     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (32, 1)                   129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 167,937\n",
            "Trainable params: 0\n",
            "Non-trainable params: 167,937\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {'GAN': ['GAN', False, synthesizer.generator]}"
      ],
      "metadata": {
        "id": "eH2gKf5glJ9G"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['GAN']\n",
        "colors = ['deepskyblue','blue']\n",
        "markers = ['o','^']"
      ],
      "metadata": {
        "id": "gR_P8DVSlNB0"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup parameters visualization parameters\n",
        "seed = 17\n",
        "test_size = 151\n",
        "noise_dim = 32\n",
        "\n",
        "np.random.seed(seed)\n",
        "z = np.random.normal(size=(test_size, noise_dim))"
      ],
      "metadata": {
        "id": "gVfxM68IlU0D"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col1,col2,col3,col4,col5,col6,col7,col8,col9,col10='conc (ppm)','ad dose(g/L)','ph value','Temperature(⁰C)','time','absorbance','conc','real conc','removal','%removal  '"
      ],
      "metadata": {
        "id": "qhxk02DGqDGV"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_steps= [ 0, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000]\n",
        "rows = len(model_steps)\n",
        "columns = 1"
      ],
      "metadata": {
        "id": "N7hiBymflc0k"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = 'model/'"
      ],
      "metadata": {
        "id": "YrN-XJ8glg96"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_step_ix, model_step in enumerate(model_steps):\n",
        "  [model_name, with_class, generator_model] = models['GAN']\n",
        "\n",
        "  generator_model.load_weights( base_dir + '_generator_model_weights_step_'+str(model_step)+'.h5')\n",
        "\n",
        "  #ax = plt.subplot(rows, columns, model_step_ix*columns + 1 + (i+1) )\n",
        "\n",
        "  g_z = generator_model.predict(z)\n",
        "  gen_samples = pd.DataFrame(g_z, columns = data_cols)"
      ],
      "metadata": {
        "id": "vQgDCnDilkdJ"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_samples"
      ],
      "metadata": {
        "id": "lrUMRENplsNN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "535b8adb-7e1c-48e6-fcf4-bcf64953a745"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     conc(ppm)  ad_dose(g/L)  ph_value  temperature(⁰C)       time  \\\n",
              "0    31.487486      1.049047  8.881372        31.199066  31.676035   \n",
              "1    32.226532      1.509070  6.820713        32.126610  33.018497   \n",
              "2    36.380421      2.029041  6.050174        36.287495  37.034786   \n",
              "3    33.025333      1.848757  5.596869        32.966301  33.948051   \n",
              "4    28.842443      1.483023  6.569973        28.537069  57.077984   \n",
              "..         ...           ...       ...              ...        ...   \n",
              "146  27.733757      1.416482  5.943005        27.640587  31.516726   \n",
              "147  29.375164      1.074924  8.254704        29.355972  30.344042   \n",
              "148  22.297516      0.958027  7.807487        32.447582  36.737713   \n",
              "149  31.583769      1.276017  3.389986        31.085382  62.289631   \n",
              "150  39.836014      1.883100  9.110263        39.171963  79.030586   \n",
              "\n",
              "     conc2(=absorbance/0.029)  \n",
              "0                    0.942461  \n",
              "1                    1.242170  \n",
              "2                    1.489969  \n",
              "3                    1.434995  \n",
              "4                    0.700526  \n",
              "..                        ...  \n",
              "146                  0.874037  \n",
              "147                  1.015706  \n",
              "148                  0.894599  \n",
              "149                  0.386164  \n",
              "150                  1.150578  \n",
              "\n",
              "[151 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1699235f-2054-4b53-967e-6dd7f9ded934\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc(ppm)</th>\n",
              "      <th>ad_dose(g/L)</th>\n",
              "      <th>ph_value</th>\n",
              "      <th>temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "      <th>conc2(=absorbance/0.029)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31.487486</td>\n",
              "      <td>1.049047</td>\n",
              "      <td>8.881372</td>\n",
              "      <td>31.199066</td>\n",
              "      <td>31.676035</td>\n",
              "      <td>0.942461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>32.226532</td>\n",
              "      <td>1.509070</td>\n",
              "      <td>6.820713</td>\n",
              "      <td>32.126610</td>\n",
              "      <td>33.018497</td>\n",
              "      <td>1.242170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>36.380421</td>\n",
              "      <td>2.029041</td>\n",
              "      <td>6.050174</td>\n",
              "      <td>36.287495</td>\n",
              "      <td>37.034786</td>\n",
              "      <td>1.489969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33.025333</td>\n",
              "      <td>1.848757</td>\n",
              "      <td>5.596869</td>\n",
              "      <td>32.966301</td>\n",
              "      <td>33.948051</td>\n",
              "      <td>1.434995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28.842443</td>\n",
              "      <td>1.483023</td>\n",
              "      <td>6.569973</td>\n",
              "      <td>28.537069</td>\n",
              "      <td>57.077984</td>\n",
              "      <td>0.700526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>27.733757</td>\n",
              "      <td>1.416482</td>\n",
              "      <td>5.943005</td>\n",
              "      <td>27.640587</td>\n",
              "      <td>31.516726</td>\n",
              "      <td>0.874037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>29.375164</td>\n",
              "      <td>1.074924</td>\n",
              "      <td>8.254704</td>\n",
              "      <td>29.355972</td>\n",
              "      <td>30.344042</td>\n",
              "      <td>1.015706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>22.297516</td>\n",
              "      <td>0.958027</td>\n",
              "      <td>7.807487</td>\n",
              "      <td>32.447582</td>\n",
              "      <td>36.737713</td>\n",
              "      <td>0.894599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>31.583769</td>\n",
              "      <td>1.276017</td>\n",
              "      <td>3.389986</td>\n",
              "      <td>31.085382</td>\n",
              "      <td>62.289631</td>\n",
              "      <td>0.386164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>39.836014</td>\n",
              "      <td>1.883100</td>\n",
              "      <td>9.110263</td>\n",
              "      <td>39.171963</td>\n",
              "      <td>79.030586</td>\n",
              "      <td>1.150578</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>151 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1699235f-2054-4b53-967e-6dd7f9ded934')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1699235f-2054-4b53-967e-6dd7f9ded934 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1699235f-2054-4b53-967e-6dd7f9ded934');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_samples.to_csv('Generated_sample.csv')"
      ],
      "metadata": {
        "id": "UHrsdjFZpSzA"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yy=gen_samples.iloc[:,-1]\n",
        "xx=gen_samples.iloc[:,0:5]"
      ],
      "metadata": {
        "id": "MdCR7wWHlkaC"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_x_train = mms.inverse_transform(x_train)"
      ],
      "metadata": {
        "id": "JMFF3OIwv7HA"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_x_train_df = pd.DataFrame(original_x_train)"
      ],
      "metadata": {
        "id": "Qb25XUQFwXmj"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(original_x_train_df.shape)\n",
        "original_x_train_df.head()"
      ],
      "metadata": {
        "id": "FcX2-PoAwglQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "126a7925-ecc1-4f77-f1e1-db6bc0c1ac1d"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(81, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      0    1    2     3     4\n",
              "0  90.0  1.0  6.0  30.0  30.0\n",
              "1  30.0  1.0  8.0  30.0  30.0\n",
              "2  30.0  1.0  8.0  30.0  60.0\n",
              "3  30.0  1.0  2.0  30.0  60.0\n",
              "4  30.0  1.0  8.0  30.0  15.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b271857e-286d-4c9e-87e1-7c55b325e31a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b271857e-286d-4c9e-87e1-7c55b325e31a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b271857e-286d-4c9e-87e1-7c55b325e31a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b271857e-286d-4c9e-87e1-7c55b325e31a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_x_train_df.columns = ['conc(ppm)', 'ad_dose(g/L)', 'ph_value', 'temperature(⁰C)', 'time']"
      ],
      "metadata": {
        "id": "Rai4qc6Eyzxs"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(original_x_train_df.shape)\n",
        "original_x_train_df.head()"
      ],
      "metadata": {
        "id": "uEl-I7DaxNYc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "ab43e00a-d35a-4b7b-e8c8-12b1929db43b"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(81, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   conc(ppm)  ad_dose(g/L)  ph_value  temperature(⁰C)  time\n",
              "0       90.0           1.0       6.0             30.0  30.0\n",
              "1       30.0           1.0       8.0             30.0  30.0\n",
              "2       30.0           1.0       8.0             30.0  60.0\n",
              "3       30.0           1.0       2.0             30.0  60.0\n",
              "4       30.0           1.0       8.0             30.0  15.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cbadadd3-e2bc-4d98-89c2-30e6c0e4b083\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc(ppm)</th>\n",
              "      <th>ad_dose(g/L)</th>\n",
              "      <th>ph_value</th>\n",
              "      <th>temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbadadd3-e2bc-4d98-89c2-30e6c0e4b083')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cbadadd3-e2bc-4d98-89c2-30e6c0e4b083 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cbadadd3-e2bc-4d98-89c2-30e6c0e4b083');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [original_x_train_df, xx]\n",
        "\n",
        "gan_x_train = pd.concat(frames)"
      ],
      "metadata": {
        "id": "lDL_2Dm3wqZK"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan_x_train"
      ],
      "metadata": {
        "id": "hdOJ6dssw4yx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a4f99f71-2d75-4d69-a40f-b403ccfda158"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     conc(ppm)  ad_dose(g/L)  ph_value  temperature(⁰C)       time\n",
              "0    90.000000      1.000000  6.000000        30.000000  30.000000\n",
              "1    30.000000      1.000000  8.000000        30.000000  30.000000\n",
              "2    30.000000      1.000000  8.000000        30.000000  60.000000\n",
              "3    30.000000      1.000000  2.000000        30.000000  60.000000\n",
              "4    30.000000      1.000000  8.000000        30.000000  15.000000\n",
              "..         ...           ...       ...              ...        ...\n",
              "146  27.733757      1.416482  5.943005        27.640587  31.516726\n",
              "147  29.375164      1.074924  8.254704        29.355972  30.344042\n",
              "148  22.297516      0.958027  7.807487        32.447582  36.737713\n",
              "149  31.583769      1.276017  3.389986        31.085382  62.289631\n",
              "150  39.836014      1.883100  9.110263        39.171963  79.030586\n",
              "\n",
              "[232 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e686ea48-230b-4971-b7e2-cd3a5c6ec057\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc(ppm)</th>\n",
              "      <th>ad_dose(g/L)</th>\n",
              "      <th>ph_value</th>\n",
              "      <th>temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>60.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>60.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>15.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>27.733757</td>\n",
              "      <td>1.416482</td>\n",
              "      <td>5.943005</td>\n",
              "      <td>27.640587</td>\n",
              "      <td>31.516726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>29.375164</td>\n",
              "      <td>1.074924</td>\n",
              "      <td>8.254704</td>\n",
              "      <td>29.355972</td>\n",
              "      <td>30.344042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>22.297516</td>\n",
              "      <td>0.958027</td>\n",
              "      <td>7.807487</td>\n",
              "      <td>32.447582</td>\n",
              "      <td>36.737713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>31.583769</td>\n",
              "      <td>1.276017</td>\n",
              "      <td>3.389986</td>\n",
              "      <td>31.085382</td>\n",
              "      <td>62.289631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>39.836014</td>\n",
              "      <td>1.883100</td>\n",
              "      <td>9.110263</td>\n",
              "      <td>39.171963</td>\n",
              "      <td>79.030586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>232 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e686ea48-230b-4971-b7e2-cd3a5c6ec057')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e686ea48-230b-4971-b7e2-cd3a5c6ec057 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e686ea48-230b-4971-b7e2-cd3a5c6ec057');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "kgW1O3s50QRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de8052f7-ba95-4025-ae03-2118a6f71a81"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21    22.924138\n",
              "61     0.868966\n",
              "63     0.782759\n",
              "48     0.103448\n",
              "60     1.010345\n",
              "        ...    \n",
              "29     9.065517\n",
              "28    10.979310\n",
              "64     0.624138\n",
              "15    28.468966\n",
              "9      0.382759\n",
              "Name: conc2(=absorbance/0.029), Length: 81, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yy"
      ],
      "metadata": {
        "id": "7lrXVQEj0Qyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf3022e-1670-410f-f360-26b477ea0a12"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0.942461\n",
              "1      1.242170\n",
              "2      1.489969\n",
              "3      1.434995\n",
              "4      0.700526\n",
              "         ...   \n",
              "146    0.874037\n",
              "147    1.015706\n",
              "148    0.894599\n",
              "149    0.386164\n",
              "150    1.150578\n",
              "Name: conc2(=absorbance/0.029), Length: 151, dtype: float32"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [y_train, yy]\n",
        "\n",
        "gan_y_train = pd.concat(frames, ignore_index = True)"
      ],
      "metadata": {
        "id": "thiz-P-80BOo"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan_y_train"
      ],
      "metadata": {
        "id": "22-WWIby0HiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b94bc95-0197-46ab-ac80-6c518eacf632"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      22.924138\n",
              "1       0.868966\n",
              "2       0.782759\n",
              "3       0.103448\n",
              "4       1.010345\n",
              "         ...    \n",
              "227     0.874037\n",
              "228     1.015706\n",
              "229     0.894599\n",
              "230     0.386164\n",
              "231     1.150578\n",
              "Name: conc2(=absorbance/0.029), Length: 232, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gan_x_train_scaled = mms.transform(gan_x_train)"
      ],
      "metadata": {
        "id": "SvjmcBuT1pmU"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANN with GAN generated data"
      ],
      "metadata": {
        "id": "PCH5Jofg8bIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building model\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units= hp.Int('N', min_value=16, max_value=512, step=16), activation= 'relu', input_dim= x_train.shape[1]))\n",
        "    for i in range(hp.Int('h_layers', min_value=1, max_value=3)):\n",
        "        model.add(Dense(units= hp.Int('neuron'+str(i), min_value=16, max_value=256, step=16), activation= 'relu'))\n",
        "    model.add(Dense(units=1, activation='linear'))\n",
        "              \n",
        "    model.compile(optimizer = Adam(learning_rate = hp.Choice('lr', [1e-2, 1e-3, 1e-4])), loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "m_aR-eaP2A03"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = RandomSearch(build_model, objective='val_mean_absolute_error', max_trials=10, executions_per_trial=3, directory='output_gan', project_name='tartazine_removal_gan')"
      ],
      "metadata": {
        "id": "p3YyYyVzsacw"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(gan_x_train_scaled, gan_y_train, epochs=20, validation_data=(x_test,y_test))"
      ],
      "metadata": {
        "id": "Zfhw3gHxtQ8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7364052-a961-4e91-c7e7-8dd4c9995919"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 09s]\n",
            "val_mean_absolute_error: 1.989112655321757\n",
            "\n",
            "Best val_mean_absolute_error So Far: 0.6087969938913981\n",
            "Total elapsed time: 00h 01m 23s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_gan = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "AGLHlWrCtTmM"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_gan.summary()"
      ],
      "metadata": {
        "id": "PWfDudvgtWyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75985102-8e82-49b9-fa98-af0eaaebd9a4"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 352)               2112      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 160)               56480     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 192)               30912     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 193       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 89,697\n",
            "Trainable params: 89,697\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_x_test = mms.inverse_transform(x_test)\n",
        "original_x_test_df = pd.DataFrame(original_x_test)\n",
        "print(original_x_test_df.shape)\n",
        "original_x_test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "wFDsD81pJ3w2",
        "outputId": "e9142e8f-ccf8-4eba-e350-1cd5c6171b6e"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      0    1    2     3      4\n",
              "0  30.0  1.0  6.0  30.0   30.0\n",
              "1  30.0  0.5  6.0  30.0   30.0\n",
              "2  10.0  1.0  6.0  30.0   60.0\n",
              "3  30.0  1.5  6.0  30.0   15.0\n",
              "4  60.0  1.0  6.0  30.0  120.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f2f47fb-c553-452e-9f2b-0161412b96cd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>120.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f2f47fb-c553-452e-9f2b-0161412b96cd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f2f47fb-c553-452e-9f2b-0161412b96cd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f2f47fb-c553-452e-9f2b-0161412b96cd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_x_test_df.columns = ['conc(ppm)', 'ad_dose(g/L)', 'ph_value', 'temperature(⁰C)', 'time']\n",
        "original_x_test_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "wAcHPRSfKJe9",
        "outputId": "061125a5-f5e9-4485-bfcc-f3ff58ce6dd9"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   conc(ppm)  ad_dose(g/L)  ph_value  temperature(⁰C)   time\n",
              "0       30.0           1.0       6.0             30.0   30.0\n",
              "1       30.0           0.5       6.0             30.0   30.0\n",
              "2       10.0           1.0       6.0             30.0   60.0\n",
              "3       30.0           1.5       6.0             30.0   15.0\n",
              "4       60.0           1.0       6.0             30.0  120.0\n",
              "5       30.0           1.0       6.0             30.0  120.0\n",
              "6       30.0           1.5       6.0             30.0   45.0\n",
              "7       30.0           1.0      10.0             30.0   60.0\n",
              "8       30.0           1.5       6.0             30.0   60.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59f26833-e907-47a9-8ae3-98a21eeb9c30\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc(ppm)</th>\n",
              "      <th>ad_dose(g/L)</th>\n",
              "      <th>ph_value</th>\n",
              "      <th>temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>120.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>120.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59f26833-e907-47a9-8ae3-98a21eeb9c30')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59f26833-e907-47a9-8ae3-98a21eeb9c30 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59f26833-e907-47a9-8ae3-98a21eeb9c30');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypred_gan = best_model_gan.predict(x_test)"
      ],
      "metadata": {
        "id": "N3mPY_rstbdY"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "v3PEnE6Otisy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dbe318c-b0ff-458c-faf5-bc00ea7ff1a2"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76     2.665517\n",
              "26    12.703448\n",
              "3      0.482759\n",
              "35     0.934483\n",
              "19     3.313793\n",
              "14     0.600000\n",
              "37     0.655172\n",
              "68     0.455172\n",
              "38     0.403448\n",
              "Name: conc2(=absorbance/0.029), dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypred_gan"
      ],
      "metadata": {
        "id": "umlczE4Etftu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dda84a8-964a-4493-e6db-e61eac07c35a"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.7703028 ],\n",
              "       [13.346735  ],\n",
              "       [ 0.703951  ],\n",
              "       [ 1.285026  ],\n",
              "       [ 0.82566977],\n",
              "       [ 0.599627  ],\n",
              "       [ 0.8153511 ],\n",
              "       [ 0.7389965 ],\n",
              "       [ 0.7148862 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('MAE :', mae(y_test, ypred_gan))"
      ],
      "metadata": {
        "id": "Un1D9MA-7qKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c991575-3f7e-41fe-81df-0a6046519ac5"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE : 0.5070827634855248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate actual '%removal'\n",
        "testt = []\n",
        "for i in y_test:\n",
        "  testt.append((30-i)*(100/30))\n",
        "testt"
      ],
      "metadata": {
        "id": "im5t-YLYt-Su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6f8893-d752-4460-af53-7d75faf85897"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[91.11494252873564,\n",
              " 57.6551724137931,\n",
              " 98.39080459770115,\n",
              " 96.88505747126437,\n",
              " 88.95402298850576,\n",
              " 98.0,\n",
              " 97.816091954023,\n",
              " 98.48275862068965,\n",
              " 98.65517241379311]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate DL model predicted '%removal'\n",
        "predd = []\n",
        "for i in ypred_gan:\n",
        "  predd.append((30-i[0])*(100/30))\n",
        "predd"
      ],
      "metadata": {
        "id": "aJJshvy8uLsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca7622f-e973-4e80-d103-9a8a1d613bad"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[90.76565742492676,\n",
              " 55.51088333129883,\n",
              " 97.65349666277568,\n",
              " 95.7165801525116,\n",
              " 97.2477674484253,\n",
              " 98.00124327341716,\n",
              " 97.28216290473938,\n",
              " 97.53667831420898,\n",
              " 97.61704603830974]"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# statistical results of Deep Learning model predicted conc\n",
        "print('R2 :', r2(testt, predd))"
      ],
      "metadata": {
        "id": "IqtDf4g07Nn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a850fcbb-0907-47b9-f204-e694b178f760"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 : 0.9449133170059256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare actual and predicted '%removal'\n",
        "\n",
        "print('percent removal error :', mae(testt, predd))"
      ],
      "metadata": {
        "id": "QiuYC0705wnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e5b888-409c-4c41-9229-b1e8a45652e9"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percent removal error : 1.6902758782850837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save best DL model for further prediction\n",
        "import pickle\n",
        "\n",
        "pickle.dump(best_model_gan,open('Nilavo_DL_GAN.pkl','wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7MEC6ZiIfon",
        "outputId": "e1b33808-601e-4741-f0b0-19635ed2744a"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://df4253e9-962e-4e93-8869-a6f036ea55c6/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking Machine Learning (KNN) & Deep Learning (ANN) models performance."
      ],
      "metadata": {
        "id": "JcpzrP1SaUiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###  Models                             Saved as         R2      MAE(Conc2)   MAE(%removal)  ###\n",
        "\n",
        "#    ML(KNN)                            Nilavo_ML      0.9753      0.4563      1.5210  \n",
        "#    DL(ANN)                            Nilavo_DL      0.9893      0.2660      0.8868\n",
        "#    DL(ANN with GAN generated data)    Nilavo_DL_GAN  0.9449      0.5070      1.6902"
      ],
      "metadata": {
        "id": "-h89GEQpW285"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN model's mean-absolute-error is low. So, I think this is the best model for further prediction."
      ],
      "metadata": {
        "id": "aSlWEBpDavoH"
      }
    }
  ]
}